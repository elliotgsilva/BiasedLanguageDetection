{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictionary, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, RandomSampler, SequentialSampler\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prepocessed Dataset(already tokenized)\n",
    "with open(\"./data/ground_truths_df.p\", 'rb') as handle:\n",
    "    datasets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=datasets[datasets['review'].apply(lambda x: len(x)<=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, datasets, include_valid=False):\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "        self.counts = {}\n",
    "        \n",
    "        # add special tokens\n",
    "        self.add_token('<pad>')\n",
    "        self.add_token('<unk>')\n",
    "        \n",
    "        for line in tqdm(datasets['review']):\n",
    "            for w in line:\n",
    "                self.add_token(w)\n",
    "\n",
    "                            \n",
    "    def add_token(self, w):\n",
    "        if w not in self.tokens:\n",
    "            self.tokens.append(w)\n",
    "            _w_id = len(self.tokens) - 1\n",
    "            self.ids[w] = _w_id\n",
    "            self.counts[w] = 1\n",
    "        else:\n",
    "            self.counts[w] += 1\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "    def decode_idx_seq(self, l):\n",
    "        return [self.tokens[i] for i in l]\n",
    "    \n",
    "    def encode_token_seq(self, l):\n",
    "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 9702.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary\n",
    "review_dict = Dictionary(datasets, include_valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict.get_id(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict.encode_token_seq(datasets.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexize_dataset(datasets, dictionary):\n",
    "    indexized_datasets = []\n",
    "    for l in tqdm(datasets[\"review\"]):\n",
    "        encoded_l = dictionary.encode_token_seq(l)\n",
    "        indexized_datasets.append(encoded_l)\n",
    "        \n",
    "    return indexized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 77687.44it/s]\n"
     ]
    }
   ],
   "source": [
    "indexized_datasets = indexize_dataset(datasets, review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensoredDataset(object):\n",
    "    def __init__(self, list_of_lists_of_tokens, list_of_labels,list_of_flagged_indexes):\n",
    "        self.input_tensors = []\n",
    "        self.label_tensors = []\n",
    "        self.flagged_index = []\n",
    "        self.problematic = []\n",
    "        \n",
    "        for sample in list_of_lists_of_tokens:\n",
    "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
    "        for sample in list_of_labels:\n",
    "            self.label_tensors.append(torch.tensor(sample, dtype=torch.long))\n",
    "        for sample in list_of_flagged_indexes:\n",
    "            self.flagged_index.append(torch.tensor(sample, dtype=torch.long))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx], self.label_tensors[idx], self.flagged_index[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = TensoredDataset(indexized_datasets,datasets[\"true_pos\"].to_list(), datasets[\"flagged_index\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10]]), tensor(0), tensor(0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first example\n",
    "tensor_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = 30\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:    \n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor[:max_length])\n",
    "        \n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    token_list = [s[0] for s in batch]\n",
    "    label_list = torch.LongTensor([s[1] for s in batch])\n",
    "    idx_list = torch.LongTensor([s[2] for s in batch])\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = 0\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(token_list, pad_token)\n",
    "    \n",
    "    return input_tensor, label_list, idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "seed = 1029\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "\n",
    "# Divide into train(95%), valid(5%) dataset\n",
    "batch_size = 32\n",
    "n_train_samples = int(0.95 * len(datasets))\n",
    "n_val_samples = len(datasets) - n_train_samples\n",
    "\n",
    "train_dataset, val_dataset = random_split(tensor_dataset, [n_train_samples, n_val_samples])\n",
    "\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=pad_collate_fn, worker_init_fn=_init_fn)\n",
    "#val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=pad_collate_fn, worker_init_fn=_init_fn)\n",
    "dataloader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[340, 305, 360,  15, 203,  15,  55,  15, 214,   6, 217,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [297, 117,   2,   6, 121,  15,  98, 298, 195, 117, 299,   6,  39,  52,\n",
      "         300,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [283, 115,   6,   2, 284, 285, 286,   4, 287,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [108, 109,  15, 110,  15, 111,  15, 112, 113,  51, 114, 115,  15, 116,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [320, 321, 162, 159, 322,  33,  24, 161,  51,  52,  53,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  2,  62,   6, 101,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 97,  15, 146,  15,   6, 147, 148,  15,   2, 149,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 44,  45,  46,  47,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [210,  15, 203,   6,  83, 211,  39,  33,  24, 212,  71,  72,  73,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [171, 151, 172,  24, 173,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [145,  13,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  2,   3,  20,  83, 112, 247, 248, 249, 250, 251, 252,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [112, 361, 146, 362, 363, 318,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 89,   3,  88,  90,  91,  88,  92,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [150, 151,  24, 152, 117, 153,  28, 154,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 49, 156, 157,  36,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 12, 213, 112, 121,  15, 214,  15, 215,   6, 201,  39,  33,  24,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  2,  33,  34,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [181,  98,  83,   2,   3,  67, 161,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 48, 219, 150, 220, 221,  67, 222, 223,  15, 224, 225,   6,  55,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 93,  94,  67,  52,   2, 252,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  2,  62,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [334, 117, 179, 328,   6,  83, 118,  39, 105, 284,  29, 335,   6, 316,\n",
      "         161, 336, 112, 284, 337, 338,  52, 339,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [270,  13, 201,  39,  33,  24,   6, 271,  20,  83, 272, 273,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [312,  70, 230,  56,  15, 313, 314,  15,   2,  33,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 37,  38,  39,  40,  41,  24,   2,  42,  43,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 48,  49,  50,  39,  33,  24,  35,  51,  52,  53,   4,  12,  13,  54,\n",
      "          15,  55,  15,   6,  56,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  2,  62,  15, 174,  52, 175, 112, 176,  15, 146, 177, 178,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [136,  67,  52,   2,  33,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  2, 330,   6, 331,  39, 332, 333, 149,  63,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [206, 117, 186,  39, 207,  83,   2, 115,  24, 208,  41,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [315,  15,  55,   6, 316, 150,  43,  83, 317,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0]]), tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 1]), tensor([ 4,  2,  3,  8,  1,  0,  8,  2,  2,  1,  2,  8,  1,  2,  1,  2,  5,  0,\n",
      "         3, 12,  4,  0,  3,  2,  1,  6, 13,  0,  3,  1,  6,  2]))\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(dataloader):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/data/'\n",
    "\n",
    "# pickle_train_dataloader = open(data_dir + \"train_dataloader.p\",\"wb\")\n",
    "# pickle.dump(train_dataloader, pickle_train_dataloader)\n",
    "# pickle_train_dataloader.close()\n",
    "\n",
    "# pickle_val_dataloader = open(data_dir + \"val_dataloader.p\",\"wb\")\n",
    "# pickle.dump(val_dataloader, pickle_val_dataloader)\n",
    "# pickle_val_dataloader.close()\n",
    "\n",
    "pickle_dataloader = open(data_dir + \"ground_truth_dataloader.p\",\"wb\")\n",
    "pickle.dump(dataloader, pickle_dataloader)\n",
    "pickle_dataloader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
