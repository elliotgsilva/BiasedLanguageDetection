{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictionary, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, RandomSampler, SequentialSampler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prepocessed Dataset(already tokenized)\n",
    "with open(\"./data/master_df.p\", 'rb') as handle:\n",
    "    datasets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=datasets[datasets['review'].apply(lambda x: len(x)<=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, datasets, include_valid=False):\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "        self.counts = {}\n",
    "        \n",
    "        # add special tokens\n",
    "        self.add_token('<bos>')\n",
    "        self.add_token('<eos>')\n",
    "        self.add_token('<pad>')\n",
    "        self.add_token('<unk>')\n",
    "        \n",
    "        for line in tqdm(datasets['review']):\n",
    "            for w in line:\n",
    "                self.add_token(w)\n",
    "\n",
    "                            \n",
    "    def add_token(self, w):\n",
    "        if w not in self.tokens:\n",
    "            self.tokens.append(w)\n",
    "            _w_id = len(self.tokens) - 1\n",
    "            self.ids[w] = _w_id\n",
    "            self.counts[w] = 1\n",
    "        else:\n",
    "            self.counts[w] += 1\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "    def decode_idx_seq(self, l):\n",
    "        return [self.tokens[i] for i in l]\n",
    "    \n",
    "    def encode_token_seq(self, l):\n",
    "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134512/134512 [00:22<00:00, 5996.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary\n",
    "review_dict = Dictionary(datasets, include_valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict.get_id(\"thank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict.encode_token_seq(datasets.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexize_dataset(datasets, dictionary):\n",
    "    indexized_datasets = []\n",
    "    for l in tqdm(datasets[\"review\"]):\n",
    "        encoded_l = dictionary.encode_token_seq(l)\n",
    "        indexized_datasets.append(encoded_l)\n",
    "        \n",
    "    return indexized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134512/134512 [00:00<00:00, 195086.74it/s]\n"
     ]
    }
   ],
   "source": [
    "indexized_datasets = indexize_dataset(datasets, review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensoredDataset(object):\n",
    "    def __init__(self, list_of_lists_of_tokens, list_of_flagged_indexes, list_of_problematic_flags):\n",
    "        self.input_tensors = []\n",
    "        self.flagged_index = []\n",
    "        self.problematic = []\n",
    "        \n",
    "        for sample in list_of_lists_of_tokens:\n",
    "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
    "        for sample in list_of_flagged_indexes:\n",
    "            self.flagged_index.append(torch.tensor(sample, dtype=torch.long))\n",
    "        for sample in list_of_problematic_flags:\n",
    "            self.problematic.append(torch.tensor(sample, dtype=torch.long))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx], self.flagged_index[idx], self.problematic[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = TensoredDataset(indexized_datasets,datasets[\"flagged_index\"].to_list(),datasets[\"problematic\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4,  5,  6,  7,  8,  9, 10]]), tensor(6), tensor(1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first example\n",
    "tensor_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = 30\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:    \n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor[:max_length])\n",
    "        \n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    token_list = [s[0] for s in batch]\n",
    "    idx_list = torch.FloatTensor([s[1] for s in batch])\n",
    "    problematic = torch.FloatTensor([s[2] for s in batch])\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = 2\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(token_list, pad_token)\n",
    "    \n",
    "    return input_tensor, idx_list, problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into train(95%), valid(5%) dataset\n",
    "batch_size = 32\n",
    "n_train_samples = int(0.95 * len(datasets))\n",
    "n_val_samples = len(datasets) - n_train_samples\n",
    "\n",
    "train_dataset, val_dataset = random_split(tensor_dataset, [n_train_samples, n_val_samples])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(train_dataloader):\n",
    "    print(x[0].shape)\n",
    "    print(x[1].shape)\n",
    "    print(x[2].shape)\n",
    "    print(x[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/data/'\n",
    "\n",
    "pickle_train_dataloader = open(data_dir + \"train_dataloader.p\",\"wb\")\n",
    "pickle.dump(train_dataloader, pickle_train_dataloader)\n",
    "pickle_train_dataloader.close()\n",
    "\n",
    "pickle_val_dataloader = open(data_dir + \"val_dataloader.p\",\"wb\")\n",
    "pickle.dump(val_dataloader, pickle_val_dataloader)\n",
    "pickle_val_dataloader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
