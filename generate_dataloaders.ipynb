{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictionary, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, RandomSampler, SequentialSampler\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prepocessed Dataset(already tokenized)\n",
    "with open(\"./data/ground_truths_df.p\", 'rb') as handle:\n",
    "    datasets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=datasets[datasets['review'].apply(lambda x: len(x)<=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>flagged_word</th>\n",
       "      <th>flagged_index</th>\n",
       "      <th>problematic</th>\n",
       "      <th>true_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9133</th>\n",
       "      <td>[great, job, as, usual-above, and, beyond, cal...</td>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>[he, is, intelligent, ,, dedicated, ,, passion...</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15243</th>\n",
       "      <td>[great, work, hire, him, again]</td>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15541</th>\n",
       "      <td>[strong, ability, to, execute, feedback, with,...</td>\n",
       "      <td>great</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16159</th>\n",
       "      <td>[awesome, +, dependable, designer, !]</td>\n",
       "      <td>dependable</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review flagged_word  \\\n",
       "9133   [great, job, as, usual-above, and, beyond, cal...        great   \n",
       "14637  [he, is, intelligent, ,, dedicated, ,, passion...  intelligent   \n",
       "15243                    [great, work, hire, him, again]        great   \n",
       "15541  [strong, ability, to, execute, feedback, with,...        great   \n",
       "16159              [awesome, +, dependable, designer, !]   dependable   \n",
       "\n",
       "       flagged_index  problematic  true_pos  \n",
       "9133               0            1       0.0  \n",
       "14637              2            1       1.0  \n",
       "15243              0            1       0.0  \n",
       "15541              6            1       0.0  \n",
       "16159              2            1       1.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, datasets, include_valid=False):\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "        self.counts = {}\n",
    "        \n",
    "        # add special tokens\n",
    "        self.add_token('<pad>')\n",
    "        self.add_token('<unk>')\n",
    "        \n",
    "        for line in tqdm(datasets['review']):\n",
    "            for w in line:\n",
    "                self.add_token(w)\n",
    "\n",
    "                            \n",
    "    def add_token(self, w):\n",
    "        if w not in self.tokens:\n",
    "            self.tokens.append(w)\n",
    "            _w_id = len(self.tokens) - 1\n",
    "            self.ids[w] = _w_id\n",
    "            self.counts[w] = 1\n",
    "        else:\n",
    "            self.counts[w] += 1\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "    def decode_idx_seq(self, l):\n",
    "        return [self.tokens[i] for i in l]\n",
    "    \n",
    "    def encode_token_seq(self, l):\n",
    "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 7860.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary\n",
    "review_dict = Dictionary(datasets, include_valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dict = open(data_dir + \"dictionary.p\",\"wb\")\n",
    "pickle.dump(review_dict, pickle_dict)\n",
    "pickle_dict.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict.get_id(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict.encode_token_seq(datasets.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexize_dataset(datasets, dictionary):\n",
    "    indexized_datasets = []\n",
    "    for l in tqdm(datasets[\"review\"]):\n",
    "        encoded_l = dictionary.encode_token_seq(l)\n",
    "        indexized_datasets.append(encoded_l)\n",
    "        \n",
    "    return indexized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 9843.84it/s]\n"
     ]
    }
   ],
   "source": [
    "indexized_datasets = indexize_dataset(datasets, review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensoredDataset(object):\n",
    "    def __init__(self, list_of_lists_of_tokens, list_of_labels,list_of_flagged_indexes):\n",
    "        self.input_tensors = []\n",
    "        self.label_tensors = []\n",
    "        self.flagged_index = []\n",
    "        self.problematic = []\n",
    "        \n",
    "        for sample in list_of_lists_of_tokens:\n",
    "            self.input_tensors.append(torch.tensor([sample], dtype=torch.long))\n",
    "        for sample in list_of_labels:\n",
    "            self.label_tensors.append(torch.tensor(sample, dtype=torch.long))\n",
    "        for sample in list_of_flagged_indexes:\n",
    "            self.flagged_index.append(torch.tensor(sample, dtype=torch.long))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx], self.label_tensors[idx], self.flagged_index[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = TensoredDataset(indexized_datasets,datasets[\"true_pos\"].to_list(), datasets[\"flagged_index\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11]]), tensor(0), tensor(0))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first example\n",
    "tensor_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = 30\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:    \n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor[:max_length])\n",
    "        \n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    token_list = [s[0] for s in batch]\n",
    "    label_list = torch.LongTensor([s[1] for s in batch])\n",
    "    idx_list = torch.LongTensor([s[2] for s in batch])\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = 0\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(token_list, pad_token)\n",
    "    \n",
    "    return input_tensor, label_list, idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "seed = 1029\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "\n",
    "# Divide into train(95%), valid(5%) dataset\n",
    "batch_size = 32\n",
    "n_train_samples = int(0.95 * len(datasets))\n",
    "n_val_samples = len(datasets) - n_train_samples\n",
    "\n",
    "train_dataset, val_dataset = random_split(tensor_dataset, [n_train_samples, n_val_samples])\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=pad_collate_fn, worker_init_fn=_init_fn)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=pad_collate_fn, worker_init_fn=_init_fn)\n",
    "dataloader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6328,   69,   34,   44,   19,   30,    7,   77,  230,    9,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0]) tensor(8)\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(val_dataloader):\n",
    "    print(x[0][0],x[2][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/data/'\n",
    "\n",
    "# pickle_train_dataloader = open(data_dir + \"train_dataloader.p\",\"wb\")\n",
    "# pickle.dump(train_dataloader, pickle_train_dataloader)\n",
    "# pickle_train_dataloader.close()\n",
    "\n",
    "# pickle_val_dataloader = open(data_dir + \"val_dataloader.p\",\"wb\")\n",
    "# pickle.dump(val_dataloader, pickle_val_dataloader)\n",
    "# pickle_val_dataloader.close()\n",
    "\n",
    "pickle_dataloader = open(data_dir + \"ground_truth_dataloader.p\",\"wb\")\n",
    "pickle.dump(dataloader, pickle_dataloader)\n",
    "pickle_dataloader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
