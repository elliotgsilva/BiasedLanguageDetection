{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## KAGGLE ONLY\n",
    "# from shutil import copyfile\n",
    "# copyfile(src=\"../input/inputs/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\n",
    "# copyfile(src=\"../input/inputs/train_dataloader.p\", dst=\"../working/train_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/val_dataloader.p\", dst=\"../working/val_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/centroids_dataloader.p\", dst=\"../working/ground_truth_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/dictionary.p\", dst=\"../working/dictionary.p\")\n",
    "\n",
    "# copyfile(src=\"../input/input2/train_unlabeld_dataloader.p\", dst=\"../working/train_unlabelled_dataloader.p\")\n",
    "# copyfile(src=\"../input/input2/train_labeled_dataloader.p\", dst=\"../working/train_labelled_dataloader.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zno22FtJPX9z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation' from '/home/jubly/FairFrame/evaluation.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from datasets import get_mnist_dataset, get_data_loader\n",
    "#from utils import *\n",
    "#from models import *\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from generate_dataloaders import *\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import evaluation\n",
    "import importlib\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaJEVd0wPX94"
   },
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1029\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nLzh007PX98"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/'\n",
    "data_dir = path +'/data/' #Uncomment for local system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Verify filenames are consistent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yq-jDGFIPX99"
   },
   "outputs": [],
   "source": [
    "train_loader_labelled = pkl.load(open(data_dir + 'train_labeled_dataloader.p','rb'))\n",
    "train_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabeled_dataloader.p','rb'))\n",
    "val_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict = pkl.load(open(data_dir + 'dictionary.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install pytorch torchvision -c pytorch\n",
    "## if torch.__version__ is not 1.3.1, run this cell then restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzz8lwNQPX-B",
    "outputId": "690cb77f-2525-4c5a-ea14-a162716e34d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE TRAINED WORD EMBEDDINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(review_dict, embedding_index ,dim = 200):\n",
    "#     embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(review_dict.tokens), dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in review_dict.ids.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOCAL - 2nd line // KAGGLE -- 1st line\n",
    "# glove_twitter = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt' #Change loc for local system\n",
    "glove_twitter = data_dir + 'glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9457b588880741e0a52453ae29431812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_index = load_embeddings(glove_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_index,unknown_words = build_matrix(review_dict, embedding_index)\n",
    "del embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16256"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_dict.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4428"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in unknown_words:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvt6N9QCPX-X"
   },
   "source": [
    "## Neural Network LSTM Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puweJhdxPX-Y"
   },
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8BZ-QhNPX-Z"
   },
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM classification model using pretrained glove embeddings\n",
    "    \"\"\"\n",
    "    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n",
    "    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n",
    "    def __init__(self, embedding_matrix, num_hidden_layers = 3, hidden_size = 100, num_classes = 2):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        vocab_size = embedding_matrix.shape[0]\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)    \n",
    "        self.embed.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        #self.embed.weight.requires_grad = unfrozen\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embed_size,self.hidden_size,self.num_hidden_layers, batch_first=True,bidirectional= True,bias=True)\n",
    "        \n",
    "        self.projection = nn.Linear(2*self.hidden_size, self.num_classes, bias=True)\n",
    "\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        batch_size, num_tokens = tokens.shape\n",
    "        embedding = self.embed(tokens)\n",
    "#         print(embedding.shape) # below assumes \"batch_size x num_tokens x Emb_dim\" (VERIFY)\n",
    "        \n",
    "        lstm_output = self.lstm(embedding)\n",
    "        # lstm_output is a tuple containing lstm output and (hidden_state, lstm_cell). \n",
    "        # lstm_output[0] would be of shape \"batch_size x num_tokens x hidden_size\" (VERIFY)\n",
    "        \n",
    "        logits = self.projection(lstm_output[0])\n",
    "        # logits would be of shape \"batch_size x num_tokens x num_classes (2)\" (VERIFY)\n",
    "        \n",
    "        batch_size, _, __ = logits.shape\n",
    "        \n",
    "        #selecting the logit at the flagged index\n",
    "        relevant_logits = logits[list(range(batch_size)),flagged_index]\n",
    "        # relevant_logits would be of shape \"batch_size x num_classes (2)\" (VERIFY)\n",
    "        \n",
    "        return relevant_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First performing fully supervised learning using the labelled set to train new vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "model = LSTM_model(glove_embedding_index, num_hidden_layers = 3, hidden_size = 100, num_classes = 2).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "#optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, criterion, train_loader_labelled, valid_loader, num_epochs=10, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    num_first_epochs = 10\n",
    "    num_second_epochs = num_epochs - num_first_epohcs\n",
    "    \n",
    "    # freeze part\n",
    "    freeze_model(model)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    \n",
    "    for epoch in range(num_first_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i,(tokens_labelled, labels, flagged_indices_labelled) in tqdm(enumerate(train_loader_labelled)):\n",
    "            \n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            logits = model(tokens_labelled,flagged_indices_labelled)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add loss to the epoch loss\n",
    "            total_epoch_loss += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = loss/len(tokens_labelled)\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= len(train_loader_labelled.dataset)\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            logits = model(tokens,flagged_indices)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += loss\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n",
    "                    \"num_hidden_layers\":model.num_hidden_layers,\\\n",
    "                    \"hidden_size\":model.hidden_size,\\\n",
    "                    \"num_classes\":model.num_classes}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict_labelled.pt')\n",
    "            torch.save(train_losses, path_to_save+'train_losses_labelled')\n",
    "            torch.save(val_losses, path_to_save+'val_losses_labelled')\n",
    "            torch.save(opts, path_to_save+'opts_labelled')\n",
    "            \n",
    "    # unfreeze part\n",
    "    unfreeze_model(model)\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            \n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    \n",
    "    for epoch in range(num_second_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i,(tokens_labelled, labels, flagged_indices_labelled) in tqdm(enumerate(train_loader_labelled)):\n",
    "            \n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            logits = model(tokens_labelled,flagged_indices_labelled)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add loss to the epoch loss\n",
    "            total_epoch_loss += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = loss/len(tokens_labelled)\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= len(train_loader_labelled.dataset)\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            logits = model(tokens,flagged_indices)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += loss\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n",
    "                    \"num_hidden_layers\":model.num_hidden_layers,\\\n",
    "                    \"hidden_size\":model.hidden_size,\\\n",
    "                    \"num_classes\":model.num_classes}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict_labelled.pt')\n",
    "            torch.save(train_losses, path_to_save+'train_losses_labelled')\n",
    "            torch.save(val_losses, path_to_save+'val_losses_labelled')\n",
    "            torch.save(opts, path_to_save+'opts_labelled')\n",
    "        \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "model_folder = 'lstm_model/'\n",
    "model_dir = path + '/models/' + model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:41:33.027204 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9824d75ba6404d50bb88445f2bd50c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.735\n",
      "\n",
      "Average training loss after epoch  0 : 0.291\n",
      "Average validation loss after epoch  0 : 0.252\n",
      "2019-11-24 17:41:46.616863 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a9400681de43249decd888a4a2eb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.230\n",
      "\n",
      "Average training loss after epoch  1 : 0.220\n",
      "Average validation loss after epoch  1 : 0.346\n",
      "2019-11-24 17:42:00.425754 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db601fa3f4d43e6bc3d5d04810b7022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.437\n",
      "\n",
      "Average training loss after epoch  2 : 0.203\n",
      "Average validation loss after epoch  2 : 0.232\n",
      "2019-11-24 17:42:14.009057 | Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645eaa73e39c46b4b9a32cf711f31047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.283\n",
      "\n",
      "Average training loss after epoch  3 : 0.179\n",
      "Average validation loss after epoch  3 : 0.291\n",
      "2019-11-24 17:42:28.000180 | Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6f3c5f1a784e7a97c7de8b8b009450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.101\n",
      "\n",
      "Average training loss after epoch  4 : 0.158\n",
      "Average validation loss after epoch  4 : 0.236\n",
      "2019-11-24 17:42:41.757071 | Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b99a5980c1047a389acc6ff6d5471d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.235\n",
      "\n",
      "Average training loss after epoch  5 : 0.143\n",
      "Average validation loss after epoch  5 : 0.266\n",
      "2019-11-24 17:42:55.238721 | Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f27670dc9b8497db276d0c48e54ca44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.075\n",
      "\n",
      "Average training loss after epoch  6 : 0.124\n",
      "Average validation loss after epoch  6 : 0.322\n",
      "2019-11-24 17:43:08.867896 | Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2406ca27ec4368b974e3d9e0d7ea9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.045\n",
      "\n",
      "Average training loss after epoch  7 : 0.098\n",
      "Average validation loss after epoch  7 : 0.260\n",
      "2019-11-24 17:43:22.675940 | Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2df11b80a984fd18cfb945fbd542d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.046\n",
      "\n",
      "Average training loss after epoch  8 : 0.087\n",
      "Average validation loss after epoch  8 : 0.392\n",
      "2019-11-24 17:43:36.501053 | Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df0783e9cf34d37b10db3a265ab292c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.027\n",
      "\n",
      "Average training loss after epoch  9 : 0.083\n",
      "Average validation loss after epoch  9 : 0.359\n"
     ]
    }
   ],
   "source": [
    "lstm_model, train_losses, val_losses = train_supervised_model(model, criterion, train_loader_labelled, val_loader, num_epochs=15, path_to_save=model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGsqcnEtPX-a"
   },
   "source": [
    "### Clustering Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrgIYm8JPX-b"
   },
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self, lmbda):\n",
    "        super().__init__()\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n",
    "        if labelled:\n",
    "            num_reviews = len(cluster_assignments)\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n",
    "            loss = self.lmbda * cluster_distances.sum()\n",
    "        else:\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances, cluster_assignments = distances.min(1)\n",
    "            loss = self.lmbda * cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TJohK2aPX-d"
   },
   "outputs": [],
   "source": [
    "def centroid_init(k, d, dataloader, model, current_device):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = torch.zeros(k, d).to(current_device)\n",
    "    centroid_counts = torch.zeros(k).to(current_device)\n",
    "    for (tokens, labels, flagged_indices) in dataloader:\n",
    "        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n",
    "        cluster_assignments = labels.to(current_device)\n",
    "        \n",
    "        model.eval()\n",
    "        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n",
    "    \n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, sentence_embed.to(current_device))\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n",
    "    centroid_counts.add_(bin_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_labelled_iter\n",
    "\n",
    "\n",
    "def loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_unlabelled_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3wynM7fPX-h"
   },
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clusters(model, centroids, criterion, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=15 num_batches = 1000, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    num_first_epochs = 10\n",
    "    num_second_epochs = num_epochs - num_first_epohcs\n",
    "    \n",
    "    # freeze part\n",
    "    freeze_model(model)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    \n",
    "    for epoch in range(num_first_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.eval() # we're only clustering, not training model\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i in tqdm(range(int(num_batches))):\n",
    "            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n",
    "            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n",
    "\n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            \n",
    "            tokens_unlabelled = tokens_unlabelled.to(current_device)\n",
    "            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n",
    "            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n",
    "            \n",
    "            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n",
    "            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n",
    "    \n",
    "            total_batch_loss = cluster_loss_labelled.data + cluster_loss_unlabelled.data\n",
    "            \n",
    "#             #Add loss to the epoch loss\n",
    "            total_epoch_loss += total_batch_loss.data\n",
    "\n",
    "#             # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_labelled.detach(), sentence_embed_labelled.detach())\n",
    "    \n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_unlabelled.detach(), sentence_embed_unlabelled.detach())\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n",
    "                    \"num_hidden_layers\":model.num_hidden_layers,\\\n",
    "                    \"hidden_size\":model.hidden_size,\\\n",
    "                    \"num_classes\":model.num_classes}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict_unlabelled.pt')\n",
    "            torch.save(centroids, path_to_save+'centroids_unlabelled')\n",
    "            torch.save(train_losses, path_to_save+'train_losses_unlabelled')\n",
    "            torch.save(val_losses, path_to_save+'val_losses_unlabelled')\n",
    "            torch.save(opts, path_to_save+'opts_unlabelled')\n",
    "    \n",
    "    \n",
    "    # unfreeze part\n",
    "    unfreeze_model(model)\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    \n",
    "    for epoch in range(num_second_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.eval() # we're only clustering, not training model\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i in tqdm(range(int(num_batches))):\n",
    "            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n",
    "            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n",
    "\n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            \n",
    "            tokens_unlabelled = tokens_unlabelled.to(current_device)\n",
    "            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n",
    "            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n",
    "            \n",
    "            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n",
    "            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n",
    "    \n",
    "            total_batch_loss = cluster_loss_labelled.data + cluster_loss_unlabelled.data\n",
    "            \n",
    "#             #Add loss to the epoch loss\n",
    "            total_epoch_loss += total_batch_loss.data\n",
    "\n",
    "#             # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_labelled.detach(), sentence_embed_labelled.detach())\n",
    "    \n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_unlabelled.detach(), sentence_embed_unlabelled.detach())\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n",
    "                    \"num_hidden_layers\":model.num_hidden_layers,\\\n",
    "                    \"hidden_size\":model.hidden_size,\\\n",
    "                    \"num_classes\":model.num_classes}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict_unlabelled.pt')\n",
    "            torch.save(centroids, path_to_save+'centroids_unlabelled')\n",
    "            torch.save(train_losses, path_to_save+'train_losses_unlabelled')\n",
    "            torch.save(val_losses, path_to_save+'val_losses_unlabelled')\n",
    "            torch.save(opts, path_to_save+'opts_unlabelled')\n",
    "        \n",
    "    return model, centroids, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pBet75ZPX-m"
   },
   "outputs": [],
   "source": [
    "unsupervised_model = model\n",
    "unsupervised_model.projection = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTFO2vp-PX-o"
   },
   "outputs": [],
   "source": [
    "centroids = centroid_init(2, 2*unsupervised_model.hidden_size, train_loader_labelled, unsupervised_model, current_device)\n",
    "criterion = KMeansCriterion(1).to(current_device)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xya2NiqcPX-q",
    "outputId": "59b3072e-c567-4e18-a242-ba8298f08e58",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 200])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "model_folder = 'lstm_unfrozen_model/'\n",
    "model_dir = path + '/models/' + model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3211"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = int(len(train_loader_unlabelled.dataset)/train_loader_unlabelled.batch_size)+1\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8813,
     "status": "error",
     "timestamp": 1573355511003,
     "user": {
      "displayName": "Eileen Cho",
      "photoUrl": "",
      "userId": "03381570147993013394"
     },
     "user_tz": 300
    },
    "id": "rgwMd27mPX-u",
    "outputId": "063ebc41-be3c-4474-d92c-7bd0680bb366",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-24 17:44:01.365323 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e8c0838d28423299db5256cdb21469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.211\n",
      "Average training loss at batch  1000 : 1.229\n",
      "Average training loss at batch  2000 : 1.669\n",
      "Average training loss at batch  3000 : 0.935\n",
      "\n",
      "Average training loss after epoch  0 : 2.681\n",
      "Average validation loss after epoch  0 : 1.249\n",
      "2019-11-24 17:48:02.945680 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55e023d10a747f28bfa36138680a41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.444\n",
      "Average training loss at batch  1000 : 1.188\n",
      "Average training loss at batch  2000 : 1.133\n",
      "Average training loss at batch  3000 : 1.461\n",
      "\n",
      "Average training loss after epoch  1 : 2.624\n",
      "Average validation loss after epoch  1 : 1.236\n",
      "2019-11-24 17:52:10.216997 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686e82fc0065462a885339d880049842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.528\n",
      "Average training loss at batch  1000 : 1.388\n",
      "Average training loss at batch  2000 : 1.062\n",
      "Average training loss at batch  3000 : 1.368\n",
      "\n",
      "Average training loss after epoch  2 : 2.621\n",
      "Average validation loss after epoch  2 : 1.234\n",
      "2019-11-24 17:56:25.354619 | Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56591bce2c804b2bac83ed45c0671d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.097\n",
      "Average training loss at batch  1000 : 1.392\n",
      "Average training loss at batch  2000 : 0.989\n",
      "Average training loss at batch  3000 : 1.688\n",
      "\n",
      "Average training loss after epoch  3 : 2.620\n",
      "Average validation loss after epoch  3 : 1.233\n",
      "2019-11-24 18:00:22.267057 | Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3980ae16b373446296777a353e93f448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.440\n",
      "Average training loss at batch  1000 : 1.132\n",
      "Average training loss at batch  2000 : 1.503\n",
      "Average training loss at batch  3000 : 1.156\n",
      "\n",
      "Average training loss after epoch  4 : 2.622\n",
      "Average validation loss after epoch  4 : 1.233\n",
      "2019-11-24 18:04:18.399286 | Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5ff01291824ed18c7a40f5abbdfe99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.154\n",
      "Average training loss at batch  1000 : 1.099\n",
      "Average training loss at batch  2000 : 1.376\n",
      "Average training loss at batch  3000 : 1.086\n",
      "\n",
      "Average training loss after epoch  5 : 2.621\n",
      "Average validation loss after epoch  5 : 1.233\n",
      "2019-11-24 18:08:19.281430 | Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7175f71e394dccaafdaab35d03a86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.191\n",
      "Average training loss at batch  1000 : 1.970\n",
      "Average training loss at batch  2000 : 0.830\n",
      "Average training loss at batch  3000 : 1.744\n",
      "\n",
      "Average training loss after epoch  6 : 2.620\n",
      "Average validation loss after epoch  6 : 1.233\n",
      "2019-11-24 18:12:17.534749 | Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a16802e7f242dea7c8bf270d3bb539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.584\n",
      "Average training loss at batch  1000 : 1.648\n",
      "Average training loss at batch  2000 : 1.219\n",
      "Average training loss at batch  3000 : 0.953\n",
      "\n",
      "Average training loss after epoch  7 : 2.622\n",
      "Average validation loss after epoch  7 : 1.233\n",
      "2019-11-24 18:16:10.946037 | Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4e51438c8c4872bde0dcb17cb96bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.351\n",
      "Average training loss at batch  1000 : 1.318\n",
      "Average training loss at batch  2000 : 1.343\n",
      "Average training loss at batch  3000 : 1.411\n",
      "\n",
      "Average training loss after epoch  8 : 2.620\n",
      "Average validation loss after epoch  8 : 1.233\n",
      "2019-11-24 18:20:20.535310 | Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4a46ab31454dd0bb9dd04d2fb3e7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 1.166\n",
      "Average training loss at batch  1000 : 1.241\n",
      "Average training loss at batch  2000 : 1.377\n",
      "Average training loss at batch  3000 : 1.470\n",
      "\n",
      "Average training loss after epoch  9 : 2.622\n",
      "Average validation loss after epoch  9 : 1.233\n"
     ]
    }
   ],
   "source": [
    "lstm_model, lstm_centroids, lstm_train_losses, lstm_val_losses = train_clusters(unsupervised_model, centroids, criterion, train_loader_labelled,train_loader_unlabelled, val_loader, num_epochs=15, num_batches=num_batches, path_to_save=model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_centroids, model_dir+'centroids_unlabelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Only needed for Kaggle\n",
    "\n",
    "# from IPython.display import FileLink, FileLinks \n",
    "# FileLinks('.') #lists all downloadable files on server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell will change for each model\n",
    "model_folder = 'lstm_model/'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "criterion = criterion.to(current_device)\n",
    "\n",
    "path = os.getcwd()\n",
    "model_dir = path + '/models/' + model_folder\n",
    "\n",
    "opts = torch.load(model_dir+'opts_labelled')\n",
    "model = LSTM_model(opts['embedding_matrix']) #change here depending on model\n",
    "model.load_state_dict(torch.load(model_dir+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\n",
    "model = model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples in val loader: 455\n",
      "Assigned to cluster 1: 407\n",
      "TP_rate: 0.9312039312039312\n",
      "FP_rate: 0.0687960687960688\n",
      "FN_rate: 0.4166666666666667\n",
      "TN_rate: 0.5833333333333334\n",
      "\n",
      "\n",
      "Accuracy: 0.7572686322686323\n",
      "Precision: 0.9312039312039312\n",
      "Recall: 0.6908704238189276\n",
      "F1 score: 0.7932327548617774\n"
     ]
    }
   ],
   "source": [
    "TP_cluster, FP_cluster=evaluation.main(model, None, val_loader, criterion, data_dir, current_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell will change for each model\n",
    "model_folder = 'lstm_model/'\n",
    "\n",
    "criterion = KMeansCriterion(1)\n",
    "criterion = criterion.to(current_device)\n",
    "\n",
    "path = os.getcwd()\n",
    "model_dir = path + '/models/' + model_folder\n",
    "\n",
    "opts = torch.load(model_dir+'opts_unlabelled')\n",
    "model = LSTM_model(opts['embedding_matrix']) #change here depending on model\n",
    "model.projection = nn.Identity()\n",
    "model.load_state_dict(torch.load(model_dir+'model_dict_unlabelled.pt',map_location=lambda storage, loc: storage))\n",
    "model = model.to(current_device)\n",
    "centroids = torch.load(model_dir+'centroids_unlabelled',map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples in val loader: 455\n",
      "Assigned to cluster 1: 376\n",
      "TP_rate: 0.9468085106382979\n",
      "FP_rate: 0.05319148936170213\n",
      "FN_rate: 0.5443037974683544\n",
      "TN_rate: 0.45569620253164556\n",
      "\n",
      "\n",
      "Accuracy: 0.7012523565849718\n",
      "Precision: 0.9468085106382979\n",
      "Recall: 0.6349679400343177\n",
      "F1 score: 0.7601491972539055\n"
     ]
    }
   ],
   "source": [
    "TP_cluster, FP_cluster=evaluation.main(model, centroids, val_loader, criterion, data_dir, current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TP_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FP_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FP_cluster[FP_cluster.original == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
