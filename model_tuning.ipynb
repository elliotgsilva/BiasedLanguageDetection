{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zno22FtJPX9z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from datasets import get_mnist_dataset, get_data_loader\n",
    "#from utils import *\n",
    "#from models import *\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "from generate_dataloaders import *\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaJEVd0wPX94"
   },
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vi6hPzadPX95"
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(train_filename,val_filename):\n",
    "    path = os.getcwd()\n",
    "    data_dir = path + '/data/'\n",
    "    train_dataloader = pkl.load(open(data_dir + train_filename,'rb'))\n",
    "    val_dataloader = pkl.load(open(data_dir + val_filename,'rb'))\n",
    "    return train_dataloader,val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nLzh007PX98"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yq-jDGFIPX99"
   },
   "outputs": [],
   "source": [
    "train_loader,val_loader = get_dataloaders('train_dataloader.p','val_dataloader.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tQEhYjtPX-A"
   },
   "outputs": [],
   "source": [
    "ground_truth_dataloader = pkl.load(open(data_dir + 'ground_truth_dataloader.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzz8lwNQPX-B",
    "outputId": "690cb77f-2525-4c5a-ea14-a162716e34d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "69-v6pTCPX-E"
   },
   "outputs": [],
   "source": [
    "## Scratchwork (IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbzHfEqEPX-F",
    "outputId": "e190e666-75a9-43f4-881c-307d2e18993a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(train_loader):\n",
    "    print(len(x[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6foV2y9kPX-H",
    "outputId": "1201dae2-e377-4311-c1af-3527d5b632c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5])\n",
      "tensor([[[1., 2., 3., 4., 5.],\n",
      "         [3., 3., 3., 3., 3.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [2., 1., 2., 1., 2.]],\n",
      "\n",
      "        [[0., 1., 0., 1., 0.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [2., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 2.]]])\n",
      "torch.Size([2])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "minibatch = torch.tensor([\n",
    "                            [[1,2,3,4,5],[3,3,3,3,3],[1,1,1,1,1],[2,1,2,1,2]],\n",
    "                            [[0,1,0,1,0],[1,1,1,1,1],[2,0,0,0,0],[0,0,0,0,2]]\n",
    "                         ], dtype=torch.float32)\n",
    "\n",
    "flagged_indices = torch.tensor([1,2])\n",
    "\n",
    "upweight_value = 10\n",
    "\n",
    "print(minibatch.shape)\n",
    "print(minibatch)\n",
    "\n",
    "print(flagged_indices.shape)\n",
    "print(flagged_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrRatzhRPX-J",
    "outputId": "283ee2e9-6cd0-439b-a2db-2996aeda85fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "2 4 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.,  3.,  4.,  5.],\n",
       "         [30., 30., 30., 30., 30.],\n",
       "         [ 1.,  1.,  1.,  1.,  1.],\n",
       "         [ 2.,  1.,  2.,  1.,  2.]],\n",
       "\n",
       "        [[ 0.,  1.,  0.,  1.,  0.],\n",
       "         [ 1.,  1.,  1.,  1.,  1.],\n",
       "         [20.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  2.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_tokens, emb_dim = minibatch.shape\n",
    "print(type(minibatch))\n",
    "minibatch[range(batch_size),flagged_indices,:] *= upweight_value\n",
    "print(batch_size, num_tokens, emb_dim)\n",
    "minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3-0cZqzPX-K",
    "outputId": "9b9e9298-fc39-4fa3-cb20-94fc22dee9bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6154, 2.6154, 2.7692, 2.7692, 2.9231],\n",
       "        [1.6154, 0.1538, 0.0769, 0.1538, 0.2308]])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch.sum(1) / (num_tokens + upweight_value - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TFiD86rvPX-M",
    "outputId": "1f4c46ab-5c61-40fc-d9cb-30256b33acdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(minibatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xyk-TsdPX-O"
   },
   "outputs": [],
   "source": [
    "embed = torch.tensor(np.array([[2,4,5,6],[1,3,45,7],[3,4,5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "paQ0ohcwPX-Q"
   },
   "outputs": [],
   "source": [
    "centers = torch.tensor(np.array(([2,3,4,5],[1,2,4,5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBKi1uXdPX-R",
    "outputId": "a5c9c793-4abb-4375-e0fb-13f56bef5718"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   3,    7],\n",
       "        [1686, 1686],\n",
       "        [   4,   10]])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((embed[:,None,:]-centers)**2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZ2VcvBzPX-T",
    "outputId": "f2369f2c-a0f3-4b62-aa0d-ffc4e9ef7c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_distances, cluster_assignments = torch.sum((embed[:,None,:]-centers)**2, 2).min(1)\n",
    "cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OU5oKfrGPX-V",
    "outputId": "ace2c69a-d28e-444a-bc38-b1feead074dc"
   },
   "outputs": [],
   "source": [
    "for i, (tokens, labels, flagged_indices) in enumerate(train_loader):\n",
    "    #print(tokens, labels, flagged_indices)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assts = torch.LongTensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
    "        0, 0, 0, 1, 0, 0, 0, 1])\n",
    "k = 2\n",
    "bin_counts = torch.bincount(cluster_assts,minlength=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16., 16.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_counts = bin_counts.type(torch.FloatTensor).to(current_device)\n",
    "bin_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvt6N9QCPX-X"
   },
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puweJhdxPX-Y"
   },
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8BZ-QhNPX-Z"
   },
   "outputs": [],
   "source": [
    "class neuralNetBow(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n",
    "    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n",
    "    def __init__(self, vocab_size, emb_dim, upweight=10):\n",
    "        super(neuralNetBow, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=2)\n",
    "        self.upweight = upweight\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        batch_size, num_tokens = tokens.shape\n",
    "        embedding = self.embed(tokens)\n",
    "#         print(embedding.shape) # below assumes \"batch_size x num_tokens x Emb_dim\" (VERIFY)\n",
    "        \n",
    "        # upweight by flagged_index\n",
    "#         print(type(embedding))\n",
    "        embedding[torch.LongTensor(range(batch_size)),flagged_index.type(torch.LongTensor),:] *= self.upweight\n",
    "        \n",
    "        # average across embeddings\n",
    "        embedding_ave = embedding.sum(1) / (num_tokens + self.upweight - 1)\n",
    "        \n",
    "        return embedding_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGsqcnEtPX-a"
   },
   "source": [
    "### Clustering Stuff (un-tailored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrgIYm8JPX-b"
   },
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self, lmbda):\n",
    "        super().__init__()\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def forward(self, embeddings, centroids):\n",
    "        distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "        cluster_distances, cluster_assignments = distances.min(1)\n",
    "        loss = self.lmbda * cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TJohK2aPX-d"
   },
   "outputs": [],
   "source": [
    "def centroid_init(k, d, dataloader, model, current_device):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = torch.zeros(k, d).to(current_device)\n",
    "    centroid_counts = torch.zeros(k).to(current_device)\n",
    "    for (tokens, labels, flagged_indices) in dataloader:\n",
    "        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n",
    "        cluster_assignments = labels.to(current_device)\n",
    "        \n",
    "        model.eval()\n",
    "        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n",
    "    \n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, sentence_embed.to(current_device))\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n",
    "    centroid_counts.add_(bin_counts)\n",
    "    \n",
    "    #np_cluster_assignments = cluster_assignments.to('cpu')\n",
    "    #np_counts = np.bincount(np_cluster_assignments.data.numpy(), minlength=k)\n",
    "    #centroid_counts.add_(torch.FloatTensor(np_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3wynM7fPX-h"
   },
   "source": [
    "### Training Function (un-tailored, needs alterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KglsYxPJPX-i"
   },
   "outputs": [],
   "source": [
    "def train_model(model, centroids, criterion, optimizer, train_loader, valid_loader, num_epochs=10, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "\n",
    "        # run one epoch of gradient descent on autoencoders wrt centroids\n",
    "        for i, (tokens, labels, flagged_indices) in tqdm(enumerate(train_loader)):\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids.detach())\n",
    "\n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            cluster_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add loss to the epoch loss\n",
    "            total_epoch_loss += cluster_loss.data\n",
    "\n",
    "            # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments, sentence_embed)\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = cluster_loss.data/len(tokens)\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            torch.save(model.state_dict(), path_to_save+'_dict_epoch'+str(epoch)+'.pt')\n",
    "            torch.save(centroids, path_to_save+'_centroids_epoch'+str(epoch))\n",
    "            torch.save(train_losses, path_to_save+'_train_losses')\n",
    "            torch.save(val_losses, path_to_save+'_val_losses')\n",
    "        \n",
    "    return model, centroids, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0kEtvbqPX-k"
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'vocab_size': 20000,\n",
    "    'emb_dim': 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pBet75ZPX-m"
   },
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "model = neuralNetBow(opts['vocab_size'], opts['emb_dim']).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTFO2vp-PX-o"
   },
   "outputs": [],
   "source": [
    "# model = neuralNetBow(opts['vocab_size'], opts['emb_dim'])\n",
    "centroids = centroid_init(2, opts['emb_dim'],ground_truth_dataloader, model, current_device)\n",
    "criterion = KMeansCriterion(1).to(current_device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xya2NiqcPX-q",
    "outputId": "59b3072e-c567-4e18-a242-ba8298f08e58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5393, -0.5985,  0.1128,  ..., -1.5948, -0.8000, -0.5975],\n",
       "        [-0.5791, -0.5462,  0.1542,  ..., -1.4489, -0.7182, -0.5470]],\n",
       "       grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1573355494600,
     "user": {
      "displayName": "Eileen Cho",
      "photoUrl": "",
      "userId": "03381570147993013394"
     },
     "user_tz": 300
    },
    "id": "2It2SvzjPX-s",
    "outputId": "5de6949c-7e9e-4ade-f222-5eb28f5347db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8813,
     "status": "error",
     "timestamp": 1573355511003,
     "user": {
      "displayName": "Eileen Cho",
      "photoUrl": "",
      "userId": "03381570147993013394"
     },
     "user_tz": 300
    },
    "id": "rgwMd27mPX-u",
    "outputId": "063ebc41-be3c-4474-d92c-7bd0680bb366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-13 18:15:20.566644 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1709c3bfa3ed4be8a52b2e340517dcf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 47.673\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-3db7d7e84877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"baseline_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-132-db53dc9a248b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, centroids, criterion, optimizer, train_loader, valid_loader, num_epochs, path_to_save, print_every)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mcluster_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#Add loss to the epoch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                     \u001b[0;31m# Use the max. for normalizing running avg. of gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseline_model, baseline_centroids, baseline_train_losses, baseline_val_losses = train_model(model, centroids, criterion, optimizer, train_loader, val_loader, num_epochs=5, path_to_save=\"baseline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model = neuralNetBow(opts['vocab_size'], opts['emb_dim'])\n",
    "baseline_model.load_state_dict(torch.load(data_dir+'baseline_model_dict_epoch9.pt',map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_centroids = torch.load(data_dir+'baseline_model_centroids_epoch9',map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, centroids, val_loader, num_examples=None):\n",
    "    model.eval()\n",
    "    token_list = []\n",
    "    cluster_assignment_list = []\n",
    "    flagged_index_list = []\n",
    "    original_label = []\n",
    "    for i, (tokens, labels, flagged_indices) in enumerate(val_loader):\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            # store in list\n",
    "            token_list+=tokens.tolist()\n",
    "            flagged_index_list+=flagged_indices.tolist()\n",
    "            cluster_assignment_list+=cluster_assignments.tolist()\n",
    "            original_label+=labels.tolist()\n",
    "            \n",
    "    return token_list, flagged_index_list, cluster_assignment_list, original_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list, index_list, cluster_assignment_list, original_label = evaluate_model(baseline_model, baseline_centroids,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5364"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cluster_assignment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pkl.load(open(data_dir+'dictionary.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(token_list,index_list,cluster_assignment_list,dictionary,original_label):\n",
    "    decoded_tokens = [' '.join(dictionary.decode_idx_seq(sent)) for sent in token_list]\n",
    "    reviews = [decoded for decoded in decoded_tokens]\n",
    "    flagged_words = [r.split()[i] for (r,i) in zip(reviews,index_list)]\n",
    "    reviews = [review.split('<pad>')[0] for review in reviews]\n",
    "    df_pred = pd.DataFrame({'review':reviews,'index':index_list,'flagged_word':flagged_words,\\\n",
    "                            'assignment':cluster_assignment_list,'original':original_label})\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = decode_predictions(token_list,index_list,cluster_assignment_list,dictionary,original_label)\n",
    "pred_pos = df_pred[df_pred.assignment==1]\n",
    "pred_not = df_pred[df_pred.assignment==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>index</th>\n",
       "      <th>flagged_word</th>\n",
       "      <th>assignment</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>very excellent professional cooperative and very skilled person has good level of progrming know and everything goes smooth very excellent customer care service</td>\n",
       "      <td>3</td>\n",
       "      <td>cooperative</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>very diligent and professional</td>\n",
       "      <td>1</td>\n",
       "      <td>diligent</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>read our brief carefully and worked with us to create a logo that we feel proud to start a new business with .</td>\n",
       "      <td>15</td>\n",
       "      <td>proud</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>[ 11 april , 2018 ] dedicated and reliable [ 11 april , 2018 ] very well communication and excellent professionalism</td>\n",
       "      <td>8</td>\n",
       "      <td>reliable</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>very diligent and fast to respond .</td>\n",
       "      <td>1</td>\n",
       "      <td>diligent</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5189</td>\n",
       "      <td>all of the work i am having him do is related to confidential submissions of intellectual property for medical device design and usage .</td>\n",
       "      <td>15</td>\n",
       "      <td>intellectual</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>great work with [ login to view url ] - did exactly what i was expecting and did it quickly .</td>\n",
       "      <td>0</td>\n",
       "      <td>great</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5284</td>\n",
       "      <td>one2nine rules !</td>\n",
       "      <td>1</td>\n",
       "      <td>rules</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5310</td>\n",
       "      <td>islam did an amazing job helping us to move our website orders from old php coded order forms into our more familiar filemaker solution .</td>\n",
       "      <td>5</td>\n",
       "      <td>helping</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5323</td>\n",
       "      <td>respectful and timely .</td>\n",
       "      <td>0</td>\n",
       "      <td>respectful</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                 review  \\\n",
       "53    very excellent professional cooperative and very skilled person has good level of progrming know and everything goes smooth very excellent customer care service    \n",
       "95    very diligent and professional                                                                                                                                      \n",
       "111   read our brief carefully and worked with us to create a logo that we feel proud to start a new business with .                                                      \n",
       "135   [ 11 april , 2018 ] dedicated and reliable [ 11 april , 2018 ] very well communication and excellent professionalism                                                \n",
       "146   very diligent and fast to respond .                                                                                                                                 \n",
       "...                                    ...                                                                                                                                \n",
       "5189  all of the work i am having him do is related to confidential submissions of intellectual property for medical device design and usage .                            \n",
       "5220  great work with [ login to view url ] - did exactly what i was expecting and did it quickly .                                                                       \n",
       "5284  one2nine rules !                                                                                                                                                    \n",
       "5310  islam did an amazing job helping us to move our website orders from old php coded order forms into our more familiar filemaker solution .                           \n",
       "5323  respectful and timely .                                                                                                                                             \n",
       "\n",
       "      index  flagged_word  assignment  original  \n",
       "53    3      cooperative   1          -1         \n",
       "95    1      diligent      1          -1         \n",
       "111   15     proud         1          -1         \n",
       "135   8      reliable      1          -1         \n",
       "146   1      diligent      1          -1         \n",
       "...  ..           ...     ..          ..         \n",
       "5189  15     intellectual  1          -1         \n",
       "5220  0      great         1          -1         \n",
       "5284  1      rules         1          -1         \n",
       "5310  5      helping       1          -1         \n",
       "5323  0      respectful    1          -1         \n",
       "\n",
       "[130 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>index</th>\n",
       "      <th>flagged_word</th>\n",
       "      <th>assignment</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>great to work with !</td>\n",
       "      <td>0</td>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>great expert on my azure web job .</td>\n",
       "      <td>1</td>\n",
       "      <td>expert</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>thanks kohei for a great job .</td>\n",
       "      <td>4</td>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>great job and nice to work with</td>\n",
       "      <td>3</td>\n",
       "      <td>nice</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>great quality and fantastic service , really helpful , thanks so much !</td>\n",
       "      <td>7</td>\n",
       "      <td>helpful</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5359</td>\n",
       "      <td>great work !</td>\n",
       "      <td>0</td>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>excellent to work with , understands your requirements and achieves the work very well .</td>\n",
       "      <td>0</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5361</td>\n",
       "      <td>great design , great experience , and was quick with revisions .</td>\n",
       "      <td>4</td>\n",
       "      <td>experience</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5362</td>\n",
       "      <td>it 's my second time working with lucy and she did an excellent job - she met the brief well to high client satisfaction .</td>\n",
       "      <td>12</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5363</td>\n",
       "      <td>more excellent work from scott</td>\n",
       "      <td>1</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5234 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           review  \\\n",
       "0     great to work with !                                                                                                          \n",
       "1     great expert on my azure web job .                                                                                            \n",
       "2     thanks kohei for a great job .                                                                                                \n",
       "3     great job and nice to work with                                                                                               \n",
       "4     great quality and fantastic service , really helpful , thanks so much !                                                       \n",
       "...                                                                        ...                                                      \n",
       "5359  great work !                                                                                                                  \n",
       "5360  excellent to work with , understands your requirements and achieves the work very well .                                      \n",
       "5361  great design , great experience , and was quick with revisions .                                                              \n",
       "5362  it 's my second time working with lucy and she did an excellent job - she met the brief well to high client satisfaction .    \n",
       "5363  more excellent work from scott                                                                                                \n",
       "\n",
       "      index flagged_word  assignment  original  \n",
       "0     0      great        0          -1         \n",
       "1     1      expert       0          -1         \n",
       "2     4      great        0          -1         \n",
       "3     3      nice         0          -1         \n",
       "4     7      helpful      0          -1         \n",
       "...  ..          ...     ..          ..         \n",
       "5359  0      great        0          -1         \n",
       "5360  0      excellent    0          -1         \n",
       "5361  4      experience   0          -1         \n",
       "5362  12     excellent    0          -1         \n",
       "5363  1      excellent    0          -1         \n",
       "\n",
       "[5234 rows x 5 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>index</th>\n",
       "      <th>flagged_word</th>\n",
       "      <th>assignment</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>thanks for the great work !</td>\n",
       "      <td>3</td>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>systematic chaos was great with my feedback and was always able to come up with new versions and variations that were consistent improvements to what i was looking for .</td>\n",
       "      <td>0</td>\n",
       "      <td>systematic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1958</td>\n",
       "      <td>jiang is an excellent freelancer .</td>\n",
       "      <td>3</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2418</td>\n",
       "      <td>great ideas , perfect work - always willing to do additional corrections and additional files .</td>\n",
       "      <td>0</td>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2669</td>\n",
       "      <td>julio is always very pleasant to work with , reliable and returned the translation ahead of schedule - would love to work with again !</td>\n",
       "      <td>4</td>\n",
       "      <td>pleasant</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4167</td>\n",
       "      <td>great design and great communications .</td>\n",
       "      <td>3</td>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                         review  \\\n",
       "379   thanks for the great work !                                                                                                                                                 \n",
       "862   systematic chaos was great with my feedback and was always able to come up with new versions and variations that were consistent improvements to what i was looking for .   \n",
       "1958  jiang is an excellent freelancer .                                                                                                                                          \n",
       "2418  great ideas , perfect work - always willing to do additional corrections and additional files .                                                                             \n",
       "2669  julio is always very pleasant to work with , reliable and returned the translation ahead of schedule - would love to work with again !                                      \n",
       "4167  great design and great communications .                                                                                                                                     \n",
       "\n",
       "      index flagged_word  assignment  original  \n",
       "379   3      great        0           0         \n",
       "862   0      systematic   0           0         \n",
       "1958  3      excellent    0           1         \n",
       "2418  0      great        0           0         \n",
       "2669  4      pleasant     0           1         \n",
       "4167  3      great        0           0         "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check=df_pred[df_pred.original!=-1]\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
