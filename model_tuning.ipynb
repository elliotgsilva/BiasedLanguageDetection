{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from datasets import get_mnist_dataset, get_data_loader\n",
    "#from utils import *\n",
    "#from models import *\n",
    "\n",
    "import pickle as pkl\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensoredDataset(Dataset):\n",
    "    def __init__(self, list_of_lists_of_tokens, list_of_flagged_indexes, list_of_problematic_flags):\n",
    "        self.input_tensors = []\n",
    "        self.flagged_index = []\n",
    "        self.problematic = []\n",
    "        \n",
    "        for sample in list_of_lists_of_tokens:\n",
    "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
    "        for sample in list_of_flagged_indexes:\n",
    "            self.flagged_index.append(torch.tensor(sample, dtype=torch.long))\n",
    "        for sample in list_of_problematic_flags:\n",
    "            self.problematic.append(torch.tensor(sample, dtype=torch.long))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx], self.flagged_index[idx], self.problematic[idx])\n",
    "    \n",
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = 100\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:    \n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor[:max_length])\n",
    "        \n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = 2\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    \n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_filename,val_filename):\n",
    "    path = os.getcwd()\n",
    "    data_dir = path + '/data/'\n",
    "    train_dataloader = pkl.load(open(data_dir + train_filename,'rb'))\n",
    "    val_dataloader = pkl.load(open(data_dir + val_filename,'rb'))\n",
    "    return train_dataloader,val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader,val_dataloader = get_dataloaders('train_dataloader.p','val_dataloader.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchwork (IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.Tensor([[0, 1, 2, 3],[4,5,6,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  6.,  8., 10.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetBow(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    # NOTE: in baseline model, can't do linear layers, because they will remember certain\n",
    "    # positions as being more important than others (ie, 4th word vs 7th word)\n",
    "    def __init__(self, vocab_size, emb_dim, upweight=10):\n",
    "        super(neuralNetBow, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.upweight = upweight\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        num_tokens = len(tokens)\n",
    "        embedding = self.embed(tokens)\n",
    "        # print(embedding.shape) # below assumes \"num_tokens x emb_dim\" (VERIFY)\n",
    "        \n",
    "        # upweight by flagged_index\n",
    "        embedding[:,flagged_index] *= self.upweight\n",
    "        \n",
    "        # average across embeddings\n",
    "        embedding_ave = embedding.sum(0) / (num_tokens + self.upweight - 1)\n",
    "        \n",
    "        return embedding_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Stuff (un-tailored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self, lmbda):\n",
    "        super().__init__()\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def forward(self, embeddings, centroids):\n",
    "        distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "        cluster_distances, cluster_assignments = distances.max(1)\n",
    "        loss = self.lmbda * cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_init(k, d):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = Variable(torch.zeros(k, d))\n",
    "    centroid_counts = Variable(torch.zeros(k))\n",
    "    for X, y in trainloader:\n",
    "        X_var, y_var = Variable(X), Variable(y)\n",
    "        cluster_assignments = Variable(torch.LongTensor(X.size(0)).random_(k))\n",
    "        embeddings = encoder(X_var)\n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, embeddings)\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None]\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    np_counts = np.bincount(cluster_assignments.data.numpy(), minlength=k)\n",
    "    centroid_counts.add_(Variable(torch.FloatTensor(np_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function (un-tailored, needs alterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, centroids, optimizer, criterion,\n",
    "          print_every=100, verbose=False):\n",
    "    k, d = centroids.size()\n",
    "    centroid_sums = torch.zeros_like(centroids)\n",
    "    centroid_counts = Variable(torch.zeros(k))\n",
    "    \n",
    "    # run one epoch of gradient descent on autoencoders wrt centroids\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "        \n",
    "        # forward pass and compute loss\n",
    "        X_var, y_var = Variable(X), Variable(y)\n",
    "        embeddings = encoder(X_var)\n",
    "        X_hat = decoder(embeddings)\n",
    "        recon_loss = F.mse_loss(X_hat, X_var)\n",
    "        cluster_loss, cluster_assignments = criterion(embeddings, centroids)\n",
    "        loss = recon_loss + cluster_loss\n",
    "        \n",
    "        # run update step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # store centroid sums and counts in memory for later centering\n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, embeddings)\n",
    "        \n",
    "        if verbose and i % print_every == 0:\n",
    "            batch_hat = autoencoder(Variable(batch))\n",
    "            plot_batch(batch_hat.data)\n",
    "            losses = (loss.data[0], recon_loss.data[0], cluster_loss.data[0])\n",
    "            print('Trn Loss: %.3f [Recon Loss %.3f, Cluster Loss %.3f]' % losses)\n",
    "    \n",
    "    # update centroids based on assignments from autoencoders\n",
    "    centroid_means = centroid_sums / (centroid_counts[:, None] + 1)\n",
    "    return centroid_means, centroid_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
