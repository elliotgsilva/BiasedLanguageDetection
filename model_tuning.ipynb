{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from datasets import get_mnist_dataset, get_data_loader\n",
    "#from utils import *\n",
    "#from models import *\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "from generate_dataloaders import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_filename,val_filename):\n",
    "    path = os.getcwd()\n",
    "    data_dir = path + '/data/'\n",
    "    train_dataloader = pkl.load(open(data_dir + train_filename,'rb'))\n",
    "    val_dataloader = pkl.load(open(data_dir + val_filename,'rb'))\n",
    "    return train_dataloader,val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,val_loader = get_dataloaders('train_dataloader.p','val_dataloader.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchwork (IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, x in enumerate(train_loader):\n",
    "#     print(x.shape)\n",
    "#     print(x)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 4, 5])\n",
      "tensor([[[1., 2., 3., 4., 5.],\n",
      "         [3., 3., 3., 3., 3.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [2., 1., 2., 1., 2.]],\n",
      "\n",
      "        [[0., 1., 0., 1., 0.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [2., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 2.]]])\n",
      "torch.Size([2])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "minibatch = torch.tensor([\n",
    "                            [[1,2,3,4,5],[3,3,3,3,3],[1,1,1,1,1],[2,1,2,1,2]],\n",
    "                            [[0,1,0,1,0],[1,1,1,1,1],[2,0,0,0,0],[0,0,0,0,2]]\n",
    "                         ], dtype=torch.float32)\n",
    "\n",
    "flagged_indices = torch.tensor([1,2])\n",
    "\n",
    "upweight_value = 10\n",
    "\n",
    "print(minibatch.shape)\n",
    "print(minibatch)\n",
    "\n",
    "print(flagged_indices.shape)\n",
    "print(flagged_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "2 4 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00],\n",
       "         [3.0000e+06, 3.0000e+06, 3.0000e+06, 3.0000e+06, 3.0000e+06],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
       "         [2.0000e+00, 1.0000e+00, 2.0000e+00, 1.0000e+00, 2.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
       "         [2.0000e+06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_tokens, emb_dim = minibatch.shape\n",
    "print(type(minibatch))\n",
    "minibatch[range(batch_size),flagged_indices,:] *= upweight_value\n",
    "print(batch_size, num_tokens, emb_dim)\n",
    "minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6154, 2.6154, 2.7692, 2.7692, 2.9231],\n",
       "        [1.6154, 0.1538, 0.0769, 0.1538, 0.2308]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch.sum(1) / (num_tokens + upweight_value - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(minibatch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetBow(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n",
    "    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n",
    "    def __init__(self, vocab_size, emb_dim, upweight=10):\n",
    "        super(neuralNetBow, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=2)\n",
    "        self.upweight = upweight\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        batch_size, num_tokens = tokens.shape\n",
    "        embedding = self.embed(tokens)\n",
    "        # print(embedding.shape) # below assumes \"batch_size x num_tokens\" (VERIFY)\n",
    "        \n",
    "        # upweight by flagged_index\n",
    "        print(type(embedding))\n",
    "        embedding[torch.LongTensor(range(batch_size)),flagged_index.type(torch.LongTensor),:] *= self.upweight\n",
    "        \n",
    "        # average across embeddings\n",
    "        embedding_ave = embedding.sum(1) / (num_tokens + self.upweight - 1)\n",
    "        \n",
    "        return embedding_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Stuff (un-tailored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self, lmbda):\n",
    "        super().__init__()\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def forward(self, embeddings, centroids):\n",
    "        distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "        cluster_distances, cluster_assignments = distances.max(1)\n",
    "        loss = self.lmbda * cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-75-19b691d91e82>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-75-19b691d91e82>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    cluster_assignments = Variable(torch.LongT ensor(X.size(0)).random_(k))\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def centroid_init(k, d):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = Variable(torch.zeros(k, d))\n",
    "    centroid_counts = Variable(torch.zeros(k))\n",
    "    for X in trainloader:\n",
    "        X_var = Variable(X)\n",
    "        cluster_assignments = Variable(torch.LongT ensor(X.size(0)).random_(k))\n",
    "        embeddings = encoder(X_var)\n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, embeddings)\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None]\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    np_counts = np.bincount(cluster_assignments.data.numpy(), minlength=k)\n",
    "    centroid_counts.add_(Variable(torch.FloatTensor(np_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function (un-tailored, needs alterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_model(model, centroids, criterion, optimizer, train_loader, valid_loader, num_epochs=10, path_to_save=path+'baseline_model.pt')\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=10):\n",
    "\n",
    "#     k, d = centroids.size()\n",
    "#     centroid_sums = torch.zeros_like(centroids)\n",
    "#     centroid_counts = Variable(torch.zeros(k))\n",
    "    \n",
    "    # run one epoch of gradient descent on autoencoders wrt centroids\n",
    "    for i, (tokens, flagged_indices, problematics) in enumerate(train_loader):\n",
    "        \n",
    "        print(tokens.shape)\n",
    "        print(flagged_indices.shape)\n",
    "        print(problematics.shape)\n",
    "        \n",
    "        sentence_embed = model(tokens,flagged_indices)\n",
    "        print(sentence_embed)\n",
    "        break\n",
    "        \n",
    "        # forward pass and compute loss\n",
    "        X_var = Variable(X)\n",
    "        embeddings = encoder(X_var)\n",
    "        X_hat = decoder(embeddings)\n",
    "        recon_loss = F.mse_loss(X_hat, X_var)\n",
    "        cluster_loss, cluster_assignments = criterion(embeddings, centroids)\n",
    "        loss = recon_loss + cluster_loss\n",
    "        \n",
    "        # run update step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # store centroid sums and counts in memory for later centering\n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, embeddings)\n",
    "        \n",
    "        if verbose and i % print_every == 0:\n",
    "            batch_hat = autoencoder(Variable(batch))\n",
    "            plot_batch(batch_hat.data)\n",
    "            losses = (loss.data[0], recon_loss.data[0], cluster_loss.data[0])\n",
    "            print('Trn Loss: %.3f [Recon Loss %.3f, Cluster Loss %.3f]' % losses)\n",
    "        \n",
    "    # update centroids based on assignments from autoencoders\n",
    "#     centroid_means = centroid_sums / (centroid_counts[:, None] + 1)\n",
    "#     return centroid_means, centroid_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'vocab_size': 20000,\n",
    "    'emb_dim': 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neuralNetBow(opts['vocab_size'], opts['emb_dim'])\n",
    "#centroids = centroid_init(2, opts['emb_dim'])\n",
    "criterion = KMeansCriterion(1)\n",
    "#optimizer = torch.optim.Adam(chat_model.parameters(), 0.01, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[ 0.0876, -0.1145, -0.0112,  ..., -0.2560, -0.0780,  0.6067],\n",
      "        [ 0.1431, -0.5604,  0.0333,  ...,  0.7312, -0.4666,  0.3559],\n",
      "        [ 0.0825, -0.4727,  0.1372,  ...,  0.4152, -0.3197,  0.3115],\n",
      "        ...,\n",
      "        [-0.1941,  0.2782, -0.0242,  ...,  0.2799, -0.0813, -0.2174],\n",
      "        [ 0.2576, -0.3327,  0.1966,  ...,  0.2630, -0.6485,  0.3384],\n",
      "        [-0.6450, -0.3033,  0.0216,  ..., -0.0927,  0.1854, -0.1138]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2])\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([2])\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
