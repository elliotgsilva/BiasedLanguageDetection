{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from datasets import get_mnist_dataset, get_data_loader\n",
    "#from utils import *\n",
    "#from models import *\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "from generate_dataloaders import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_filename,val_filename):\n",
    "    path = os.getcwd()\n",
    "    data_dir = path + '/data/'\n",
    "    train_dataloader = pkl.load(open(data_dir + train_filename,'rb'))\n",
    "    val_dataloader = pkl.load(open(data_dir + val_filename,'rb'))\n",
    "    return train_dataloader,val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,val_loader = get_dataloaders('train_dataloader.p','val_dataloader.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_dataloader = pkl.load(open(data_dir + 'ground_truth_dataloader.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchwork (IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(train_loader):\n",
    "    print(len(x[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5])\n",
      "tensor([[[1., 2., 3., 4., 5.],\n",
      "         [3., 3., 3., 3., 3.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [2., 1., 2., 1., 2.]],\n",
      "\n",
      "        [[0., 1., 0., 1., 0.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [2., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 2.]]])\n",
      "torch.Size([2])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "minibatch = torch.tensor([\n",
    "                            [[1,2,3,4,5],[3,3,3,3,3],[1,1,1,1,1],[2,1,2,1,2]],\n",
    "                            [[0,1,0,1,0],[1,1,1,1,1],[2,0,0,0,0],[0,0,0,0,2]]\n",
    "                         ], dtype=torch.float32)\n",
    "\n",
    "flagged_indices = torch.tensor([1,2])\n",
    "\n",
    "upweight_value = 10\n",
    "\n",
    "print(minibatch.shape)\n",
    "print(minibatch)\n",
    "\n",
    "print(flagged_indices.shape)\n",
    "print(flagged_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "2 4 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.,  3.,  4.,  5.],\n",
       "         [30., 30., 30., 30., 30.],\n",
       "         [ 1.,  1.,  1.,  1.,  1.],\n",
       "         [ 2.,  1.,  2.,  1.,  2.]],\n",
       "\n",
       "        [[ 0.,  1.,  0.,  1.,  0.],\n",
       "         [ 1.,  1.,  1.,  1.,  1.],\n",
       "         [20.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  2.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_tokens, emb_dim = minibatch.shape\n",
    "print(type(minibatch))\n",
    "minibatch[range(batch_size),flagged_indices,:] *= upweight_value\n",
    "print(batch_size, num_tokens, emb_dim)\n",
    "minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6154, 2.6154, 2.7692, 2.7692, 2.9231],\n",
       "        [1.6154, 0.1538, 0.0769, 0.1538, 0.2308]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch.sum(1) / (num_tokens + upweight_value - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(minibatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.tensor(np.array([[2,4,5,6],[1,3,45,7],[3,4,5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = torch.tensor(np.array(([2,3,4,5],[1,2,4,5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   3,    7],\n",
       "        [1686, 1686],\n",
       "        [   4,   10]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((embed[:,None,:]-centers)**2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_distances, cluster_assignments = torch.sum((embed[:,None,:]-centers)**2, 2).min(1)\n",
    "cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   34,    19,  3042,   165,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1901,   257,   171,   664,    85,    41,  2684,    20,   519,   182,\n",
      "            41,   643,   214,   170,    30,    41,   958,  1789,   214,  8043,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 4418,    69,    60,   518,    11,    20,   216,    10,    84,     7,\n",
      "            18,  4419,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   68,   302,    60,  1207,  2449,   135,   398,    52,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  360,    20,    78,    20,   532,     7,   435,    44,     6,   135,\n",
      "         12914,    21,  5020,   361,    76,   135,   227,   427,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [10740,   156,    77,   207,     7,    77,   333,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   10,   784,    20,    34,    84,     7,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  165,   166,    20,   996,     7,  1331,    41,  1241,    29,    95,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  210,   880,     7,   165,    19,   121,   294,   194,   117, 13646,\n",
      "           227,   484,    60,   572,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   39,   302,    60,   609,   167,   876,    85,    71,   140,    34,\n",
      "           170,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   34,    19,    20,   401,  2722,   556,  2775,   294,    41,    16,\n",
      "           214,    19,   156,    34,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   37,    83,     7,    77,   210,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1351,   383,    69,    34,     7,    77,   333,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   60,    34,    36,     7,   113,    14,    20,   157,   246,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   10,    38,    20,    30,    77,   250,   879,    95,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 7309,    69,    34,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   34,    38,   123,    77,    27,  1917,   123,    17,    77,   210,\n",
      "           166,    25,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    3,   177,   593, 14301,    44,   759,    41,    38,   246,     7,\n",
      "           246,   157,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  365,   646,    20,    34,    51,    44,    52,    20,   142,  3038,\n",
      "           214,    36,   258,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   68,   302,    60,  1964,     7,  1597,   234,   214,  3341,     7,\n",
      "            41,  3342,    44,  1123,    41,   212,   156,    10,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  636,   366,  3407,    73,    99,   171,  1613,   113,   152,   241,\n",
      "           115,   214,   171,   412,   136,   139,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   34,   170,    20,    84,     7,  7956,    83,    29,    21,    22,\n",
      "            19,    30,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   10,    44,    19,    30,   123,    29,    21,    42,  1169,    24,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   73,    20,   310,     7,   333,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   34,    19,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  247,   165,    20,    77,   289,   214,   558,    20,    34,    30,\n",
      "           937,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   29,   117,   118,   171,    28,     4,  1309,   914,    20,     7,\n",
      "            29,   394,   163,   318,    30,    41,    19,   246,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   77,   230,     7,  1346,    44,    19,    30,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   34,    19,    20,     7,   210,   196,  1271,   741,  2468,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  163,   135,   167,     7,   163,   512,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   29,   156,   485,     4,   145,  9716,  9717,   464,     4,   199,\n",
      "           297,    20,   171,  6756,  1214,   214,    41,    36,   156,  9718,\n",
      "           356,  1352,   124,   214,    41,   601,   112,     0,     0,     0],\n",
      "        [   66,    19,  2057,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]) tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]) tensor([ 0,  3,  7,  3,  6,  3,  0,  3,  1,  3, 13,  5,  1,  1,  0,  2,  0,  2,\n",
      "         3, 17,  3,  0,  0,  0,  0,  4,  4,  1,  0,  5, 13,  0])\n"
     ]
    }
   ],
   "source": [
    "for i, (tokens, labels, flagged_indices) in enumerate(train_loader):\n",
    "    print(tokens, labels, flagged_indices)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetBow(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n",
    "    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n",
    "    def __init__(self, vocab_size, emb_dim, upweight=10):\n",
    "        super(neuralNetBow, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=2)\n",
    "        self.upweight = upweight\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        batch_size, num_tokens = tokens.shape\n",
    "        embedding = self.embed(tokens)\n",
    "#         print(embedding.shape) # below assumes \"batch_size x num_tokens x Emb_dim\" (VERIFY)\n",
    "        \n",
    "        # upweight by flagged_index\n",
    "#         print(type(embedding))\n",
    "        embedding[torch.LongTensor(range(batch_size)),flagged_index.type(torch.LongTensor),:] *= self.upweight\n",
    "        \n",
    "        # average across embeddings\n",
    "        embedding_ave = embedding.sum(1) / (num_tokens + self.upweight - 1)\n",
    "        \n",
    "        return embedding_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Stuff (un-tailored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self, lmbda):\n",
    "        super().__init__()\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def forward(self, embeddings, centroids):\n",
    "        distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "        cluster_distances, cluster_assignments = distances.min(1)\n",
    "        loss = self.lmbda * cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_init(k, d, dataloader, model):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = torch.zeros(k, d)\n",
    "    centroid_counts = torch.zeros(k)\n",
    "    for (tokens, labels, flagged_indices) in dataloader:\n",
    "#         cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n",
    "        cluster_assignments = labels\n",
    "        \n",
    "        model.eval()\n",
    "        sentence_embed = model(tokens,flagged_indices)\n",
    "    \n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, sentence_embed)\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None]\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    \n",
    "    np_counts = np.bincount(cluster_assignments.data.numpy(), minlength=k)\n",
    "    centroid_counts.add_(torch.FloatTensor(np_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function (un-tailored, needs alterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, centroids, criterion, optimizer, train_loader, valid_loader, num_epochs=10, path_to_save=None, print_every = 100):\n",
    "\n",
    "    model.train()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids)\n",
    "        centroid_counts = torch.zeros(k)\n",
    "        total_epoch_loss = 0\n",
    "\n",
    "        # run one epoch of gradient descent on autoencoders wrt centroids\n",
    "        for i, (tokens, labels, flagged_indices) in tqdm(enumerate(train_loader)):\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "\n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            cluster_loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add loss to the epoch loss\n",
    "            total_epoch_loss += cluster_loss.data\n",
    "\n",
    "            # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments, sentence_embed)\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = cluster_loss.data/len(tokens)\n",
    "                print('Average training Loss at batch',i,': %.3f' % losses)\n",
    "                \n",
    "            \n",
    "        total_epoch_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        print('Average training Loss after epoch',epoch,': %.3f' % total_epoch_loss)\n",
    "\n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1)\n",
    "        \n",
    "    if path_to_save == None:\n",
    "        pass\n",
    "    else:\n",
    "        torch.save(model.state_dict(), path_to_save)\n",
    "        \n",
    "    return model, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'vocab_size': 20000,\n",
    "    'emb_dim': 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "model = neuralNetBow(opts['vocab_size'], opts['emb_dim']).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = neuralNetBow(opts['vocab_size'], opts['emb_dim'])\n",
    "centroids = centroid_init(2, opts['emb_dim'],ground_truth_dataloader, model)\n",
    "criterion = KMeansCriterion(1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4438, -0.0525,  0.1054,  ...,  0.1595,  0.1759,  0.5402],\n",
       "        [ 0.3969, -0.0488,  0.1069,  ...,  0.2194,  0.1766,  0.5496]],\n",
       "       grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:00,  2.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training Loss at batch 0 : 36.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2it [00:00,  2.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3it [00:01,  2.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4it [00:01,  2.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5it [00:02,  2.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6it [00:02,  2.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7it [00:03,  2.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8it [00:03,  2.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9it [00:03,  2.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10it [00:04,  2.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11it [00:04,  2.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12it [00:05,  2.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13it [00:05,  2.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14it [00:06,  2.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15it [00:06,  2.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-63b65fb17a99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-166-dd4c3cba39a2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, centroids, criterion, optimizer, train_loader, valid_loader, num_epochs, path_to_save, print_every)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# run update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mcluster_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, centroids, criterion, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2])\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([2])\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
