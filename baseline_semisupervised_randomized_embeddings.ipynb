{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## KAGGLE ONLY\n",
    "# from shutil import copyfile\n",
    "# copyfile(src=\"../input/inputs/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\n",
    "# copyfile(src=\"../input/inputs/train_dataloader.p\", dst=\"../working/train_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/val_dataloader.p\", dst=\"../working/val_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/centroids_dataloader.p\", dst=\"../working/ground_truth_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/dictionary.p\", dst=\"../working/dictionary.p\")\n",
    "\n",
    "# copyfile(src=\"../input/input2/train_unlabeld_dataloader.p\", dst=\"../working/train_unlabelled_dataloader.p\")\n",
    "# copyfile(src=\"../input/input2/train_labeled_dataloader.p\", dst=\"../working/train_labelled_dataloader.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zno22FtJPX9z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation' from '/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/evaluation.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from datasets import get_mnist_dataset, get_data_loader\n",
    "#from utils import *\n",
    "#from models import *\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from generate_dataloaders import *\n",
    "\n",
    "from generate_dataloaders import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import evaluation\n",
    "import importlib\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaJEVd0wPX94"
   },
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1029\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nLzh007PX98"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yq-jDGFIPX99"
   },
   "outputs": [],
   "source": [
    "train_loader = pkl.load(open(data_dir + 'train_dataloader.p','rb'))\n",
    "train_loader_labelled = pkl.load(open(data_dir + 'train_labeled_dataloader.p','rb'))\n",
    "train_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabeled_dataloader.p','rb'))\n",
    "val_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install pytorch torchvision -c pytorch\n",
    "## if torch.__version__ is not 1.3.1, run this cell then restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzz8lwNQPX-B",
    "outputId": "690cb77f-2525-4c5a-ea14-a162716e34d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvt6N9QCPX-X"
   },
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puweJhdxPX-Y"
   },
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8BZ-QhNPX-Z"
   },
   "outputs": [],
   "source": [
    "class neuralNetBow(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n",
    "    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n",
    "    def __init__(self, opts):\n",
    "        super(neuralNetBow, self).__init__()\n",
    "        self.embed = nn.Embedding(opts['vocab_size'], opts['emb_dim'], padding_idx=0)\n",
    "        self.upweight = opts['upweight']\n",
    "        self.lambda_loss = opts['lambda_loss']\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        batch_size, num_tokens = tokens.shape\n",
    "        embedding = self.embed(tokens)\n",
    "        \n",
    "        # upweight by flagged_index\n",
    "        embedding[torch.LongTensor(range(batch_size)),flagged_index.type(torch.LongTensor),:] *= self.upweight\n",
    "        \n",
    "        # average across embeddings\n",
    "        embedding_ave = embedding.sum(1) / (num_tokens + self.upweight - 1)\n",
    "        \n",
    "        return embedding_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGsqcnEtPX-a"
   },
   "source": [
    "### Clustering Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrgIYm8JPX-b"
   },
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n",
    "        if labelled:\n",
    "            num_reviews = len(cluster_assignments)\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n",
    "            loss = cluster_distances.sum()\n",
    "        else:\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances, cluster_assignments = distances.min(1)\n",
    "            loss = cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TJohK2aPX-d"
   },
   "outputs": [],
   "source": [
    "def centroid_init(k, d, dataloader, model, current_device):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = torch.zeros(k, d).to(current_device)\n",
    "    centroid_counts = torch.zeros(k).to(current_device)\n",
    "    for (tokens, labels, flagged_indices) in dataloader:\n",
    "        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n",
    "        cluster_assignments = labels.to(current_device)\n",
    "        \n",
    "        model.eval()\n",
    "        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n",
    "    \n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, sentence_embed.to(current_device))\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n",
    "    centroid_counts.add_(bin_counts)\n",
    "    \n",
    "    #np_cluster_assignments = cluster_assignments.to('cpu')\n",
    "    #np_counts = np.bincount(np_cluster_assignments.data.numpy(), minlength=k)\n",
    "    #centroid_counts.add_(torch.FloatTensor(np_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_labelled_iter\n",
    "\n",
    "\n",
    "def loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_unlabelled_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3wynM7fPX-h"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KglsYxPJPX-i"
   },
   "outputs": [],
   "source": [
    "def train_model(model, centroids, criterion, optimizer, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=10, num_batches = 1000, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "    lambda_loss = model.lambda_loss\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n",
    "            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n",
    "\n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            \n",
    "            tokens_unlabelled = tokens_unlabelled.to(current_device)\n",
    "            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n",
    "            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n",
    "            \n",
    "            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n",
    "            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n",
    "            \n",
    "            total_batch_loss = cluster_loss_unlabelled + lambda_loss * cluster_loss_labelled\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add loss to the epoch loss\n",
    "            total_epoch_loss += total_batch_loss\n",
    "\n",
    "            # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments_labelled, sentence_embed_labelled)\n",
    "    \n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments_unlabelled, sentence_embed_unlabelled)\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            sub_folder = \"/\"\n",
    "            path_to_save+=sub_folder\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict.pt')\n",
    "            torch.save(centroids, path_to_save+'centroids')\n",
    "            torch.save(train_losses, path_to_save+'train_losses')\n",
    "            torch.save(val_losses, path_to_save+'val_losses')\n",
    "            torch.save(opts, path_to_save+'opts') #change options depending on model inputs required\n",
    "        \n",
    "    return model, centroids, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pBet75ZPX-m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cpu\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "print(\"Current Device:\",current_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_directory(opts):\n",
    "    path = os.getcwd()\n",
    "    model_folder = 'baseline_semisupervised_randomized_embeddings/'\n",
    "    model_dir = path + '/models/' + model_folder\n",
    "    \n",
    "    # subfolder for each hyperparam config\n",
    "    emb_dim = opts['emb_dim']\n",
    "    upweight = opts['upweight']\n",
    "    lambda_loss = opts['lambda_loss']\n",
    "    subfolder = \"emb_dim=\"+str(emb_dim) + \",upweight=\"+str(upweight) + \",lambda=\"+str(lambda_loss) + '/'\n",
    "    \n",
    "    # need to actually create these subfolders lol\n",
    "    try:\n",
    "        os.makedirs(model_dir + subfolder) # will throw error if subfolder already exists\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return model_dir + subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_config(opts):\n",
    "    model = neuralNetBow(opts).to(current_device)\n",
    "    centroids = centroid_init(2, opts['emb_dim'],train_loader_labelled, model, current_device)\n",
    "    criterion = KMeansCriterion().to(current_device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    print(path_to_save)\n",
    "    \n",
    "    train_model(model, centroids, criterion, optimizer, train_loader_labelled, train_loader_unlabelled, val_loader, num_epochs=10, path_to_save=path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_randomized_embeddings/emb_dim=256,upweight=10,lambda=0.5/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7ff4ec6d0a4a9aa75675e3502136d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-07 19:53:54.986322 | Epoch 0\n",
      "Average training loss at batch  0 : 11.760\n",
      "Average training loss after epoch  0 : 1.019\n",
      "Average validation loss after epoch  0 : 0.542\n",
      "2019-12-07 19:54:44.977095 | Epoch 1\n",
      "Average training loss at batch  0 : 0.189\n",
      "Average training loss after epoch  1 : 0.118\n",
      "Average validation loss after epoch  1 : 0.245\n",
      "2019-12-07 19:55:40.888111 | Epoch 2\n",
      "Average training loss at batch  0 : 0.077\n",
      "Average training loss after epoch  2 : 0.059\n",
      "Average validation loss after epoch  2 : 0.143\n",
      "2019-12-07 19:56:44.001080 | Epoch 3\n",
      "Average training loss at batch  0 : 0.052\n",
      "Average training loss after epoch  3 : 0.033\n",
      "Average validation loss after epoch  3 : 0.115\n",
      "2019-12-07 19:57:52.560153 | Epoch 4\n",
      "Average training loss at batch  0 : 0.032\n",
      "Average training loss after epoch  4 : 0.026\n",
      "Average validation loss after epoch  4 : 0.092\n",
      "2019-12-07 19:58:57.721604 | Epoch 5\n",
      "Average training loss at batch  0 : 0.030\n",
      "Average training loss after epoch  5 : 0.021\n",
      "Average validation loss after epoch  5 : 0.081\n",
      "2019-12-07 20:00:02.820665 | Epoch 6\n",
      "Average training loss at batch  0 : 0.026\n",
      "Average training loss after epoch  6 : 0.015\n",
      "Average validation loss after epoch  6 : 0.069\n",
      "2019-12-07 20:01:07.948580 | Epoch 7\n",
      "Average training loss at batch  0 : 0.015\n",
      "Average training loss after epoch  7 : 0.012\n",
      "Average validation loss after epoch  7 : 0.064\n",
      "2019-12-07 20:02:14.536377 | Epoch 8\n",
      "Average training loss at batch  0 : 0.012\n",
      "Average training loss after epoch  8 : 0.011\n",
      "Average validation loss after epoch  8 : 0.060\n",
      "2019-12-07 20:03:23.291995 | Epoch 9\n",
      "Average training loss at batch  0 : 0.017\n"
     ]
    }
   ],
   "source": [
    "emb_dims = [256]\n",
    "upweights = [10]\n",
    "lambda_losses = [.5, 1, 5, 10, 25]\n",
    "\n",
    "for emb_dim in emb_dims:\n",
    "    for upweight in upweights:\n",
    "        for lambda_loss in lambda_losses:\n",
    "            opts = {\n",
    "                'vocab_size': 20000,\n",
    "                'emb_dim': emb_dim,\n",
    "                'upweight': upweight,\n",
    "                'lambda_loss': lambda_loss\n",
    "            }\n",
    "            train_config(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_config(opts,verbose=True):\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    print(path_to_save)\n",
    "    \n",
    "    model = neuralNetBow(opts) #change according to model inputs\n",
    "    model.load_state_dict(torch.load(path_to_save+'model_dict.pt',map_location=lambda storage, loc: storage))\n",
    "    model = model.to(current_device)\n",
    "    criterion = KMeansCriterion().to(current_device)\n",
    "    centroids = torch.load(path_to_save+'centroids',map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    TP_cluster, FP_cluster, results_dict = evaluation.main(model, centroids, val_loader, criterion, data_dir, current_device, verbose)\n",
    "    results_dict.update(opts)\n",
    "    return TP_cluster, FP_cluster, results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples in val loader: 455\n",
      "Assigned to cluster 1: 389\n",
      "TP_rate: 0.9691516709511568\n",
      "FP_rate: 0.030848329048843187\n",
      "FN_rate: 0.3333333333333333\n",
      "TN_rate: 0.6666666666666666\n",
      "\n",
      "\n",
      "Accuracy: 0.8179091688089117\n",
      "Precision: 0.9691516709511568\n",
      "Recall: 0.7440789473684211\n",
      "F1 score: 0.8418310383327131\n"
     ]
    }
   ],
   "source": [
    "emb_dims = [256]\n",
    "upweights = [10]\n",
    "lambda_losses = [.5, 1, 5, 10, 25]\n",
    "\n",
    "columns=['emb_dim','upweight']\n",
    "results_df = pd.DataFrame()\n",
    "for emb_dim in emb_dims:\n",
    "    for upweight in upweights:\n",
    "        for lambda_loss in lambda_losses\n",
    "            opts = {\n",
    "                'vocab_size': 20000,\n",
    "                'emb_dim': emb_dim,\n",
    "                'upweight': upweight\n",
    "            }\n",
    "            _, _, results_dict = evaluate_config(opts,False)\n",
    "            results_df = results_df.append(results_dict,ignore_index=True)\n",
    "        \n",
    "results_df = results_df[['emb_dim','upweight','Accuracy','F1 score','Precision','Recall',\n",
    "                        'TP_rate','FP_rate','FN_rate','TN_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
