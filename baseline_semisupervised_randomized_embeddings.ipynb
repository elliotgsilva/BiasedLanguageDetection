{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## KAGGLE ONLY\n",
    "# from shutil import copyfile\n",
    "# copyfile(src=\"../input/inputs/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\n",
    "# copyfile(src=\"../input/inputs/train_dataloader.p\", dst=\"../working/train_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/val_dataloader.p\", dst=\"../working/val_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/centroids_dataloader.p\", dst=\"../working/ground_truth_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/dictionary.p\", dst=\"../working/dictionary.p\")\n",
    "\n",
    "# copyfile(src=\"../input/input2/train_unlabeld_dataloader.p\", dst=\"../working/train_unlabelled_dataloader.p\")\n",
    "# copyfile(src=\"../input/input2/train_labeled_dataloader.p\", dst=\"../working/train_labelled_dataloader.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zno22FtJPX9z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation' from '/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/evaluation.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from datasets import get_mnist_dataset, get_data_loader\n",
    "#from utils import *\n",
    "#from models import *\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from generate_dataloaders import *\n",
    "\n",
    "from generate_dataloaders import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import evaluation\n",
    "import importlib\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaJEVd0wPX94"
   },
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1029\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nLzh007PX98"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yq-jDGFIPX99"
   },
   "outputs": [],
   "source": [
    "train_loader = pkl.load(open(data_dir + 'train_dataloader.p','rb'))\n",
    "train_loader_labelled = pkl.load(open(data_dir + 'train_labeled_dataloader.p','rb'))\n",
    "train_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabeled_dataloader.p','rb'))\n",
    "val_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install pytorch torchvision -c pytorch\n",
    "## if torch.__version__ is not 1.3.1, run this cell then restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzz8lwNQPX-B",
    "outputId": "690cb77f-2525-4c5a-ea14-a162716e34d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvt6N9QCPX-X"
   },
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puweJhdxPX-Y"
   },
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8BZ-QhNPX-Z"
   },
   "outputs": [],
   "source": [
    "class neuralNetBow(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n",
    "    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n",
    "    def __init__(self, opts):\n",
    "        super(neuralNetBow, self).__init__()\n",
    "        self.embed = nn.Embedding(opts['vocab_size'], opts['emb_dim'], padding_idx=0)\n",
    "        self.upweight = opts['upweight']\n",
    "        self.lambda_loss = opts['lambda_loss']\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        batch_size, num_tokens = tokens.shape\n",
    "        embedding = self.embed(tokens)\n",
    "        \n",
    "        # upweight by flagged_index\n",
    "        embedding[torch.LongTensor(range(batch_size)),flagged_index.type(torch.LongTensor),:] *= self.upweight\n",
    "        \n",
    "        # average across embeddings\n",
    "        embedding_ave = embedding.sum(1) / (num_tokens + self.upweight - 1)\n",
    "        \n",
    "        return embedding_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGsqcnEtPX-a"
   },
   "source": [
    "### Clustering Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrgIYm8JPX-b"
   },
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n",
    "        if labelled:\n",
    "            num_reviews = len(cluster_assignments)\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n",
    "            loss = cluster_distances.sum()\n",
    "        else:\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances, cluster_assignments = distances.min(1)\n",
    "            loss = cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TJohK2aPX-d"
   },
   "outputs": [],
   "source": [
    "def centroid_init(k, d, dataloader, model, current_device):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = torch.zeros(k, d).to(current_device)\n",
    "    centroid_counts = torch.zeros(k).to(current_device)\n",
    "    for (tokens, labels, flagged_indices) in dataloader:\n",
    "        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n",
    "        cluster_assignments = labels.to(current_device)\n",
    "        \n",
    "        model.eval()\n",
    "        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n",
    "    \n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, sentence_embed.to(current_device))\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n",
    "    centroid_counts.add_(bin_counts)\n",
    "    \n",
    "    #np_cluster_assignments = cluster_assignments.to('cpu')\n",
    "    #np_counts = np.bincount(np_cluster_assignments.data.numpy(), minlength=k)\n",
    "    #centroid_counts.add_(torch.FloatTensor(np_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_labelled_iter\n",
    "\n",
    "\n",
    "def loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_unlabelled_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3wynM7fPX-h"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KglsYxPJPX-i"
   },
   "outputs": [],
   "source": [
    "def train_model(model, centroids, criterion, optimizer, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=10, num_batches = 1000, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "    lambda_loss = model.lambda_loss\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n",
    "            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n",
    "\n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            \n",
    "            tokens_unlabelled = tokens_unlabelled.to(current_device)\n",
    "            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n",
    "            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n",
    "            \n",
    "            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n",
    "            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n",
    "            \n",
    "            total_batch_loss = cluster_loss_unlabelled + lambda_loss * cluster_loss_labelled\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add loss to the epoch loss\n",
    "            total_epoch_loss += total_batch_loss\n",
    "\n",
    "            # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments_labelled, sentence_embed_labelled)\n",
    "    \n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments_unlabelled, sentence_embed_unlabelled)\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            sub_folder = \"/\"\n",
    "            path_to_save+=sub_folder\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict.pt')\n",
    "            torch.save(centroids, path_to_save+'centroids')\n",
    "            torch.save(train_losses, path_to_save+'train_losses')\n",
    "            torch.save(val_losses, path_to_save+'val_losses')\n",
    "            torch.save(opts, path_to_save+'opts') #change options depending on model inputs required\n",
    "        \n",
    "    return model, centroids, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pBet75ZPX-m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cpu\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "print(\"Current Device:\",current_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_directory(opts):\n",
    "    path = os.getcwd()\n",
    "    model_folder = 'baseline_semisupervised_randomized_embeddings/'\n",
    "    model_dir = path + '/models/' + model_folder\n",
    "    \n",
    "    # subfolder for each hyperparam config\n",
    "    emb_dim = opts['emb_dim']\n",
    "    upweight = opts['upweight']\n",
    "    lambda_loss = opts['lambda_loss']\n",
    "    subfolder = \"emb_dim=\"+str(emb_dim) + \",upweight=\"+str(upweight) + \",lambda=\"+str(lambda_loss) + '/'\n",
    "    \n",
    "    # need to actually create these subfolders lol\n",
    "    try:\n",
    "        os.makedirs(model_dir + subfolder) # will throw error if subfolder already exists\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return model_dir + subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_config(opts):\n",
    "    model = neuralNetBow(opts).to(current_device)\n",
    "    centroids = centroid_init(2, opts['emb_dim'],train_loader_labelled, model, current_device)\n",
    "    criterion = KMeansCriterion().to(current_device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    print(path_to_save)\n",
    "    \n",
    "    train_model(model, centroids, criterion, optimizer, train_loader_labelled, train_loader_unlabelled, val_loader, num_epochs=10, path_to_save=path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_randomized_embeddings/emb_dim=256,upweight=10,lambda=0.1/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66427360398d43548a470682ebd585cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-07 21:05:50.196100 | Epoch 0\n",
      "Average training loss at batch  0 : 8.240\n",
      "Average training loss after epoch  0 : 0.815\n",
      "Average validation loss after epoch  0 : 0.653\n",
      "2019-12-07 21:06:41.095764 | Epoch 1\n",
      "Average training loss at batch  0 : 0.370\n",
      "Average training loss after epoch  1 : 0.111\n",
      "Average validation loss after epoch  1 : 0.255\n",
      "2019-12-07 21:07:36.504812 | Epoch 2\n",
      "Average training loss at batch  0 : 0.128\n",
      "Average training loss after epoch  2 : 0.054\n",
      "Average validation loss after epoch  2 : 0.156\n",
      "2019-12-07 21:08:39.336000 | Epoch 3\n",
      "Average training loss at batch  0 : 0.044\n",
      "Average training loss after epoch  3 : 0.031\n",
      "Average validation loss after epoch  3 : 0.124\n",
      "2019-12-07 21:09:46.757005 | Epoch 4\n",
      "Average training loss at batch  0 : 0.026\n",
      "Average training loss after epoch  4 : 0.022\n",
      "Average validation loss after epoch  4 : 0.104\n",
      "2019-12-07 21:10:55.624334 | Epoch 5\n",
      "Average training loss at batch  0 : 0.035\n",
      "Average training loss after epoch  5 : 0.017\n",
      "Average validation loss after epoch  5 : 0.081\n",
      "2019-12-07 21:12:04.719629 | Epoch 6\n",
      "Average training loss at batch  0 : 0.018\n",
      "Average training loss after epoch  6 : 0.013\n",
      "Average validation loss after epoch  6 : 0.070\n",
      "2019-12-07 21:13:13.943971 | Epoch 7\n",
      "Average training loss at batch  0 : 0.025\n",
      "Average training loss after epoch  7 : 0.010\n",
      "Average validation loss after epoch  7 : 0.066\n",
      "2019-12-07 21:14:23.747243 | Epoch 8\n",
      "Average training loss at batch  0 : 0.009\n",
      "Average training loss after epoch  8 : 0.009\n",
      "Average validation loss after epoch  8 : 0.057\n",
      "2019-12-07 21:15:32.920389 | Epoch 9\n",
      "Average training loss at batch  0 : 0.012\n",
      "Average training loss after epoch  9 : 0.007\n",
      "Average validation loss after epoch  9 : 0.054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_dims = [128, 256, 512]\n",
    "upweights = [1, 5, 10, 25]\n",
    "lambda_losses = [.1, .5, 1, 5, 10, 25]\n",
    "\n",
    "for emb_dim in emb_dims:\n",
    "    for upweight in upweights:\n",
    "        for lambda_loss in lambda_losses:\n",
    "            opts = {\n",
    "                'vocab_size': 20000,\n",
    "                'emb_dim': emb_dim,\n",
    "                'upweight': upweight,\n",
    "                'lambda_loss': lambda_loss\n",
    "            }\n",
    "            train_config(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_config(opts,verbose=True):\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    print(path_to_save)\n",
    "    \n",
    "    model = neuralNetBow(opts) #change according to model inputs\n",
    "    model.load_state_dict(torch.load(path_to_save+'model_dict.pt',map_location=lambda storage, loc: storage))\n",
    "    model = model.to(current_device)\n",
    "    criterion = KMeansCriterion().to(current_device)\n",
    "    centroids = torch.load(path_to_save+'centroids',map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    TP_cluster, FP_cluster, results_dict = evaluation.main(model, centroids, val_loader, criterion, data_dir, current_device, verbose)\n",
    "    results_dict.update(opts)\n",
    "    return TP_cluster, FP_cluster, results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_randomized_embeddings/emb_dim=256,upweight=10,lambda=0.1/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_randomized_embeddings/emb_dim=256,upweight=10,lambda=0.5/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_randomized_embeddings/emb_dim=256,upweight=10,lambda=1/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_randomized_embeddings/emb_dim=256,upweight=10,lambda=5/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_randomized_embeddings/emb_dim=256,upweight=10,lambda=10/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_randomized_embeddings/emb_dim=256,upweight=10,lambda=25/\n"
     ]
    }
   ],
   "source": [
    "emb_dims = [256]\n",
    "upweights = [10]\n",
    "lambda_losses = [.1, .5, 1, 5, 10, 25]\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "for emb_dim in emb_dims:\n",
    "    for upweight in upweights:\n",
    "        for lambda_loss in lambda_losses:\n",
    "            opts = {\n",
    "                'vocab_size': 20000,\n",
    "                'emb_dim': emb_dim,\n",
    "                'upweight': upweight,\n",
    "                'lambda_loss': lambda_loss\n",
    "            }\n",
    "            _, _, results_dict = evaluate_config(opts,False)\n",
    "            results_df = results_df.append(results_dict,ignore_index=True)\n",
    "        \n",
    "results_df = results_df[['emb_dim','upweight','lambda_loss','Accuracy','F1 score','Precision','Recall',\n",
    "                        'TP_rate','FP_rate','FN_rate','TN_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>upweight</th>\n",
       "      <th>lambda_loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>TP_rate</th>\n",
       "      <th>FP_rate</th>\n",
       "      <th>FN_rate</th>\n",
       "      <th>TN_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.576290</td>\n",
       "      <td>0.691055</td>\n",
       "      <td>0.947761</td>\n",
       "      <td>0.543771</td>\n",
       "      <td>0.947761</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.204819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.725815</td>\n",
       "      <td>0.778135</td>\n",
       "      <td>0.961631</td>\n",
       "      <td>0.653446</td>\n",
       "      <td>0.961631</td>\n",
       "      <td>0.038369</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.745447</td>\n",
       "      <td>0.948529</td>\n",
       "      <td>0.613991</td>\n",
       "      <td>0.948529</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.596330</td>\n",
       "      <td>0.403670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.634167</td>\n",
       "      <td>0.718770</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.583767</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>256.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.621915</td>\n",
       "      <td>0.710529</td>\n",
       "      <td>0.928040</td>\n",
       "      <td>0.575618</td>\n",
       "      <td>0.928040</td>\n",
       "      <td>0.071960</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>256.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.626044</td>\n",
       "      <td>0.713291</td>\n",
       "      <td>0.930348</td>\n",
       "      <td>0.578356</td>\n",
       "      <td>0.930348</td>\n",
       "      <td>0.069652</td>\n",
       "      <td>0.678261</td>\n",
       "      <td>0.321739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emb_dim  upweight  lambda_loss  Accuracy  F1 score  Precision    Recall  \\\n",
       "0  256.0    10.0      0.1          0.576290  0.691055  0.947761   0.543771   \n",
       "1  256.0    10.0      0.5          0.725815  0.778135  0.961631   0.653446   \n",
       "2  256.0    10.0      1.0          0.676100  0.745447  0.948529   0.613991   \n",
       "3  256.0    10.0      5.0          0.634167  0.718770  0.935000   0.583767   \n",
       "4  256.0    10.0      10.0         0.621915  0.710529  0.928040   0.575618   \n",
       "5  256.0    10.0      25.0         0.626044  0.713291  0.930348   0.578356   \n",
       "\n",
       "    TP_rate   FP_rate   FN_rate   TN_rate  \n",
       "0  0.947761  0.052239  0.795181  0.204819  \n",
       "1  0.961631  0.038369  0.510000  0.490000  \n",
       "2  0.948529  0.051471  0.596330  0.403670  \n",
       "3  0.935000  0.065000  0.666667  0.333333  \n",
       "4  0.928040  0.071960  0.684211  0.315789  \n",
       "5  0.930348  0.069652  0.678261  0.321739  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
