{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## KAGGLE ONLY\nfrom shutil import copyfile\ncopyfile(src=\"../input/inputs/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\ncopyfile(src=\"../input/inputs/train_dataloader.p\", dst=\"../working/train_dataloader.p\")\ncopyfile(src=\"../input/inputs/val_dataloader.p\", dst=\"../working/val_dataloader.p\")\ncopyfile(src=\"../input/inputs/centroids_dataloader.p\", dst=\"../working/ground_truth_dataloader.p\")\ncopyfile(src=\"../input/inputs/dictionary.p\", dst=\"../working/dictionary.p\")\n\ncopyfile(src=\"../input/input2/train_unlabeld_dataloader.p\", dst=\"../working/train_unlabelled_dataloader.p\")\ncopyfile(src=\"../input/input2/train_labeled_dataloader.p\", dst=\"../working/train_labelled_dataloader.p\")","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"zno22FtJPX9z","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n#from datasets import get_mnist_dataset, get_data_loader\n#from utils import *\n#from models import *\n\nimport pickle as pkl\nimport os\nimport datetime as dt\nimport pandas as pd\nimport random\n\nfrom generate_dataloaders import *\n\nfrom tqdm import tqdm_notebook as tqdm","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"oaJEVd0wPX94"},"cell_type":"markdown","source":"## Get Dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1029\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\nnp.random.seed(seed)  # Numpy module.\nrandom.seed(seed)  # Python random module.\ntorch.manual_seed(seed)\ntorch.backends.cudnn.enabled = False \ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\ndef _init_fn(worker_id):\n    np.random.seed(int(seed))","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"6nLzh007PX98","trusted":true},"cell_type":"code","source":"path = os.getcwd()\ndata_dir = path + '/'\n# data_dir = path +'/data/' #Uncomment for local system","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Verify filenames are consistent*"},{"metadata":{"colab":{},"colab_type":"code","id":"yq-jDGFIPX99","trusted":true},"cell_type":"code","source":"#train_loader = pkl.load(open(data_dir + 'train_dataloader.p','rb'))\ntrain_loader_labelled = pkl.load(open(data_dir + 'train_labelled_dataloader.p','rb'))\ntrain_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabelled_dataloader.p','rb'))\nval_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))\n#centroids = pkl.load(open(data_dir + 'train_labeld_dataloader.p','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_dict = pkl.load(open(data_dir + 'dictionary.p','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%conda install pytorch torchvision -c pytorch\n## if torch.__version__ is not 1.3.1, run this cell then restart kernel","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Lzz8lwNQPX-B","outputId":"690cb77f-2525-4c5a-ea14-a162716e34d3","trusted":true},"cell_type":"code","source":"print(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PRE TRAINED WORD EMBEDDINGS "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(review_dict, embedding_index ,dim = 200):\n#     embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(review_dict.tokens), dim))\n    unknown_words = []\n    \n    for word, i in review_dict.ids.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_twitter = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt' #Change loc for local system\n# glove_twitter = data_dir + 'glove.twitter.27B.200d.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_index = load_embeddings(glove_twitter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embedding_index,unknown_words = build_matrix(review_dict, embedding_index)\ndel embedding_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(review_dict.tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(unknown_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in unknown_words:\n    print(word)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"Cvt6N9QCPX-X"},"cell_type":"markdown","source":"## Neural Network Class"},{"metadata":{"colab_type":"text","id":"puweJhdxPX-Y"},"cell_type":"markdown","source":"NOTE: Data loader is defined as:\n- tuple: (tokens, flagged_index, problematic)"},{"metadata":{"colab":{},"colab_type":"code","id":"W8BZ-QhNPX-Z","trusted":true},"cell_type":"code","source":"class neuralNetBow_glove(nn.Module):\n    \"\"\"\n    BagOfWords classification model\n    \"\"\"\n    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n    def __init__(self, embedding_matrix, upweight=10):\n        super(neuralNetBow_glove, self).__init__()\n        vocab_size = embedding_matrix.shape[0]\n        embed_size = embedding_matrix.shape[1]\n        \n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        \n        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=2)\n        \n        self.embed.weight = nn.Parameter(torch.tensor(embedding_matrix,\n                                                          dtype=torch.float32))\n        self.embed.weight.requires_grad = False\n\n        self.upweight = upweight\n    \n    def forward(self, tokens, flagged_index):\n        batch_size, num_tokens = tokens.shape\n        embedding = self.embed(tokens)\n#         print(embedding.shape) # below assumes \"batch_size x num_tokens x Emb_dim\" (VERIFY)\n        \n        # upweight by flagged_index\n#         print(type(embedding))\n        embedding[torch.LongTensor(range(batch_size)),flagged_index.type(torch.LongTensor),:] *= self.upweight\n        \n        # average across embeddings\n        embedding_ave = embedding.sum(1) / (num_tokens + self.upweight - 1)\n        \n        return embedding_ave","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"SGsqcnEtPX-a"},"cell_type":"markdown","source":"### Clustering Stuff (un-tailored)"},{"metadata":{"colab":{},"colab_type":"code","id":"MrgIYm8JPX-b","trusted":true},"cell_type":"code","source":"class KMeansCriterion(nn.Module):\n    \n    def __init__(self, lmbda):\n        super().__init__()\n        self.lmbda = lmbda\n    \n    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n        if labelled:\n            num_reviews = len(cluster_assignments)\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n            loss = self.lmbda * cluster_distances.sum()\n        else:\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances, cluster_assignments = distances.min(1)\n            loss = self.lmbda * cluster_distances.sum()\n        return loss, cluster_assignments","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"-TJohK2aPX-d","trusted":true},"cell_type":"code","source":"def centroid_init(k, d, dataloader, model, current_device):\n    ## Here we ideally don't want to do randomized/zero initialization\n    centroid_sums = torch.zeros(k, d).to(current_device)\n    centroid_counts = torch.zeros(k).to(current_device)\n    for (tokens, labels, flagged_indices) in dataloader:\n        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n        cluster_assignments = labels.to(current_device)\n        \n        model.eval()\n        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n    \n        update_clusters(centroid_sums, centroid_counts,\n                        cluster_assignments, sentence_embed.to(current_device))\n    \n    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n    return centroid_means.clone()\n\ndef update_clusters(centroid_sums, centroid_counts,\n                    cluster_assignments, embeddings):\n    k = centroid_sums.size(0)\n\n    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n    centroid_counts.add_(bin_counts)\n    \n    #np_cluster_assignments = cluster_assignments.to('cpu')\n    #np_counts = np.bincount(np_cluster_assignments.data.numpy(), minlength=k)\n    #centroid_counts.add_(torch.FloatTensor(np_counts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataloader stuff"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n    try:\n        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n    except StopIteration:\n        train_loader_labelled_iter = iter(train_loader_labelled)\n        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n\n    return tokens, labels, flagged_indices, train_loader_labelled_iter\n\n\ndef loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n    try:\n        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n    except StopIteration:\n        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n\n    return tokens, labels, flagged_indices, train_loader_unlabelled_iter\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"u3wynM7fPX-h"},"cell_type":"markdown","source":"### Training Function (un-tailored, needs alterations)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, centroids, criterion, optimizer, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=10, num_batches = 1000, path_to_save=None, print_every = 1000):\n\n    train_loader_labelled_iter = iter(train_loader_labelled)\n    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n\n    train_losses=[]\n    val_losses=[]\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 0:\n        current_device = 'cuda'\n    else:\n        current_device = 'cpu'\n    \n    for epoch in range(num_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.train()\n        k, d = centroids.size()\n        centroid_sums = torch.zeros_like(centroids).to(current_device)\n        centroid_counts = torch.zeros(k).to(current_device)\n        total_epoch_loss = 0\n        \n        for i in range(num_batches):\n            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n\n            tokens_labelled = tokens_labelled.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n            \n            tokens_unlabelled = tokens_unlabelled.to(current_device)\n            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n\n            # forward pass and compute loss\n            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n            \n            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n    \n            total_batch_loss = cluster_loss_labelled.data + cluster_loss_unlabelled.data\n        \n            # run update step\n            optimizer.zero_grad()\n#             total_batch_loss.backward()\n            optimizer.step()\n            \n#             #Add loss to the epoch loss\n            total_epoch_loss += total_batch_loss\n\n#             # store centroid sums and counts in memory for later centering\n            update_clusters(centroid_sums, centroid_counts,\n                            cluster_assignments_labelled, sentence_embed_labelled)\n    \n            update_clusters(centroid_sums, centroid_counts,\n                            cluster_assignments_unlabelled, sentence_embed_unlabelled)\n\n            if i % print_every == 0:\n                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # update centroids based on assignments from autoencoders\n        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n            model.eval()\n            tokens = tokens.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices = flagged_indices.to(current_device)\n            \n            # forward pass and compute loss\n            sentence_embed = model(tokens,flagged_indices)\n            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n            \n            #Add loss to the validation loss\n            total_validation_loss += cluster_loss.data\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"vocab_size\":model.vocab_size, \"embed_size\": model.embed_size}\n            torch.save(model.state_dict(), path_to_save+'model_dict.pt')\n            torch.save(centroids, path_to_save+'centroids')\n            torch.save(train_losses, path_to_save+'train_losses')\n            torch.save(val_losses, path_to_save+'val_losses')\n            torch.save(opts, path_to_save+'opts')\n            \n        \n    return model, centroids, train_losses, val_losses","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"0pBet75ZPX-m","trusted":true},"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\nif num_gpus > 0:\n    current_device = 'cuda'\nelse:\n    current_device = 'cpu'\n\nmodel = neuralNetBow_glove(glove_embedding_index).to(current_device)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"mTFO2vp-PX-o","trusted":true},"cell_type":"code","source":"# model = neuralNetBow(opts['vocab_size'], opts['emb_dim'])\ncentroids = centroid_init(2, 200,train_loader_labelled, model, current_device)\ncriterion = KMeansCriterion(1).to(current_device)\noptimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Xya2NiqcPX-q","outputId":"59b3072e-c567-4e18-a242-ba8298f08e58","scrolled":true,"trusted":true},"cell_type":"code","source":"centroids","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":892,"status":"ok","timestamp":1573355494600,"user":{"displayName":"Eileen Cho","photoUrl":"","userId":"03381570147993013394"},"user_tz":300},"id":"2It2SvzjPX-s","outputId":"5de6949c-7e9e-4ade-f222-5eb28f5347db","trusted":true},"cell_type":"code","source":"current_device","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_dict.get_id(\"the\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.tensor([41])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.embed(torch.tensor([41]).to(current_device))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = os.getcwd()\nmodel_folder= 'baseline_semisupervised_frozen_glove/'\nmodel_dir = path + '/models/' + model_folder\nmodel_dir = path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_batches = int(len(train_loader_unlabelled.dataset)/train_loader_unlabelled.batch_size)+1\nnum_batches","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"colab_type":"code","executionInfo":{"elapsed":8813,"status":"error","timestamp":1573355511003,"user":{"displayName":"Eileen Cho","photoUrl":"","userId":"03381570147993013394"},"user_tz":300},"id":"rgwMd27mPX-u","outputId":"063ebc41-be3c-4474-d92c-7bd0680bb366","scrolled":true,"trusted":true},"cell_type":"code","source":"baseline_model, baseline_centroids, baseline_train_losses, baseline_val_losses = train_model(model, centroids, criterion, optimizer, train_loader_labelled,train_loader_unlabelled, val_loader, num_epochs=10, num_batches=num_batches, path_to_save=model_dir)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# #Only needed for Kaggle\n\n# from IPython.display import FileLink, FileLinks \n# FileLinks('.') #lists all downloadable files on server","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"model_tuning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}