{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## KAGGLE ONLY\n",
    "# from shutil import copyfile\n",
    "# copyfile(src=\"../input/inputs/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\n",
    "# copyfile(src=\"../input/inputs/train_dataloader.p\", dst=\"../working/train_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/val_dataloader.p\", dst=\"../working/val_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/centroids_dataloader.p\", dst=\"../working/ground_truth_dataloader.p\")\n",
    "# copyfile(src=\"../input/inputs/dictionary.p\", dst=\"../working/dictionary.p\")\n",
    "\n",
    "# copyfile(src=\"../input/input2/train_unlabeld_dataloader.p\", dst=\"../working/train_unlabelled_dataloader.p\")\n",
    "# copyfile(src=\"../input/input2/train_labeled_dataloader.p\", dst=\"../working/train_labelled_dataloader.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zno22FtJPX9z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation' from '/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/evaluation.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from datasets import get_mnist_dataset, get_data_loader\n",
    "#from utils import *\n",
    "#from models import *\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from generate_dataloaders import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import evaluation\n",
    "import importlib\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaJEVd0wPX94"
   },
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1029\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nLzh007PX98"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/'\n",
    "data_dir = path +'/data/' #Uncomment for local system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Verify filenames are consistent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yq-jDGFIPX99"
   },
   "outputs": [],
   "source": [
    "train_loader = pkl.load(open(data_dir + 'train_dataloader.p','rb'))\n",
    "train_loader_labelled = pkl.load(open(data_dir + 'train_labeled_dataloader.p','rb'))\n",
    "train_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabeled_dataloader.p','rb'))\n",
    "val_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict = pkl.load(open(data_dir + 'dictionary.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install pytorch torchvision -c pytorch\n",
    "## if torch.__version__ is not 1.3.1, run this cell then restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzz8lwNQPX-B",
    "outputId": "690cb77f-2525-4c5a-ea14-a162716e34d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE TRAINED WORD EMBEDDINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(review_dict, embedding_index ,dim = 200):\n",
    "#     embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(review_dict.tokens), dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in review_dict.ids.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_twitter = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt' #Change loc for local system\n",
    "glove_twitter = data_dir + 'glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc68c389c714272b1400e7bde26a23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_index = load_embeddings(glove_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_index,unknown_words = build_matrix(review_dict, embedding_index)\n",
    "del embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_dict.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4428"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in unknown_words:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvt6N9QCPX-X"
   },
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puweJhdxPX-Y"
   },
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8BZ-QhNPX-Z"
   },
   "outputs": [],
   "source": [
    "class neuralNetBow_glove(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n",
    "    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n",
    "    def __init__(self, embedding_matrix, upweight=10, unfrozen=False):\n",
    "        super(neuralNetBow_glove, self).__init__()\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.vocab_size = embedding_matrix.shape[0]\n",
    "        self.embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=0)\n",
    "        self.embed.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n",
    "        self.embed.weight.requires_grad = unfrozen\n",
    "        \n",
    "        self.upweight = upweight\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        batch_size, num_tokens = tokens.shape\n",
    "        embedding = self.embed(tokens)\n",
    "        \n",
    "        # upweight by flagged_index\n",
    "        embedding[torch.LongTensor(range(batch_size)),flagged_index.type(torch.LongTensor),:] *= self.upweight\n",
    "        \n",
    "        # average across embeddings\n",
    "        embedding_ave = embedding.sum(1) / (num_tokens + self.upweight - 1)\n",
    "        \n",
    "        return embedding_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGsqcnEtPX-a"
   },
   "source": [
    "### Clustering Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrgIYm8JPX-b"
   },
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self, lmbda):\n",
    "        super().__init__()\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n",
    "        if labelled:\n",
    "            num_reviews = len(cluster_assignments)\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n",
    "            loss = self.lmbda * cluster_distances.sum()\n",
    "        else:\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances, cluster_assignments = distances.min(1)\n",
    "            loss = self.lmbda * cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TJohK2aPX-d"
   },
   "outputs": [],
   "source": [
    "def centroid_init(k, d, dataloader, model, current_device):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = torch.zeros(k, d).to(current_device)\n",
    "    centroid_counts = torch.zeros(k).to(current_device)\n",
    "    for (tokens, labels, flagged_indices) in dataloader:\n",
    "        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n",
    "        cluster_assignments = labels.to(current_device)\n",
    "        \n",
    "        model.eval()\n",
    "        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n",
    "    \n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, sentence_embed.to(current_device))\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n",
    "    centroid_counts.add_(bin_counts)\n",
    "    \n",
    "    #np_cluster_assignments = cluster_assignments.to('cpu')\n",
    "    #np_counts = np.bincount(np_cluster_assignments.data.numpy(), minlength=k)\n",
    "    #centroid_counts.add_(torch.FloatTensor(np_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_labelled_iter\n",
    "\n",
    "\n",
    "def loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_unlabelled_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3wynM7fPX-h"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, centroids, criterion, optimizer, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=10, num_batches = 1000, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n",
    "            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n",
    "\n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            \n",
    "            tokens_unlabelled = tokens_unlabelled.to(current_device)\n",
    "            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n",
    "            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n",
    "            \n",
    "            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n",
    "            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n",
    "    \n",
    "            total_batch_loss = cluster_loss_labelled + cluster_loss_unlabelled\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "#             total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add loss to the epoch loss\n",
    "            total_epoch_loss += total_batch_loss\n",
    "\n",
    "#             # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments_labelled, sentence_embed_labelled)\n",
    "    \n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments_unlabelled, sentence_embed_unlabelled)\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict.pt')\n",
    "            torch.save(centroids, path_to_save+'centroids')\n",
    "            torch.save(train_losses, path_to_save+'train_losses')\n",
    "            torch.save(val_losses, path_to_save+'val_losses')\n",
    "            torch.save(opts, path_to_save+'opts')\n",
    "            \n",
    "        \n",
    "    return model, centroids, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pBet75ZPX-m"
   },
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "model = neuralNetBow_glove(glove_embedding_index).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTFO2vp-PX-o"
   },
   "outputs": [],
   "source": [
    "centroids = centroid_init(2, 200,train_loader_labelled, model, current_device)\n",
    "criterion = KMeansCriterion(1).to(current_device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xya2NiqcPX-q",
    "outputId": "59b3072e-c567-4e18-a242-ba8298f08e58",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.3920e-02,  4.7916e-02, -3.4403e-03,  6.2405e-02, -1.4993e-02,\n",
       "          9.7727e-02,  3.1345e-01, -2.0820e-02, -5.8466e-02, -6.3985e-02,\n",
       "         -2.5678e-02, -6.1849e-02, -4.1237e-01, -5.8385e-02, -2.4222e-02,\n",
       "          4.8410e-02, -1.2295e-02,  2.9833e-02, -7.6221e-02, -2.1353e-02,\n",
       "         -1.3972e-02,  7.9938e-03, -3.5308e-02,  1.9144e-02, -6.5173e-02,\n",
       "          4.9148e-01,  2.8338e-03,  8.5547e-02,  1.0131e-01,  3.7258e-03,\n",
       "         -2.4144e-02, -7.5183e-02, -4.1977e-02, -5.0204e-03,  3.2769e-02,\n",
       "          5.0823e-02,  3.5903e-02, -2.0590e-02,  2.4465e-02,  1.4467e-02,\n",
       "          2.2642e-01, -1.7756e-02,  7.1540e-02,  9.6466e-03,  1.8655e-02,\n",
       "         -3.1804e-02,  9.3824e-02, -1.1422e-02, -6.5178e-02,  2.9992e-02,\n",
       "          2.2362e-02,  3.6858e-02, -5.0271e-02, -1.5484e-02,  5.2415e-03,\n",
       "          2.4776e-02, -3.1752e-02, -1.2649e-02, -2.4775e-02, -1.0467e-02,\n",
       "          1.2805e-02,  3.1579e-02, -3.3153e-02, -2.3706e-02,  5.6861e-02,\n",
       "          3.1290e-02, -3.6886e-02, -2.4460e-02,  3.9984e-02, -1.1415e-02,\n",
       "         -3.6171e-02, -1.9382e-02,  4.8292e-02,  4.1943e-02,  1.6476e-01,\n",
       "         -3.2153e-02,  5.1683e-02, -3.7033e-02, -1.2125e-02,  2.0999e-02,\n",
       "          1.7672e-01,  4.7964e-02,  4.5311e-02,  7.6131e-03,  1.1992e-01,\n",
       "          2.1964e-02,  9.4089e-02, -1.9929e-02,  2.4961e-02, -8.6674e-04,\n",
       "          1.1740e-02,  2.0294e-02,  6.2904e-02, -1.8970e-02,  6.8738e-02,\n",
       "         -5.8608e-02,  7.7588e-02, -6.5321e-03,  9.6949e-03, -3.6378e-02,\n",
       "         -8.3469e-05, -8.8981e-03, -3.5627e-02,  3.0056e-02,  2.4877e-02,\n",
       "         -6.6342e-02,  3.5848e-02, -2.9160e-03, -2.2127e-02,  1.9481e-02,\n",
       "          1.1476e-02, -3.1959e-02, -1.0779e-02,  9.6134e-02, -1.0182e-02,\n",
       "          3.1801e-02, -5.1356e-02,  1.7475e-02,  1.2167e-01,  1.5015e-01,\n",
       "          5.9146e-02,  3.6423e-02,  3.1895e-02, -5.5349e-02,  4.1756e-03,\n",
       "         -1.2306e-01, -2.6812e-02, -1.6313e-02,  8.7617e-02, -5.5360e-02,\n",
       "         -3.9967e-02,  1.9463e-03, -1.2652e-02,  3.4519e-02, -6.7654e-02,\n",
       "         -4.7394e-02,  4.6134e-02, -1.0922e-02, -5.6923e-02, -1.4936e-02,\n",
       "          3.9040e-02, -1.5598e-02,  1.4544e-02,  1.0382e-01, -3.7103e-03,\n",
       "          1.6360e-02, -4.6424e-02, -7.9082e-03,  5.9555e-02, -2.0056e-02,\n",
       "          4.2734e-02, -3.3963e-03, -2.4354e+00,  1.1036e-01,  2.1941e-02,\n",
       "         -2.8713e-02,  8.9974e-02, -9.3268e-02,  3.8442e-02,  6.5561e-02,\n",
       "         -3.5787e-02,  9.9102e-03, -6.5870e-02,  1.6775e-02,  5.9534e-02,\n",
       "         -6.2415e-02, -5.6872e-02,  2.0232e-02,  7.3768e-02,  3.8986e-02,\n",
       "          5.0914e-02, -4.0564e-02,  2.2724e-02,  1.5859e-02, -1.2015e-02,\n",
       "         -3.6403e-02,  4.9472e-02, -7.5374e-02,  2.1448e-02, -3.5725e-02,\n",
       "          5.0587e-02, -3.1673e-03, -2.2547e-02, -1.0693e-02, -2.7047e-03,\n",
       "         -1.1155e-02, -3.2722e-02, -2.7897e-02, -3.0179e-02, -9.2269e-03,\n",
       "         -4.6703e-02,  2.3142e-02,  8.8128e-02,  9.3663e-04,  1.6066e-02,\n",
       "          1.2068e-02,  3.7630e-02,  5.8469e-02, -4.2431e-02, -2.3616e-02],\n",
       "        [ 6.2430e-02,  1.5813e-02, -2.1161e-02,  8.0321e-02, -1.8456e-02,\n",
       "          1.0840e-01,  2.7002e-01, -4.2716e-03, -7.2370e-02, -9.5038e-02,\n",
       "         -1.0086e-01, -9.1395e-02, -3.6870e-01, -7.4947e-02,  3.3022e-03,\n",
       "          5.7955e-02, -7.4895e-05,  1.5444e-02, -3.5648e-02, -4.8490e-02,\n",
       "         -6.3813e-02,  2.5507e-02, -5.2558e-02,  4.0685e-02, -7.7022e-02,\n",
       "          4.8468e-01, -1.5985e-02,  1.1236e-01,  1.2495e-01,  2.4181e-02,\n",
       "         -2.2257e-02, -1.0531e-01, -4.2161e-02, -4.1629e-03,  9.2263e-02,\n",
       "          7.7236e-02,  3.1175e-02, -1.7671e-02,  1.0340e-02,  3.9899e-04,\n",
       "          2.2683e-01, -1.3347e-02,  5.5746e-02,  6.4330e-02,  1.6446e-03,\n",
       "          4.0211e-03,  8.7097e-02,  2.9635e-02, -8.9090e-02,  9.7812e-03,\n",
       "         -2.5628e-03,  2.4638e-02, -1.0348e-01, -2.1921e-02,  7.8485e-02,\n",
       "          1.9254e-02, -1.8567e-02, -4.0366e-02, -2.2711e-02, -4.0104e-02,\n",
       "          1.2022e-03,  8.5858e-03, -7.9648e-02, -3.5799e-02,  7.5272e-02,\n",
       "          8.5114e-02, -7.6176e-02, -4.8176e-02,  3.5970e-02, -3.0776e-02,\n",
       "         -5.4776e-02,  1.8548e-02,  3.3998e-02,  4.1287e-02,  1.8121e-01,\n",
       "         -4.7860e-02,  1.1205e-01, -3.7054e-02,  2.9010e-04,  3.0519e-02,\n",
       "          1.9230e-01,  6.1527e-02,  5.7561e-02,  1.9677e-03,  1.4862e-01,\n",
       "          8.3630e-02,  1.4032e-01, -2.5945e-02,  2.7411e-02,  1.8615e-02,\n",
       "          7.7318e-03, -3.0238e-02,  1.8702e-02, -1.1683e-02,  1.1173e-01,\n",
       "          9.9340e-04,  1.2653e-01,  1.4724e-02, -4.4214e-03, -1.5825e-02,\n",
       "          4.6982e-02,  6.1572e-04, -2.6766e-02,  2.1242e-03,  3.3747e-02,\n",
       "         -5.5036e-02,  2.3071e-02, -4.3938e-02,  3.3293e-03,  5.4447e-02,\n",
       "          1.6474e-03, -1.1019e-02,  4.5018e-02,  1.3018e-01,  6.3145e-02,\n",
       "          3.2439e-02, -5.0701e-02, -1.1326e-02,  1.0975e-01,  1.9586e-01,\n",
       "          1.0236e-01,  6.8243e-02,  3.3015e-02, -2.2080e-03,  1.2412e-02,\n",
       "         -2.1265e-01, -2.0358e-02, -3.9315e-02,  9.5298e-02, -1.7883e-02,\n",
       "         -2.5277e-02,  2.8382e-02, -3.3335e-02,  5.4809e-02, -4.3438e-02,\n",
       "         -5.4632e-02,  1.1740e-02, -4.0177e-02, -1.1562e-01,  6.3642e-02,\n",
       "          1.5912e-02, -4.1621e-02,  4.3583e-02,  1.2763e-01,  3.3262e-02,\n",
       "          2.1451e-02, -6.1690e-03,  4.8763e-03,  8.2257e-02,  2.3084e-03,\n",
       "          4.2099e-02, -2.9138e-03, -2.0991e+00,  1.1592e-01,  4.6482e-02,\n",
       "         -8.6306e-02,  1.3912e-01, -1.1601e-01,  7.6736e-02,  5.9115e-02,\n",
       "         -6.2493e-02,  3.5961e-02, -5.8280e-02,  2.6782e-02,  6.5899e-02,\n",
       "         -4.1463e-02, -9.5149e-02,  4.0631e-02,  8.4991e-02,  5.9881e-02,\n",
       "          1.0567e-01, -6.5505e-02,  1.3324e-02,  7.0337e-02, -3.1942e-02,\n",
       "         -3.9299e-02,  6.9357e-02, -1.0200e-01,  2.5618e-02, -4.4702e-02,\n",
       "          9.8359e-02, -6.4680e-03, -2.1003e-02, -1.2112e-03,  1.8355e-02,\n",
       "         -2.7127e-02, -8.3993e-02, -2.6359e-02, -6.0884e-02, -2.8291e-02,\n",
       "         -8.8273e-02,  3.0024e-02,  1.4999e-01, -2.8780e-02,  3.8757e-02,\n",
       "         -2.7526e-02,  4.2378e-02,  3.3372e-02, -8.9659e-02, -3.6045e-02]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1573355494600,
     "user": {
      "displayName": "Eileen Cho",
      "photoUrl": "",
      "userId": "03381570147993013394"
     },
     "user_tz": 300
    },
    "id": "2It2SvzjPX-s",
    "outputId": "5de6949c-7e9e-4ade-f222-5eb28f5347db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict.get_id(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9341e-01,  3.5693e-01,  6.6064e-01, -3.2990e-02,  2.4988e-01,\n",
       "          2.5928e-01, -2.7176e-02,  6.8420e-02, -2.9053e-01, -4.5703e-01,\n",
       "         -7.7942e-02,  3.2520e-01, -1.4854e+00, -6.7444e-02, -1.7029e-01,\n",
       "         -9.2926e-03,  3.4619e-01, -1.1574e-02,  3.7964e-02,  4.5605e-01,\n",
       "          8.0505e-02,  1.5308e-01, -1.5308e-01, -1.8811e-01, -1.8201e-01,\n",
       "          8.7256e-01,  3.9795e-01,  4.0991e-01,  4.4971e-01, -1.9646e-03,\n",
       "         -4.1138e-02, -4.7882e-02, -2.4048e-01, -8.6853e-02,  1.4183e-02,\n",
       "         -2.3755e-01,  2.5171e-01,  2.8540e-01,  4.4507e-01, -4.9634e-01,\n",
       "         -1.2708e-01, -1.7480e-01,  8.2214e-02,  4.5410e-02,  5.1709e-01,\n",
       "          3.4546e-02, -8.5815e-02, -3.4912e-01,  5.2197e-01, -3.9502e-01,\n",
       "          6.4148e-02, -4.2017e-01, -1.5942e-01,  1.8286e-01, -5.7892e-02,\n",
       "         -1.9180e-02, -4.4556e-01,  3.1543e-01, -1.6101e-01, -9.2163e-02,\n",
       "         -2.4963e-01, -1.3895e-03, -4.2651e-01, -1.7932e-01,  8.1665e-02,\n",
       "          1.8323e-01, -3.2056e-01, -1.2010e-04, -1.3098e-01, -2.9810e-01,\n",
       "         -2.5139e-03, -1.1316e-01, -4.6387e-01, -1.9958e-01,  8.5107e-01,\n",
       "         -6.7993e-02,  1.2866e-01, -6.7236e-01, -1.2222e-02,  1.4197e-01,\n",
       "          9.2480e-01,  8.5144e-02,  2.6880e-01,  3.0396e-01, -1.4026e-01,\n",
       "          1.8604e-01, -2.5537e-01,  1.8384e-01, -3.5229e-01, -7.8369e-02,\n",
       "         -2.6953e-01, -1.8079e-01, -2.7298e-02, -3.9331e-01,  2.4536e-01,\n",
       "         -1.1200e-01,  1.2091e-01, -4.6118e-01,  3.7766e-03,  2.3804e-01,\n",
       "         -1.9397e-01,  1.5479e-01, -9.9468e-04,  9.2285e-02,  3.9844e-01,\n",
       "         -3.8062e-01, -3.6499e-01,  4.9286e-03, -7.3975e-02,  1.4038e-01,\n",
       "         -2.3083e-01,  1.3000e-01,  3.0420e-01,  1.0590e-01,  7.8918e-02,\n",
       "          8.6243e-02,  5.8655e-02,  1.7920e-01, -1.5100e-01,  9.5264e-01,\n",
       "          3.7305e-01,  2.2537e-02,  3.2806e-02, -4.9951e-01,  1.6748e-01,\n",
       "          3.0396e-01,  3.3057e-01, -4.7314e-01,  5.6396e-01,  5.7129e-01,\n",
       "          5.5664e-02, -2.9077e-01, -1.7468e-01, -1.3086e-01,  1.5405e-01,\n",
       "         -2.1460e-01,  3.1665e-01,  2.0935e-01,  1.1917e-02, -3.4399e-01,\n",
       "          1.1987e-01,  3.9111e-01, -4.1187e-01,  2.0837e-01,  1.9727e-01,\n",
       "          4.0063e-01,  4.1626e-01,  7.2754e-02, -3.3984e-01,  2.0386e-01,\n",
       "          8.6182e-02,  2.8320e-01, -6.7617e+00, -5.6836e-01,  1.4954e-01,\n",
       "         -2.4670e-01,  6.0449e-01,  1.2146e-01, -5.2734e-01,  1.3586e-01,\n",
       "          1.5674e-01,  5.8960e-02, -3.2983e-01,  2.1301e-02,  1.2751e-03,\n",
       "          2.5000e-01, -2.4643e-02,  2.5513e-01,  3.8574e-01, -2.4219e-01,\n",
       "         -1.8274e-01,  2.9614e-01,  2.1851e-01,  1.9238e-01,  1.5234e-01,\n",
       "         -1.3649e-02, -5.0629e-02, -9.6436e-02,  1.5640e-02,  5.1172e-01,\n",
       "          1.4722e-01, -8.1482e-02, -3.1079e-01, -1.1169e-01, -1.3428e-01,\n",
       "          3.6670e-01, -2.9663e-01, -4.9951e-01, -6.0486e-02,  4.6959e-03,\n",
       "         -1.6614e-01, -1.7017e-01,  1.1230e-01,  1.6040e-01, -1.7139e-01,\n",
       "         -1.5625e-01, -1.1365e-01,  1.7700e-01, -5.3711e-01, -2.9688e-01]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed(torch.tensor([41]).to(current_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "model_folder= 'baseline_semisupervised_frozen_glove/'\n",
    "model_dir = path + '/models/' + model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3211"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = int(len(train_loader_unlabelled.dataset)/train_loader_unlabelled.batch_size)+1\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8813,
     "status": "error",
     "timestamp": 1573355511003,
     "user": {
      "displayName": "Eileen Cho",
      "photoUrl": "",
      "userId": "03381570147993013394"
     },
     "user_tz": 300
    },
    "id": "rgwMd27mPX-u",
    "outputId": "063ebc41-be3c-4474-d92c-7bd0680bb366",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-21 17:07:14.858166 | Epoch 0\n",
      "Average training loss at batch  0 : 2.094\n",
      "Average training loss at batch  1000 : 1.956\n",
      "Average training loss at batch  2000 : 2.045\n",
      "Average training loss at batch  3000 : 2.173\n",
      "Average training loss after epoch  0 : 4.015\n",
      "Average validation loss after epoch  0 : 1.714\n",
      "2019-11-21 17:07:19.592296 | Epoch 1\n",
      "Average training loss at batch  0 : 2.088\n",
      "Average training loss at batch  1000 : 1.865\n",
      "Average training loss at batch  2000 : 1.894\n",
      "Average training loss at batch  3000 : 2.035\n",
      "Average training loss after epoch  1 : 3.828\n",
      "Average validation loss after epoch  1 : 1.692\n",
      "2019-11-21 17:07:24.322464 | Epoch 2\n",
      "Average training loss at batch  0 : 1.684\n",
      "Average training loss at batch  1000 : 1.674\n",
      "Average training loss at batch  2000 : 1.953\n",
      "Average training loss at batch  3000 : 2.725\n",
      "Average training loss after epoch  2 : 3.821\n",
      "Average validation loss after epoch  2 : 1.690\n",
      "2019-11-21 17:07:29.268973 | Epoch 3\n",
      "Average training loss at batch  0 : 1.824\n",
      "Average training loss at batch  1000 : 2.180\n",
      "Average training loss at batch  2000 : 2.125\n",
      "Average training loss at batch  3000 : 1.892\n",
      "Average training loss after epoch  3 : 3.822\n",
      "Average validation loss after epoch  3 : 1.689\n",
      "2019-11-21 17:07:34.285257 | Epoch 4\n",
      "Average training loss at batch  0 : 1.904\n",
      "Average training loss at batch  1000 : 2.053\n",
      "Average training loss at batch  2000 : 1.751\n",
      "Average training loss at batch  3000 : 1.990\n",
      "Average training loss after epoch  4 : 3.821\n",
      "Average validation loss after epoch  4 : 1.689\n",
      "2019-11-21 17:07:39.075890 | Epoch 5\n",
      "Average training loss at batch  0 : 1.944\n",
      "Average training loss at batch  1000 : 1.927\n",
      "Average training loss at batch  2000 : 2.166\n",
      "Average training loss at batch  3000 : 1.979\n",
      "Average training loss after epoch  5 : 3.821\n",
      "Average validation loss after epoch  5 : 1.689\n",
      "2019-11-21 17:07:44.192447 | Epoch 6\n",
      "Average training loss at batch  0 : 1.998\n",
      "Average training loss at batch  1000 : 2.089\n",
      "Average training loss at batch  2000 : 2.037\n",
      "Average training loss at batch  3000 : 1.912\n",
      "Average training loss after epoch  6 : 3.821\n",
      "Average validation loss after epoch  6 : 1.689\n",
      "2019-11-21 17:07:49.304343 | Epoch 7\n",
      "Average training loss at batch  0 : 2.058\n",
      "Average training loss at batch  1000 : 2.065\n",
      "Average training loss at batch  2000 : 1.979\n",
      "Average training loss at batch  3000 : 2.203\n",
      "Average training loss after epoch  7 : 3.822\n",
      "Average validation loss after epoch  7 : 1.689\n",
      "2019-11-21 17:07:54.146853 | Epoch 8\n",
      "Average training loss at batch  0 : 1.974\n",
      "Average training loss at batch  1000 : 2.105\n",
      "Average training loss at batch  2000 : 2.335\n",
      "Average training loss at batch  3000 : 1.668\n",
      "Average training loss after epoch  8 : 3.821\n",
      "Average validation loss after epoch  8 : 1.689\n",
      "2019-11-21 17:07:59.179708 | Epoch 9\n",
      "Average training loss at batch  0 : 2.362\n",
      "Average training loss at batch  1000 : 2.006\n",
      "Average training loss at batch  2000 : 1.853\n",
      "Average training loss at batch  3000 : 1.966\n",
      "Average training loss after epoch  9 : 3.821\n",
      "Average validation loss after epoch  9 : 1.689\n"
     ]
    }
   ],
   "source": [
    "baseline_model, baseline_centroids, baseline_train_losses, baseline_val_losses = train_model(model, centroids, criterion, optimizer, train_loader_labelled,train_loader_unlabelled, val_loader, num_epochs=10, num_batches=num_batches, path_to_save=model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Only needed for Kaggle\n",
    "\n",
    "# from IPython.display import FileLink, FileLinks \n",
    "# FileLinks('.') #lists all downloadable files on server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell will change for each model\n",
    "model_folder = 'baseline_semisupervised_frozen_glove/'\n",
    "\n",
    "criterion = KMeansCriterion(1)\n",
    "criterion = criterion.to(current_device)\n",
    "\n",
    "path = os.getcwd()\n",
    "model_dir = path + '/models/' + model_folder\n",
    "\n",
    "opts = torch.load(model_dir+'opts')\n",
    "model = neuralNetBow_glove(opts['embedding_matrix']) #change here depending on model\n",
    "model.load_state_dict(torch.load(model_dir+'model_dict.pt',map_location=lambda storage, loc: storage))\n",
    "model = model.to(current_device)\n",
    "centroids = torch.load(model_dir+'centroids',map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples in val loader: 455\n",
      "Assigned to cluster 1: 298\n",
      "TP_rate: 0.8959731543624161\n",
      "FP_rate: 0.1040268456375839\n",
      "FN_rate: 0.8407643312101911\n",
      "TN_rate: 0.1592356687898089\n",
      "\n",
      "\n",
      "Accuracy: 0.5276044115761125\n",
      "Precision: 0.8959731543624161\n",
      "Recall: 0.5158944064980616\n",
      "F1 score: 0.6547746424973251\n"
     ]
    }
   ],
   "source": [
    "TP_cluster, FP_cluster=evaluation.main(model, centroids, val_loader, criterion, data_dir, current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>index</th>\n",
       "      <th>flagged_word</th>\n",
       "      <th>assignment</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>i am very happy with all that transpired between the design firm and the winning artist , alicia .</td>\n",
       "      <td>11</td>\n",
       "      <td>firm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>he took a vague concept and crafted a perfect logo for what we were looking for .</td>\n",
       "      <td>3</td>\n",
       "      <td>vague</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>even after lack of contact on my part , he was prompt in assisting me and making whatever changes i asked .</td>\n",
       "      <td>13</td>\n",
       "      <td>assisting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>thank you very much and i look forward to working together again .</td>\n",
       "      <td>10</td>\n",
       "      <td>together</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>really good , imaginative designer and pretty much nailed it first time</td>\n",
       "      <td>6</td>\n",
       "      <td>pretty</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>the first submission i received was the eventual winner , with only minor changes suggested by me .</td>\n",
       "      <td>8</td>\n",
       "      <td>winner</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>in this divided world we can only come together in our united pursuit of beauty in an artistic collaboration , and working with chris has been just that .</td>\n",
       "      <td>8</td>\n",
       "      <td>together</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>james did really great and went to great lengths to get it right for me .</td>\n",
       "      <td>7</td>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>will be working together in the future and am happy to have found them !</td>\n",
       "      <td>3</td>\n",
       "      <td>together</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>nick is amazingly knowledgeable in seo , and helped me better understand how my competitors were acquiring links and improving their search visibility .</td>\n",
       "      <td>11</td>\n",
       "      <td>understand</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>very pleased with initial work and looking forward to working together again .</td>\n",
       "      <td>10</td>\n",
       "      <td>together</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>a pleasure to work with you james and we will be doing lots together going forward .</td>\n",
       "      <td>13</td>\n",
       "      <td>together</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>a full custom build for a medium sized merchant is a huge job , with numerous tasks , inputs , opinions , and more .</td>\n",
       "      <td>16</td>\n",
       "      <td>tasks</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>i have tried to resolve the issue with naren but have not been able to .</td>\n",
       "      <td>13</td>\n",
       "      <td>able</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>perfect understanding of the project , quick to complete and very good quality report .</td>\n",
       "      <td>1</td>\n",
       "      <td>understanding</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>the design concepts created by this artist were instant favorites among the great competition .</td>\n",
       "      <td>13</td>\n",
       "      <td>competition</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>she also has a way of offering helpful suggestions while still considering the creative elements that i want to convey in my design .</td>\n",
       "      <td>13</td>\n",
       "      <td>creative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>he is able to bring characters to life and make them feel real .</td>\n",
       "      <td>11</td>\n",
       "      <td>feel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>i feel very fortunate to have found id design team .</td>\n",
       "      <td>1</td>\n",
       "      <td>feel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>the first logo has been a hit , and the second will be too .</td>\n",
       "      <td>13</td>\n",
       "      <td>too</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>i was looking for someone to create a sample of a page from the children 's book i would like illustrated .</td>\n",
       "      <td>14</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>working with ossob√ºko visual was easy , all my vague requests and comments were immediately taken into account .</td>\n",
       "      <td>9</td>\n",
       "      <td>vague</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>i have enjoyed working with chris and he has helped me to implement and strategize difficult and challenging situations .</td>\n",
       "      <td>15</td>\n",
       "      <td>difficult</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>i will definitely work with them again and look forward to continuing work and building a good working relationship .</td>\n",
       "      <td>18</td>\n",
       "      <td>relationship</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>could n't be happier and will definitely continue working together</td>\n",
       "      <td>9</td>\n",
       "      <td>together</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          review  \\\n",
       "25   i am very happy with all that transpired between the design firm and the winning artist , alicia .                                                            \n",
       "46   he took a vague concept and crafted a perfect logo for what we were looking for .                                                                             \n",
       "63   even after lack of contact on my part , he was prompt in assisting me and making whatever changes i asked .                                                   \n",
       "92   thank you very much and i look forward to working together again .                                                                                            \n",
       "96   really good , imaginative designer and pretty much nailed it first time                                                                                       \n",
       "97   the first submission i received was the eventual winner , with only minor changes suggested by me .                                                           \n",
       "106  in this divided world we can only come together in our united pursuit of beauty in an artistic collaboration , and working with chris has been just that .    \n",
       "139  james did really great and went to great lengths to get it right for me .                                                                                     \n",
       "143  will be working together in the future and am happy to have found them !                                                                                      \n",
       "149  nick is amazingly knowledgeable in seo , and helped me better understand how my competitors were acquiring links and improving their search visibility .      \n",
       "164  very pleased with initial work and looking forward to working together again .                                                                                \n",
       "168  a pleasure to work with you james and we will be doing lots together going forward .                                                                          \n",
       "206  a full custom build for a medium sized merchant is a huge job , with numerous tasks , inputs , opinions , and more .                                          \n",
       "240  i have tried to resolve the issue with naren but have not been able to .                                                                                      \n",
       "253  perfect understanding of the project , quick to complete and very good quality report .                                                                       \n",
       "257  the design concepts created by this artist were instant favorites among the great competition .                                                               \n",
       "274  she also has a way of offering helpful suggestions while still considering the creative elements that i want to convey in my design .                         \n",
       "287  he is able to bring characters to life and make them feel real .                                                                                              \n",
       "318  i feel very fortunate to have found id design team .                                                                                                          \n",
       "322  the first logo has been a hit , and the second will be too .                                                                                                  \n",
       "326  i was looking for someone to create a sample of a page from the children 's book i would like illustrated .                                                   \n",
       "329  working with ossob√ºko visual was easy , all my vague requests and comments were immediately taken into account .                                              \n",
       "402  i have enjoyed working with chris and he has helped me to implement and strategize difficult and challenging situations .                                     \n",
       "411  i will definitely work with them again and look forward to continuing work and building a good working relationship .                                         \n",
       "452  could n't be happier and will definitely continue working together                                                                                            \n",
       "\n",
       "     index   flagged_word  assignment  original  \n",
       "25   11     firm           0           0         \n",
       "46   3      vague          0           0         \n",
       "63   13     assisting      0           0         \n",
       "92   10     together       0           0         \n",
       "96   6      pretty         0           0         \n",
       "97   8      winner         0           0         \n",
       "106  8      together       0           0         \n",
       "139  7      great          0           0         \n",
       "143  3      together       0           0         \n",
       "149  11     understand     0           0         \n",
       "164  10     together       0           0         \n",
       "168  13     together       0           0         \n",
       "206  16     tasks          0           0         \n",
       "240  13     able           0           0         \n",
       "253  1      understanding  0           0         \n",
       "257  13     competition    0           0         \n",
       "274  13     creative       0           0         \n",
       "287  11     feel           0           0         \n",
       "318  1      feel           0           0         \n",
       "322  13     too            0           0         \n",
       "326  14     children       0           0         \n",
       "329  9      vague          0           0         \n",
       "402  15     difficult      0           0         \n",
       "411  18     relationship   0           0         \n",
       "452  9      together       0           0         "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP_cluster[FP_cluster.original == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
