{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## KAGGLE ONLY\nfrom shutil import copyfile\ncopyfile(src=\"../input/inputs/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\ncopyfile(src=\"../input/inputs/train_dataloader.p\", dst=\"../working/train_dataloader.p\")\ncopyfile(src=\"../input/inputs/val_dataloader.p\", dst=\"../working/val_dataloader.p\")\ncopyfile(src=\"../input/inputs/centroids_dataloader.p\", dst=\"../working/ground_truth_dataloader.p\")\ncopyfile(src=\"../input/inputs/dictionary.p\", dst=\"../working/dictionary.p\")\n\ncopyfile(src=\"../input/input2/train_unlabeld_dataloader.p\", dst=\"../working/train_unlabelled_dataloader.p\")\ncopyfile(src=\"../input/input2/train_labeled_dataloader.p\", dst=\"../working/train_labelled_dataloader.p\")","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"'../working/train_labelled_dataloader.p'"},"metadata":{}}]},{"metadata":{"colab":{},"colab_type":"code","id":"zno22FtJPX9z","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n#from datasets import get_mnist_dataset, get_data_loader\n#from utils import *\n#from models import *\n\nimport pickle as pkl\nimport os\nimport datetime as dt\nimport pandas as pd\nimport random\n\nfrom generate_dataloaders import *\n\nfrom tqdm import tqdm_notebook as tqdm\n\n# import evaluation\n# import importlib\n# importlib.reload(evaluation)","execution_count":2,"outputs":[]},{"metadata":{"colab_type":"text","id":"oaJEVd0wPX94"},"cell_type":"markdown","source":"## Get Dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1029\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\nnp.random.seed(seed)  # Numpy module.\nrandom.seed(seed)  # Python random module.\ntorch.manual_seed(seed)\ntorch.backends.cudnn.enabled = False \ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\ndef _init_fn(worker_id):\n    np.random.seed(int(seed))","execution_count":3,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"6nLzh007PX98","trusted":true},"cell_type":"code","source":"path = os.getcwd()\ndata_dir = path + '/'\n# data_dir = path +'/data/' #Uncomment for local system","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Verify filenames are consistent*"},{"metadata":{"colab":{},"colab_type":"code","id":"yq-jDGFIPX99","trusted":true},"cell_type":"code","source":"train_loader_labelled = pkl.load(open(data_dir + 'train_labelled_dataloader.p','rb'))\ntrain_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabelled_dataloader.p','rb'))\nval_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_dict = pkl.load(open(data_dir + 'dictionary.p','rb'))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%conda install pytorch torchvision -c pytorch\n## if torch.__version__ is not 1.3.1, run this cell then restart kernel","execution_count":7,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Lzz8lwNQPX-B","outputId":"690cb77f-2525-4c5a-ea14-a162716e34d3","trusted":true},"cell_type":"code","source":"print(torch.__version__)","execution_count":8,"outputs":[{"output_type":"stream","text":"1.1.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## PRE TRAINED WORD EMBEDDINGS "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float16')","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(review_dict, embedding_index ,dim = 200):\n#     embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(review_dict.tokens), dim))\n    unknown_words = []\n    \n    for word, i in review_dict.ids.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_twitter = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt' #Change loc for local system\n# glove_twitter = data_dir + 'glove.twitter.27B.200d.txt'","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_index = load_embeddings(glove_twitter)","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"426ad9cce0324fdfb94fd9164dc8ca53"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embedding_index,unknown_words = build_matrix(review_dict, embedding_index)\ndel embedding_index","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(review_dict.tokens)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"16256"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(unknown_words)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"4428"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for word in unknown_words:\n#     print(word)","execution_count":17,"outputs":[]},{"metadata":{"colab_type":"text","id":"Cvt6N9QCPX-X"},"cell_type":"markdown","source":"## Neural Network LSTM Class"},{"metadata":{"colab_type":"text","id":"puweJhdxPX-Y"},"cell_type":"markdown","source":"NOTE: Data loader is defined as:\n- tuple: (tokens, flagged_index, problematic)"},{"metadata":{"colab":{},"colab_type":"code","id":"W8BZ-QhNPX-Z","trusted":true},"cell_type":"code","source":"class LSTM_model(nn.Module):\n    \"\"\"\n    LSTM classification model using pretrained glove embeddings\n    \"\"\"\n    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n    def __init__(self, embedding_matrix, num_hidden_layers = 3, hidden_size = 100, num_classes = 2):\n        super(LSTM_model, self).__init__()\n        vocab_size = embedding_matrix.shape[0]\n        embed_size = embedding_matrix.shape[1]\n        \n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.num_hidden_layers = num_hidden_layers\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        \n        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)    \n        self.embed.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embed.weight.requires_grad = False\n        \n        self.lstm = nn.LSTM(self.embed_size,self.hidden_size,self.num_hidden_layers, batch_first=True,bidirectional= True,bias=True)\n        \n        self.projection = nn.Linear(2*self.hidden_size, self.num_classes, bias=True)\n\n    \n    def forward(self, tokens, flagged_index):\n        batch_size, num_tokens = tokens.shape\n        embedding = self.embed(tokens)\n#         print(embedding.shape) # below assumes \"batch_size x num_tokens x Emb_dim\" (VERIFY)\n        \n        lstm_output = self.lstm(embedding)\n        # lstm_output is a tuple containing lstm output and (hidden_state, lstm_cell). \n        # lstm_output[0] would be of shape \"batch_size x num_tokens x hidden_size\" (VERIFY)\n        \n        logits = self.projection(lstm_output[0])\n        # logits would be of shape \"batch_size x num_tokens x num_classes (2)\" (VERIFY)\n        \n        batch_size, _, __ = logits.shape\n        \n        #selecting the logit at the flagged index\n        relevant_logits = logits[list(range(batch_size)),flagged_index]\n        # relevant_logits would be of shape \"batch_size x num_classes (2)\" (VERIFY)\n        \n        return relevant_logits","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First performing fully supervised learning using the labelled set to train new vector representations"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\nif num_gpus > 0:\n    current_device = 'cuda'\nelse:\n    current_device = 'cpu'\n\nmodel = LSTM_model(glove_embedding_index, num_hidden_layers = 3, hidden_size = 100, num_classes = 2).to(current_device)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(reduction='sum')\noptimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scratch work to understand the lstm output and the loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,X in enumerate(val_loader):\n    tokens_labelled, labels, flagged_indices_labelled, = X\n    break","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_labelled = tokens_labelled.to(current_device)\nflagged_indices_labelled = flagged_indices_labelled.to(current_device)\nlabels = labels.to(current_device)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_labelled.shape","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"torch.Size([32, 30])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = model.embed(tokens_labelled)\nembeddings.shape","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"torch.Size([32, 30, 200])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_output = model.lstm(embeddings)\nlstm_output[0].shape","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"torch.Size([32, 30, 200])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logits = model.projection(lstm_output[0])\nlogits.shape","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"torch.Size([32, 30, 2])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_logits = logits[list(range(32)),flagged_indices_labelled]\nrelevant_logits.shape","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"torch.Size([32, 2])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.shape","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"torch.Size([32])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = criterion(relevant_logits,labels)\nloss","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"tensor(21.9358, grad_fn=<NllLossBackward>)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Supervised model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_supervised_model(model, criterion, optimizer, train_loader_labelled, valid_loader, num_epochs=10, path_to_save=None, print_every = 1000):\n\n    train_losses=[]\n    val_losses=[]\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 0:\n        current_device = 'cuda'\n    else:\n        current_device = 'cpu'\n    \n    for epoch in range(num_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.train()\n        total_epoch_loss = 0\n        \n        for i,(tokens_labelled, labels, flagged_indices_labelled) in enumerate(train_loader_labelled):\n            \n            tokens_labelled = tokens_labelled.to(current_device)\n            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n            labels = labels.to(current_device)\n\n            # forward pass and compute loss\n            logits = model(tokens_labelled,flagged_indices_labelled)\n            \n            loss = criterion(logits, labels)\n        \n            # run update step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            #Add loss to the epoch loss\n            total_epoch_loss += loss\n\n            if i % print_every == 0:\n                losses = loss/len(tokens_labelled)\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= len(train_loader_labelled.dataset)\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n            model.eval()\n            tokens = tokens.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices = flagged_indices.to(current_device)\n            \n            # forward pass and compute loss\n            logits = model(tokens,flagged_indices)\n            \n            loss = criterion(logits, labels)\n            \n            #Add loss to the validation loss\n            total_validation_loss += loss\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"vocab_size\":model.vocab_size, \"embed_size\": model.embed_size}\n            torch.save(model.state_dict(), path_to_save+'model_dict.pt')\n            torch.save(train_losses, path_to_save+'train_losses')\n            torch.save(val_losses, path_to_save+'val_losses')\n            torch.save(opts, path_to_save+'opts')\n        \n    return model, train_losses, val_losses","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = os.getcwd()\nmodel_dir = path","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_loader_labelled)","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"27"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model, train_losses, val_losses = train_supervised_model(model, criterion, optimizer, train_loader_labelled, val_loader, num_epochs=10, path_to_save=model_dir)\n","execution_count":33,"outputs":[{"output_type":"stream","text":"2019-11-22 21:04:50.251646 | Epoch 0\nAverage training loss at batch  0 : 0.679\nAverage training loss after epoch  0 : 0.560\nAverage validation loss after epoch  0 : 0.254\n2019-11-22 21:04:55.801111 | Epoch 1\nAverage training loss at batch  0 : 0.302\nAverage training loss after epoch  1 : 0.307\nAverage validation loss after epoch  1 : 0.165\n2019-11-22 21:05:01.663792 | Epoch 2\nAverage training loss at batch  0 : 0.212\nAverage training loss after epoch  2 : 0.224\nAverage validation loss after epoch  2 : 0.155\n2019-11-22 21:05:07.570111 | Epoch 3\nAverage training loss at batch  0 : 0.109\nAverage training loss after epoch  3 : 0.166\nAverage validation loss after epoch  3 : 0.190\n2019-11-22 21:05:13.440210 | Epoch 4\nAverage training loss at batch  0 : 0.103\nAverage training loss after epoch  4 : 0.151\nAverage validation loss after epoch  4 : 0.159\n2019-11-22 21:05:19.358166 | Epoch 5\nAverage training loss at batch  0 : 0.060\nAverage training loss after epoch  5 : 0.094\nAverage validation loss after epoch  5 : 0.141\n2019-11-22 21:05:25.253327 | Epoch 6\nAverage training loss at batch  0 : 0.095\nAverage training loss after epoch  6 : 0.052\nAverage validation loss after epoch  6 : 0.192\n2019-11-22 21:05:31.222772 | Epoch 7\nAverage training loss at batch  0 : 0.006\nAverage training loss after epoch  7 : 0.038\nAverage validation loss after epoch  7 : 0.211\n2019-11-22 21:05:37.167261 | Epoch 8\nAverage training loss at batch  0 : 0.008\nAverage training loss after epoch  8 : 0.094\nAverage validation loss after epoch  8 : 0.183\n2019-11-22 21:05:43.060699 | Epoch 9\nAverage training loss at batch  0 : 0.033\nAverage training loss after epoch  9 : 0.045\nAverage validation loss after epoch  9 : 0.250\n","name":"stdout"}]},{"metadata":{"colab_type":"text","id":"SGsqcnEtPX-a"},"cell_type":"markdown","source":"### Clustering Stuff (un-tailored)"},{"metadata":{"colab":{},"colab_type":"code","id":"MrgIYm8JPX-b","trusted":true},"cell_type":"code","source":"class KMeansCriterion(nn.Module):\n    \n    def __init__(self, lmbda):\n        super().__init__()\n        self.lmbda = lmbda\n    \n    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n        if labelled:\n            num_reviews = len(cluster_assignments)\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n            loss = self.lmbda * cluster_distances.sum()\n        else:\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances, cluster_assignments = distances.min(1)\n            loss = self.lmbda * cluster_distances.sum()\n        return loss, cluster_assignments","execution_count":36,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"-TJohK2aPX-d","trusted":true},"cell_type":"code","source":"def centroid_init(k, d, dataloader, model, current_device):\n    ## Here we ideally don't want to do randomized/zero initialization\n    centroid_sums = torch.zeros(k, d).to(current_device)\n    centroid_counts = torch.zeros(k).to(current_device)\n    for (tokens, labels, flagged_indices) in dataloader:\n        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n        cluster_assignments = labels.to(current_device)\n        \n        model.eval()\n        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n    \n        update_clusters(centroid_sums, centroid_counts,\n                        cluster_assignments, sentence_embed.to(current_device))\n    \n    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n    return centroid_means.clone()\n\ndef update_clusters(centroid_sums, centroid_counts,\n                    cluster_assignments, embeddings):\n    k = centroid_sums.size(0)\n\n    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n    centroid_counts.add_(bin_counts)","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataloader stuff"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n    try:\n        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n    except StopIteration:\n        train_loader_labelled_iter = iter(train_loader_labelled)\n        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n\n    return tokens, labels, flagged_indices, train_loader_labelled_iter\n\n\ndef loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n    try:\n        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n    except StopIteration:\n        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n\n    return tokens, labels, flagged_indices, train_loader_unlabelled_iter","execution_count":38,"outputs":[]},{"metadata":{"colab_type":"text","id":"u3wynM7fPX-h"},"cell_type":"markdown","source":"### Training Function (un-tailored, needs alterations)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_clusters(model, centroids, criterion, optimizer, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=10, num_batches = 1000, path_to_save=None, print_every = 1000):\n\n    train_loader_labelled_iter = iter(train_loader_labelled)\n    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n\n    train_losses=[]\n    val_losses=[]\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 0:\n        current_device = 'cuda'\n    else:\n        current_device = 'cpu'\n    \n    for epoch in range(num_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.eval()\n        k, d = centroids.size()\n        centroid_sums = torch.zeros_like(centroids).to(current_device)\n        centroid_counts = torch.zeros(k).to(current_device)\n        total_epoch_loss = 0\n        \n        for i in range(num_batches):\n            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n\n            tokens_labelled = tokens_labelled.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n            \n            tokens_unlabelled = tokens_unlabelled.to(current_device)\n            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n\n            # forward pass and compute loss\n            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n            \n            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n    \n            total_batch_loss = cluster_loss_labelled.data + cluster_loss_unlabelled.data\n            \n#             #Add loss to the epoch loss\n            total_epoch_loss += total_batch_loss\n\n#             # store centroid sums and counts in memory for later centering\n            update_clusters(centroid_sums, centroid_counts,\n                            cluster_assignments_labelled, sentence_embed_labelled)\n    \n            update_clusters(centroid_sums, centroid_counts,\n                            cluster_assignments_unlabelled, sentence_embed_unlabelled)\n\n            if i % print_every == 0:\n                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # update centroids based on assignments from autoencoders\n        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n            model.eval()\n            tokens = tokens.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices = flagged_indices.to(current_device)\n            \n            # forward pass and compute loss\n            sentence_embed = model(tokens,flagged_indices)\n            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n            \n            #Add loss to the validation loss\n            total_validation_loss += cluster_loss.data\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"vocab_size\":model.vocab_size, \"embed_size\": model.embed_size}\n            torch.save(model.state_dict(), path_to_save+'model_dict.pt')\n            torch.save(centroids, path_to_save+'centroids')\n            torch.save(train_losses, path_to_save+'train_losses')\n            torch.save(val_losses, path_to_save+'val_losses')\n            torch.save(opts, path_to_save+'opts')\n            \n        \n    return model, centroids, train_losses, val_losses","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"0pBet75ZPX-m","trusted":true},"cell_type":"code","source":"unsupervised_model = model\nunsupervised_model.projection = nn.Identity()","execution_count":39,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"mTFO2vp-PX-o","trusted":true},"cell_type":"code","source":"# model = neuralNetBow(opts['vocab_size'], opts['emb_dim'])\ncentroids = centroid_init(2, 200,train_loader_labelled, unsupervised_model, current_device)\ncriterion = KMeansCriterion(1).to(current_device)\noptimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)","execution_count":40,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Xya2NiqcPX-q","outputId":"59b3072e-c567-4e18-a242-ba8298f08e58","scrolled":true,"trusted":true},"cell_type":"code","source":"centroids.shape","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"tensor([[ 4.6505e-01,  7.8163e-01,  4.8528e-01, -4.6817e-03,  1.9037e-02,\n          1.6668e-01,  1.5806e-01,  2.1668e-04,  2.7631e-01,  1.3465e-01,\n          8.0866e-01, -4.4393e-01,  3.4200e-02,  2.6709e-03, -1.8867e-02,\n         -2.3933e-01, -2.6263e-01, -6.3401e-01, -2.9267e-01, -3.3880e-01,\n          3.9647e-01,  1.1013e-01, -6.1875e-03, -2.3355e-03, -2.1256e-01,\n         -8.5083e-01, -8.6541e-01,  2.5603e-01,  1.7084e-01,  2.1494e-02,\n         -6.6351e-01, -3.8501e-02, -1.3520e-01, -5.3624e-02, -2.7725e-02,\n         -1.6295e-02, -4.4198e-02, -9.1232e-02,  8.4665e-03,  1.6025e-01,\n         -1.2624e-01, -3.1942e-01, -5.3474e-02, -4.1226e-01, -7.0297e-01,\n         -1.9236e-01,  3.1778e-02,  4.0695e-01, -2.9812e-01, -4.6574e-02,\n          2.8392e-01,  7.4745e-01, -3.1627e-01,  7.7724e-02,  2.9165e-01,\n          2.3843e-01, -3.7677e-02,  3.6977e-01,  9.2153e-02,  4.5235e-01,\n          7.1038e-01, -1.5485e-02,  2.5018e-01, -4.1925e-01,  1.2048e-03,\n          4.1945e-01, -4.0571e-01,  6.8488e-03,  3.4549e-01, -2.6939e-01,\n         -5.1882e-01, -1.9797e-01,  3.4712e-01, -3.8454e-01,  2.4270e-01,\n         -1.2107e-02, -6.2372e-03, -8.6229e-01,  3.2407e-01,  8.2416e-01,\n         -1.9051e-02, -8.9503e-02,  7.0812e-02,  7.5755e-02, -9.3794e-02,\n         -3.4503e-05,  1.1086e-01,  1.5956e-02, -9.9221e-02,  5.2783e-01,\n          4.4122e-01, -9.4212e-03, -7.3619e-01, -6.1630e-01,  2.9958e-01,\n          4.7019e-01, -4.4752e-01, -5.4531e-01,  7.3609e-01,  3.1828e-04,\n         -2.4276e-01, -4.8092e-03,  4.8557e-03,  6.7733e-01,  4.2261e-02,\n         -6.8507e-01, -3.7417e-02, -1.0531e-02,  3.8501e-03,  3.9692e-03,\n         -2.1654e-03, -4.0785e-03, -9.0136e-02, -1.9148e-01,  4.0222e-02,\n         -4.5336e-02, -5.1629e-01,  7.4319e-01, -1.7029e-01, -1.1124e-02,\n         -3.2850e-03,  4.8765e-02, -1.4068e-02,  9.0306e-03,  6.2865e-01,\n          5.7995e-02, -6.1602e-03,  2.0775e-03,  1.6905e-02,  8.0363e-03,\n          8.3437e-02, -8.2710e-01, -3.8004e-02,  8.8614e-02, -2.0055e-02,\n         -5.0947e-03, -6.9877e-01, -4.0103e-03,  5.7748e-01,  9.8948e-02,\n         -8.5162e-01, -2.0309e-03,  3.8456e-02, -1.5250e-03,  1.2712e-02,\n         -2.7892e-02, -3.6904e-03,  2.1672e-01, -1.5718e-01, -7.9939e-03,\n          1.5112e-01, -8.0322e-01,  1.0050e-05,  1.7491e-01, -7.5395e-03,\n          2.9378e-02,  9.7087e-01,  1.4665e-02, -3.2969e-01, -1.8725e-01,\n          2.4491e-03, -6.6797e-01,  3.3802e-03,  8.7324e-04, -8.4979e-02,\n          2.8263e-01, -3.9050e-01, -5.6737e-02,  3.8443e-03, -1.7290e-01,\n         -9.0645e-02,  3.3141e-04,  2.4533e-01, -1.1759e-01, -1.5529e-01,\n         -9.3857e-03,  6.1903e-01,  1.2194e-03,  2.0055e-01, -2.7522e-03,\n         -5.4267e-04, -2.2625e-02, -3.3808e-04, -9.5589e-01, -3.7471e-02,\n         -1.1758e-01,  8.8642e-01, -4.9869e-04, -9.4537e-03, -1.4485e-01,\n         -3.5932e-02,  3.5555e-01,  9.4777e-01, -1.5520e-02, -7.1084e-03,\n          1.1360e-01,  1.5663e-02,  8.0302e-01, -1.2693e-01, -3.1314e-03],\n        [ 3.0848e-03,  2.2454e-03, -5.5692e-03,  6.8390e-01, -6.4056e-01,\n         -2.1774e-02, -4.2189e-01,  5.0998e-01, -2.4991e-02, -4.3327e-01,\n         -2.2504e-03,  2.7535e-03,  1.0632e-03,  7.6633e-01,  1.4423e-03,\n          4.1466e-03, -1.4982e-03, -1.5373e-03, -5.6000e-04,  1.2622e-03,\n          1.4997e-01, -2.0465e-02,  1.9262e-01,  8.1269e-01,  6.2811e-02,\n          2.2700e-03, -3.3676e-03, -5.5786e-03, -2.5947e-02,  3.7384e-01,\n          2.0665e-02, -2.4623e-03,  7.4418e-03,  2.1858e-02,  9.7300e-03,\n          2.0036e-04, -4.4834e-03,  1.0623e-01, -6.2465e-03, -1.3319e-03,\n          4.9251e-01, -3.7033e-05,  1.3718e-02,  4.0711e-03,  3.4978e-04,\n          5.7515e-02, -3.8070e-01, -1.0414e-03,  1.0943e-02,  1.7485e-03,\n         -2.1664e-01,  3.3249e-03,  3.6453e-03, -1.5404e-02, -5.9412e-02,\n          1.5027e-01,  7.1851e-01, -2.1288e-02, -3.0152e-03, -7.1444e-02,\n         -7.4231e-01,  1.6745e-05, -2.1179e-01,  6.0625e-01, -8.4598e-01,\n         -3.8786e-04,  5.8089e-03, -4.8509e-04, -4.5459e-04, -1.8964e-03,\n          3.0984e-01,  6.3164e-01, -1.2851e-03,  7.6425e-02, -5.4693e-01,\n          9.7366e-03, -8.6528e-01, -3.6908e-03, -1.3427e-02,  2.5142e-03,\n          5.7518e-01,  8.7278e-02, -5.0581e-01,  5.8992e-03, -1.6212e-03,\n         -8.1221e-01, -5.0921e-03,  2.6789e-01,  2.2120e-02, -5.1624e-03,\n          1.2404e-03,  1.3469e-04,  5.8352e-01,  5.6262e-01,  5.5822e-04,\n         -1.3864e-02,  2.5177e-02,  4.4026e-03, -8.1284e-04, -8.8360e-01,\n          1.6266e-03, -2.8521e-03,  1.8832e-02,  1.5365e-03,  9.8401e-03,\n          7.2854e-01, -4.0598e-03,  7.6970e-01, -1.8169e-02,  3.6294e-02,\n          4.9692e-04,  7.2037e-01,  4.4549e-01,  1.8122e-03, -5.0736e-01,\n          2.3332e-03, -3.5225e-03,  1.5045e-03,  1.1109e-04, -3.3918e-01,\n         -9.6776e-01, -8.3486e-01, -4.4525e-04, -8.0763e-01, -2.3958e-02,\n         -4.6046e-01,  7.0373e-01, -9.6575e-01, -1.3217e-02,  8.8031e-01,\n          2.2748e-04, -3.4130e-04, -3.7947e-03,  1.2890e-03,  4.2662e-03,\n          7.4043e-03,  4.0269e-04,  2.3488e-03,  8.2130e-04, -3.0701e-03,\n          5.1999e-03,  7.7879e-01, -4.2015e-03,  7.2369e-01, -8.6395e-01,\n          4.4986e-01, -9.5306e-01, -2.1505e-01,  4.1036e-03,  8.9280e-01,\n         -2.3733e-03, -1.4506e-04,  3.1380e-02, -1.0882e-03,  5.9696e-01,\n         -3.6457e-03,  4.4916e-03, -6.8087e-01,  1.9752e-03,  3.9470e-01,\n         -6.2631e-01,  6.5280e-01, -6.2407e-01, -5.2448e-01,  7.5784e-01,\n         -8.6271e-04,  8.6551e-05,  4.6085e-01,  6.1089e-03,  1.8832e-03,\n          7.0117e-01, -5.5116e-01, -1.6883e-03,  2.4624e-04,  3.5122e-03,\n          8.5479e-01, -1.4710e-03, -8.4343e-01, -1.3567e-04,  8.5737e-01,\n          8.6215e-01,  2.5546e-03,  6.6655e-01,  8.1880e-03,  4.6301e-01,\n         -1.3527e-02,  2.5269e-03, -6.0060e-03,  3.2154e-03,  3.4641e-03,\n          8.2703e-01, -1.6037e-03,  3.1474e-03,  5.7810e-02,  1.7263e-02,\n         -4.7314e-03,  2.7760e-03, -2.7837e-01, -8.7179e-04, -9.5188e-01]],\n       grad_fn=<CloneBackward>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = os.getcwd()\nmodel_folder= 'baseline_semisupervised_frozen_glove/'\nmodel_dir = path + '/models/' + model_folder\nmodel_dir = path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_batches = int(len(train_loader_unlabelled.dataset)/train_loader_unlabelled.batch_size)+1\nnum_batches","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"colab_type":"code","executionInfo":{"elapsed":8813,"status":"error","timestamp":1573355511003,"user":{"displayName":"Eileen Cho","photoUrl":"","userId":"03381570147993013394"},"user_tz":300},"id":"rgwMd27mPX-u","outputId":"063ebc41-be3c-4474-d92c-7bd0680bb366","scrolled":true,"trusted":true},"cell_type":"code","source":"lstm_model, lstm_centroids, lstm_train_losses, lstm_val_losses = train_clusters(unsupervised_model, centroids, criterion, optimizer, train_loader_labelled,train_loader_unlabelled, val_loader, num_epochs=10, num_batches=num_batches, path_to_save=model_dir)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# #Only needed for Kaggle\n\n# from IPython.display import FileLink, FileLinks \n# FileLinks('.') #lists all downloadable files on server","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"model_tuning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}