{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train embedding representations using semi-supervised clustering\n",
    "\n",
    "This notebook trains  embedding representations using semi-supervised models over the training dataset, initialized with pre-trained GloVe embeddings. In this context, \"semi-supervised\" means that in addition to using labeled data to initialize cluster centroids, we also force assignment of sentences with known labels to the corresponding cluster, thus greatly penalizing known-misclassified sentences in calculation of loss function.\n",
    "\n",
    "This code has the following dependencies:\n",
    "- training dataloaders stored in `data` folder, as generated by `data_prep/generate_dataloders.ipynb`\n",
    "- custom functions found in `data_prep/generate_dataloaders.py`\n",
    "- custom functions found in `evaluation/evaluation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zno22FtJPX9z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation.evaluation' from '/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/evaluation/evaluation.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle as pkl\n",
    "import datetime as dt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import scripts from data_prep & evaluation folders\n",
    "path = os.getcwd()\n",
    "parentdir = os.path.dirname(path)\n",
    "sys.path.append(parentdir)\n",
    "sys.path.append(parentdir + '/data_prep')\n",
    "from generate_dataloaders import *\n",
    "from evaluation import evaluation\n",
    "\n",
    "import importlib\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaJEVd0wPX94"
   },
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1029\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nLzh007PX98"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/'\n",
    "data_dir = path +'/data/' #Uncomment for local system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Verify filenames are consistent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yq-jDGFIPX99"
   },
   "outputs": [],
   "source": [
    "train_loader = pkl.load(open(data_dir + 'train_dataloader.p','rb'))\n",
    "train_loader_labelled = pkl.load(open(data_dir + 'train_labeled_dataloader.p','rb'))\n",
    "train_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabeled_dataloader.p','rb'))\n",
    "val_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict = pkl.load(open(data_dir + 'dictionary.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install pytorch torchvision -c pytorch\n",
    "## if torch.__version__ is not 1.3.1, run this cell then restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzz8lwNQPX-B",
    "outputId": "690cb77f-2525-4c5a-ea14-a162716e34d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE TRAINED WORD EMBEDDINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(review_dict, embedding_index ,dim = 200):\n",
    "#     embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(review_dict.tokens), dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in review_dict.ids.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_twitter = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt' #Change loc for local system\n",
    "glove_twitter = data_dir + 'glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040f046cd44040a4816dc477a078ec92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_index = load_embeddings(glove_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_index,unknown_words = build_matrix(review_dict, embedding_index)\n",
    "del embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16256"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_dict.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4428"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in unknown_words:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvt6N9QCPX-X"
   },
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puweJhdxPX-Y"
   },
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8BZ-QhNPX-Z"
   },
   "outputs": [],
   "source": [
    "class neuralNetBow_glove(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n",
    "    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n",
    "    def __init__(self, opts):\n",
    "        super(neuralNetBow_glove, self).__init__()\n",
    "        self.embedding_matrix = opts['embedding_matrix']\n",
    "        self.vocab_size = self.embedding_matrix.shape[0]\n",
    "        self.embed_size = self.embedding_matrix.shape[1]\n",
    "        self.upweight = opts['upweight']\n",
    "        self.lambda_loss = opts['lambda_loss']\n",
    "        \n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=0)\n",
    "        self.embed.weight = nn.Parameter(torch.tensor(self.embedding_matrix,dtype=torch.float32))\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        self.upweight = upweight\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        batch_size, num_tokens = tokens.shape\n",
    "        embedding = self.embed(tokens)\n",
    "        \n",
    "        # upweight by flagged_index\n",
    "        embedding[torch.LongTensor(range(batch_size)),flagged_index.type(torch.LongTensor),:] *= self.upweight\n",
    "        \n",
    "        # average across embeddings\n",
    "        embedding_ave = embedding.sum(1) / (num_tokens + self.upweight - 1)\n",
    "        \n",
    "        return embedding_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGsqcnEtPX-a"
   },
   "source": [
    "### Clustering Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrgIYm8JPX-b"
   },
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n",
    "        if labelled:\n",
    "            num_reviews = len(cluster_assignments)\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n",
    "            loss = cluster_distances.sum()\n",
    "        else:\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances, cluster_assignments = distances.min(1)\n",
    "            loss = cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TJohK2aPX-d"
   },
   "outputs": [],
   "source": [
    "def centroid_init(k, d, dataloader, model, current_device):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = torch.zeros(k, d).to(current_device)\n",
    "    centroid_counts = torch.zeros(k).to(current_device)\n",
    "    for (tokens, labels, flagged_indices) in dataloader:\n",
    "        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n",
    "        cluster_assignments = labels.to(current_device)\n",
    "        \n",
    "        model.eval()\n",
    "        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n",
    "    \n",
    "        update_clusters(centroid_sums, centroid_counts,\n",
    "                        cluster_assignments, sentence_embed.to(current_device))\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n",
    "    centroid_counts.add_(bin_counts)\n",
    "    \n",
    "    #np_cluster_assignments = cluster_assignments.to('cpu')\n",
    "    #np_counts = np.bincount(np_cluster_assignments.data.numpy(), minlength=k)\n",
    "    #centroid_counts.add_(torch.FloatTensor(np_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_labelled_iter\n",
    "\n",
    "\n",
    "def loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_unlabelled_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3wynM7fPX-h"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, centroids, criterion, train_loader_labelled, train_loader_unlabelled, valid_loader, num_frozen_epochs=10, num_unfrozen_epochs=0, num_batches = 1000, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "    lambda_loss = model.lambda_loss\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    # freeze part\n",
    "    freeze_model(model)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    \n",
    "    for epoch in tqdm(range(num_frozen_epochs)):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n",
    "            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n",
    "\n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            \n",
    "            tokens_unlabelled = tokens_unlabelled.to(current_device)\n",
    "            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n",
    "            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n",
    "            \n",
    "            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n",
    "            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n",
    "    \n",
    "            total_batch_loss = cluster_loss_unlabelled.data + lambda_loss * cluster_loss_labelled.data\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "#             total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add loss to the epoch loss\n",
    "            total_epoch_loss += total_batch_loss\n",
    "\n",
    "#             # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments_labelled, sentence_embed_labelled)\n",
    "    \n",
    "            update_clusters(centroid_sums, centroid_counts,\n",
    "                            cluster_assignments_unlabelled, sentence_embed_unlabelled)\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict.pt')\n",
    "            torch.save(centroids, path_to_save+'centroids')\n",
    "            torch.save(train_losses, path_to_save+'train_losses')\n",
    "            torch.save(val_losses, path_to_save+'val_losses')\n",
    "            torch.save(opts, path_to_save+'opts')\n",
    "    \n",
    "    # unfreeze part\n",
    "    print(\"*** UNFREEZING MODEL ***\")\n",
    "    unfreeze_model(model)\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    \n",
    "    for epoch in tqdm(range(num_unfrozen_epochs)):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n",
    "            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n",
    "\n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            \n",
    "            tokens_unlabelled = tokens_unlabelled.to(current_device)\n",
    "            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n",
    "            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n",
    "            \n",
    "            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n",
    "            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n",
    "    \n",
    "            total_batch_loss = cluster_loss_unlabelled + lambda_loss * cluster_loss_labelled\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add loss to the epoch loss\n",
    "            total_epoch_loss += total_batch_loss.data\n",
    "\n",
    "#             # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_labelled.detach(), sentence_embed_labelled.detach())\n",
    "    \n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_unlabelled.detach(), sentence_embed_unlabelled.detach())\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict.pt')\n",
    "            torch.save(centroids, path_to_save+'centroids')\n",
    "            torch.save(train_losses, path_to_save+'train_losses')\n",
    "            torch.save(val_losses, path_to_save+'val_losses')\n",
    "            torch.save(opts, path_to_save+'opts')\n",
    "            \n",
    "        \n",
    "    return model, centroids, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pBet75ZPX-m"
   },
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTFO2vp-PX-o"
   },
   "outputs": [],
   "source": [
    "def get_save_directory(opts):\n",
    "    path = os.getcwd()\n",
    "    model_folder = 'baseline_semisupervised_frozen_glove/'\n",
    "    model_dir = path + '/models/' + model_folder\n",
    "    \n",
    "    # subfolder for each hyperparam config\n",
    "    num_unfrozen_epochs = opts['num_unfrozen_epochs']\n",
    "    upweight = opts['upweight']\n",
    "    lambda_loss = opts['lambda_loss']\n",
    "    subfolder = \"num_unfrozen_epochs=\"+str(num_unfrozen_epochs) + \",upweight=\"+str(upweight) + \",lambda=\"+str(lambda_loss) + '/'\n",
    "    \n",
    "    # need to actually create these subfolders lol\n",
    "    try:\n",
    "        os.makedirs(model_dir + subfolder) # will throw error if subfolder already exists\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return model_dir + subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xya2NiqcPX-q",
    "outputId": "59b3072e-c567-4e18-a242-ba8298f08e58",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_config(opts):\n",
    "    model = neuralNetBow_glove(opts).to(current_device)\n",
    "    num_unfrozen_epochs = opts['num_unfrozen_epochs']\n",
    "    centroids = centroid_init(2, 200, train_loader_labelled, model, current_device)\n",
    "    criterion = KMeansCriterion().to(current_device)\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    print(path_to_save)\n",
    "    \n",
    "    train_model(model, centroids, criterion, train_loader_labelled, train_loader_unlabelled, val_loader, num_frozen_epochs=10, num_unfrozen_epochs=num_unfrozen_epochs, path_to_save=path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=3,upweight=25,lambda=0.1/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89869a7a81cd4dd0a287310c6254d010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 09:54:28.848303 | Epoch 0\n",
      "Average training loss at batch  0 : 2.509\n",
      "Average training loss after epoch  0 : 1.490\n",
      "Average validation loss after epoch  0 : 4.796\n",
      "2019-12-08 09:54:30.448626 | Epoch 1\n",
      "Average training loss at batch  0 : 2.910\n",
      "Average training loss after epoch  1 : 1.476\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 09:54:32.000215 | Epoch 2\n",
      "Average training loss at batch  0 : 2.947\n",
      "Average training loss after epoch  2 : 1.470\n",
      "Average validation loss after epoch  2 : 4.787\n",
      "2019-12-08 09:54:33.501372 | Epoch 3\n",
      "Average training loss at batch  0 : 2.856\n",
      "Average training loss after epoch  3 : 1.471\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 09:54:35.156478 | Epoch 4\n",
      "Average training loss at batch  0 : 2.484\n",
      "Average training loss after epoch  4 : 1.471\n",
      "Average validation loss after epoch  4 : 4.787\n",
      "2019-12-08 09:54:36.789977 | Epoch 5\n",
      "Average training loss at batch  0 : 2.128\n",
      "Average training loss after epoch  5 : 1.469\n",
      "Average validation loss after epoch  5 : 4.784\n",
      "2019-12-08 09:54:38.329367 | Epoch 6\n",
      "Average training loss at batch  0 : 2.406\n",
      "Average training loss after epoch  6 : 1.465\n",
      "Average validation loss after epoch  6 : 4.781\n",
      "2019-12-08 09:54:40.314963 | Epoch 7\n",
      "Average training loss at batch  0 : 2.584\n",
      "Average training loss after epoch  7 : 1.467\n",
      "Average validation loss after epoch  7 : 4.775\n",
      "2019-12-08 09:54:42.078264 | Epoch 8\n",
      "Average training loss at batch  0 : 2.772\n",
      "Average training loss after epoch  8 : 1.465\n",
      "Average validation loss after epoch  8 : 4.769\n",
      "2019-12-08 09:54:43.839500 | Epoch 9\n",
      "Average training loss at batch  0 : 2.284\n",
      "Average training loss after epoch  9 : 1.462\n",
      "Average validation loss after epoch  9 : 4.763\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874125627ad644d69472cb8b99f62150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 09:54:45.625110 | Epoch 0\n",
      "Average training loss at batch  0 : 2.649\n",
      "Average training loss after epoch  0 : 0.193\n",
      "Average validation loss after epoch  0 : 0.189\n",
      "2019-12-08 09:55:15.803976 | Epoch 1\n",
      "Average training loss at batch  0 : 0.083\n",
      "Average training loss after epoch  1 : 0.038\n",
      "Average validation loss after epoch  1 : 0.076\n",
      "2019-12-08 09:55:50.504594 | Epoch 2\n",
      "Average training loss at batch  0 : 0.032\n",
      "Average training loss after epoch  2 : 0.019\n",
      "Average validation loss after epoch  2 : 0.049\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=3,upweight=25,lambda=0.5/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8979ff4ed6ff4d52be04bb2e557af98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 09:56:30.533186 | Epoch 0\n",
      "Average training loss at batch  0 : 3.684\n",
      "Average training loss after epoch  0 : 2.095\n",
      "Average validation loss after epoch  0 : 4.795\n",
      "2019-12-08 09:56:32.276397 | Epoch 1\n",
      "Average training loss at batch  0 : 2.917\n",
      "Average training loss after epoch  1 : 2.070\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 09:56:33.998025 | Epoch 2\n",
      "Average training loss at batch  0 : 3.339\n",
      "Average training loss after epoch  2 : 2.063\n",
      "Average validation loss after epoch  2 : 4.786\n",
      "2019-12-08 09:56:35.476260 | Epoch 3\n",
      "Average training loss at batch  0 : 3.425\n",
      "Average training loss after epoch  3 : 2.066\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 09:56:37.025540 | Epoch 4\n",
      "Average training loss at batch  0 : 3.670\n",
      "Average training loss after epoch  4 : 2.068\n",
      "Average validation loss after epoch  4 : 4.787\n",
      "2019-12-08 09:56:38.578993 | Epoch 5\n",
      "Average training loss at batch  0 : 3.027\n",
      "Average training loss after epoch  5 : 2.059\n",
      "Average validation loss after epoch  5 : 4.786\n",
      "2019-12-08 09:56:40.148786 | Epoch 6\n",
      "Average training loss at batch  0 : 3.611\n",
      "Average training loss after epoch  6 : 2.070\n",
      "Average validation loss after epoch  6 : 4.781\n",
      "2019-12-08 09:56:41.721257 | Epoch 7\n",
      "Average training loss at batch  0 : 3.852\n",
      "Average training loss after epoch  7 : 2.064\n",
      "Average validation loss after epoch  7 : 4.775\n",
      "2019-12-08 09:56:43.298448 | Epoch 8\n",
      "Average training loss at batch  0 : 3.343\n",
      "Average training loss after epoch  8 : 2.061\n",
      "Average validation loss after epoch  8 : 4.771\n",
      "2019-12-08 09:56:45.014358 | Epoch 9\n",
      "Average training loss at batch  0 : 3.353\n",
      "Average training loss after epoch  9 : 2.063\n",
      "Average validation loss after epoch  9 : 4.764\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e4a0cf04ea41f5a0a1f6096e11397b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 09:56:46.724726 | Epoch 0\n",
      "Average training loss at batch  0 : 3.814\n",
      "Average training loss after epoch  0 : 0.243\n",
      "Average validation loss after epoch  0 : 0.159\n",
      "2019-12-08 09:57:17.509396 | Epoch 1\n",
      "Average training loss at batch  0 : 0.095\n",
      "Average training loss after epoch  1 : 0.041\n",
      "Average validation loss after epoch  1 : 0.063\n",
      "2019-12-08 09:57:53.434213 | Epoch 2\n",
      "Average training loss at batch  0 : 0.035\n",
      "Average training loss after epoch  2 : 0.020\n",
      "Average validation loss after epoch  2 : 0.043\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=3,upweight=25,lambda=1/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131a04dae9c04d6ba0152cc799e6d525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 09:58:33.337224 | Epoch 0\n",
      "Average training loss at batch  0 : 5.307\n",
      "Average training loss after epoch  0 : 2.833\n",
      "Average validation loss after epoch  0 : 4.795\n",
      "2019-12-08 09:58:34.870727 | Epoch 1\n",
      "Average training loss at batch  0 : 4.664\n",
      "Average training loss after epoch  1 : 2.813\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 09:58:36.395805 | Epoch 2\n",
      "Average training loss at batch  0 : 4.986\n",
      "Average training loss after epoch  2 : 2.817\n",
      "Average validation loss after epoch  2 : 4.788\n",
      "2019-12-08 09:58:37.937305 | Epoch 3\n",
      "Average training loss at batch  0 : 5.711\n",
      "Average training loss after epoch  3 : 2.810\n",
      "Average validation loss after epoch  3 : 4.788\n",
      "2019-12-08 09:58:39.483933 | Epoch 4\n",
      "Average training loss at batch  0 : 4.560\n",
      "Average training loss after epoch  4 : 2.824\n",
      "Average validation loss after epoch  4 : 4.785\n",
      "2019-12-08 09:58:41.034509 | Epoch 5\n",
      "Average training loss at batch  0 : 4.503\n",
      "Average training loss after epoch  5 : 2.810\n",
      "Average validation loss after epoch  5 : 4.784\n",
      "2019-12-08 09:58:42.558927 | Epoch 6\n",
      "Average training loss at batch  0 : 4.148\n",
      "Average training loss after epoch  6 : 2.814\n",
      "Average validation loss after epoch  6 : 4.781\n",
      "2019-12-08 09:58:44.084365 | Epoch 7\n",
      "Average training loss at batch  0 : 4.913\n",
      "Average training loss after epoch  7 : 2.813\n",
      "Average validation loss after epoch  7 : 4.775\n",
      "2019-12-08 09:58:45.617520 | Epoch 8\n",
      "Average training loss at batch  0 : 4.876\n",
      "Average training loss after epoch  8 : 2.807\n",
      "Average validation loss after epoch  8 : 4.768\n",
      "2019-12-08 09:58:47.145997 | Epoch 9\n",
      "Average training loss at batch  0 : 4.781\n",
      "Average training loss after epoch  9 : 2.812\n",
      "Average validation loss after epoch  9 : 4.763\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2953f24312194e399539d158005312b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 09:58:48.709658 | Epoch 0\n",
      "Average training loss at batch  0 : 4.363\n",
      "Average training loss after epoch  0 : 0.324\n",
      "Average validation loss after epoch  0 : 0.142\n",
      "2019-12-08 09:59:18.064885 | Epoch 1\n",
      "Average training loss at batch  0 : 0.103\n",
      "Average training loss after epoch  1 : 0.051\n",
      "Average validation loss after epoch  1 : 0.064\n",
      "2019-12-08 09:59:53.575933 | Epoch 2\n",
      "Average training loss at batch  0 : 0.035\n",
      "Average training loss after epoch  2 : 0.024\n",
      "Average validation loss after epoch  2 : 0.044\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=3,upweight=25,lambda=5/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3f6c4c4b03448986544e6908b68257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:00:33.392883 | Epoch 0\n",
      "Average training loss at batch  0 : 13.711\n",
      "Average training loss after epoch  0 : 8.767\n",
      "Average validation loss after epoch  0 : 4.796\n",
      "2019-12-08 10:00:34.933413 | Epoch 1\n",
      "Average training loss at batch  0 : 13.521\n",
      "Average training loss after epoch  1 : 8.789\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 10:00:36.451791 | Epoch 2\n",
      "Average training loss at batch  0 : 12.813\n",
      "Average training loss after epoch  2 : 8.798\n",
      "Average validation loss after epoch  2 : 4.785\n",
      "2019-12-08 10:00:37.979526 | Epoch 3\n",
      "Average training loss at batch  0 : 14.676\n",
      "Average training loss after epoch  3 : 8.792\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 10:00:39.524722 | Epoch 4\n",
      "Average training loss at batch  0 : 15.406\n",
      "Average training loss after epoch  4 : 8.795\n",
      "Average validation loss after epoch  4 : 4.787\n",
      "2019-12-08 10:00:41.035930 | Epoch 5\n",
      "Average training loss at batch  0 : 15.635\n",
      "Average training loss after epoch  5 : 8.783\n",
      "Average validation loss after epoch  5 : 4.784\n",
      "2019-12-08 10:00:42.547782 | Epoch 6\n",
      "Average training loss at batch  0 : 14.853\n",
      "Average training loss after epoch  6 : 8.792\n",
      "Average validation loss after epoch  6 : 4.781\n",
      "2019-12-08 10:00:44.072318 | Epoch 7\n",
      "Average training loss at batch  0 : 17.013\n",
      "Average training loss after epoch  7 : 8.806\n",
      "Average validation loss after epoch  7 : 4.774\n",
      "2019-12-08 10:00:45.600477 | Epoch 8\n",
      "Average training loss at batch  0 : 17.106\n",
      "Average training loss after epoch  8 : 8.799\n",
      "Average validation loss after epoch  8 : 4.769\n",
      "2019-12-08 10:00:47.128275 | Epoch 9\n",
      "Average training loss at batch  0 : 12.220\n",
      "Average training loss after epoch  9 : 8.804\n",
      "Average validation loss after epoch  9 : 4.761\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d02fa42ff2548b6a950d531543aa7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:00:48.672209 | Epoch 0\n",
      "Average training loss at batch  0 : 15.760\n",
      "Average training loss after epoch  0 : 1.067\n",
      "Average validation loss after epoch  0 : 0.185\n",
      "2019-12-08 10:01:17.810670 | Epoch 1\n",
      "Average training loss at batch  0 : 0.665\n",
      "Average training loss after epoch  1 : 0.154\n",
      "Average validation loss after epoch  1 : 0.076\n",
      "2019-12-08 10:01:52.800075 | Epoch 2\n",
      "Average training loss at batch  0 : 0.135\n",
      "Average training loss after epoch  2 : 0.063\n",
      "Average validation loss after epoch  2 : 0.050\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=3,upweight=25,lambda=10/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39490694b6cb4a509abd2731db986b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:02:32.250276 | Epoch 0\n",
      "Average training loss at batch  0 : 25.673\n",
      "Average training loss after epoch  0 : 16.210\n",
      "Average validation loss after epoch  0 : 4.795\n",
      "2019-12-08 10:02:33.790817 | Epoch 1\n",
      "Average training loss at batch  0 : 28.167\n",
      "Average training loss after epoch  1 : 16.224\n",
      "Average validation loss after epoch  1 : 4.787\n",
      "2019-12-08 10:02:35.333936 | Epoch 2\n",
      "Average training loss at batch  0 : 27.584\n",
      "Average training loss after epoch  2 : 16.280\n",
      "Average validation loss after epoch  2 : 4.786\n",
      "2019-12-08 10:02:36.856992 | Epoch 3\n",
      "Average training loss at batch  0 : 23.191\n",
      "Average training loss after epoch  3 : 16.272\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 10:02:38.396272 | Epoch 4\n",
      "Average training loss at batch  0 : 26.320\n",
      "Average training loss after epoch  4 : 16.247\n",
      "Average validation loss after epoch  4 : 4.786\n",
      "2019-12-08 10:02:39.917622 | Epoch 5\n",
      "Average training loss at batch  0 : 24.589\n",
      "Average training loss after epoch  5 : 16.263\n",
      "Average validation loss after epoch  5 : 4.787\n",
      "2019-12-08 10:02:41.424159 | Epoch 6\n",
      "Average training loss at batch  0 : 26.159\n",
      "Average training loss after epoch  6 : 16.290\n",
      "Average validation loss after epoch  6 : 4.783\n",
      "2019-12-08 10:02:42.943769 | Epoch 7\n",
      "Average training loss at batch  0 : 24.117\n",
      "Average training loss after epoch  7 : 16.273\n",
      "Average validation loss after epoch  7 : 4.778\n",
      "2019-12-08 10:02:44.467945 | Epoch 8\n",
      "Average training loss at batch  0 : 23.436\n",
      "Average training loss after epoch  8 : 16.277\n",
      "Average validation loss after epoch  8 : 4.774\n",
      "2019-12-08 10:02:45.993681 | Epoch 9\n",
      "Average training loss at batch  0 : 28.199\n",
      "Average training loss after epoch  9 : 16.288\n",
      "Average validation loss after epoch  9 : 4.768\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c38a84370d446d902077dd5ce070d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:02:47.540969 | Epoch 0\n",
      "Average training loss at batch  0 : 30.094\n",
      "Average training loss after epoch  0 : 2.048\n",
      "Average validation loss after epoch  0 : 0.184\n",
      "2019-12-08 10:03:17.017159 | Epoch 1\n",
      "Average training loss at batch  0 : 0.669\n",
      "Average training loss after epoch  1 : 0.294\n",
      "Average validation loss after epoch  1 : 0.089\n",
      "2019-12-08 10:03:51.810081 | Epoch 2\n",
      "Average training loss at batch  0 : 0.284\n",
      "Average training loss after epoch  2 : 0.118\n",
      "Average validation loss after epoch  2 : 0.057\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=3,upweight=25,lambda=25/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5301a4534a2483d8e42d33fa05697af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:04:31.419346 | Epoch 0\n",
      "Average training loss at batch  0 : 56.553\n",
      "Average training loss after epoch  0 : 38.518\n",
      "Average validation loss after epoch  0 : 4.796\n",
      "2019-12-08 10:04:32.906709 | Epoch 1\n",
      "Average training loss at batch  0 : 59.252\n",
      "Average training loss after epoch  1 : 38.607\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 10:04:34.439128 | Epoch 2\n",
      "Average training loss at batch  0 : 69.546\n",
      "Average training loss after epoch  2 : 38.635\n",
      "Average validation loss after epoch  2 : 4.785\n",
      "2019-12-08 10:04:35.966086 | Epoch 3\n",
      "Average training loss at batch  0 : 63.226\n",
      "Average training loss after epoch  3 : 38.766\n",
      "Average validation loss after epoch  3 : 4.788\n",
      "2019-12-08 10:04:37.481857 | Epoch 4\n",
      "Average training loss at batch  0 : 53.384\n",
      "Average training loss after epoch  4 : 38.669\n",
      "Average validation loss after epoch  4 : 4.786\n",
      "2019-12-08 10:04:38.985553 | Epoch 5\n",
      "Average training loss at batch  0 : 65.384\n",
      "Average training loss after epoch  5 : 38.631\n",
      "Average validation loss after epoch  5 : 4.785\n",
      "2019-12-08 10:04:40.511692 | Epoch 6\n",
      "Average training loss at batch  0 : 61.125\n",
      "Average training loss after epoch  6 : 38.763\n",
      "Average validation loss after epoch  6 : 4.781\n",
      "2019-12-08 10:04:42.033928 | Epoch 7\n",
      "Average training loss at batch  0 : 69.364\n",
      "Average training loss after epoch  7 : 38.708\n",
      "Average validation loss after epoch  7 : 4.776\n",
      "2019-12-08 10:04:43.550742 | Epoch 8\n",
      "Average training loss at batch  0 : 71.272\n",
      "Average training loss after epoch  8 : 38.739\n",
      "Average validation loss after epoch  8 : 4.772\n",
      "2019-12-08 10:04:45.091086 | Epoch 9\n",
      "Average training loss at batch  0 : 72.084\n",
      "Average training loss after epoch  9 : 38.735\n",
      "Average validation loss after epoch  9 : 4.765\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce03faf3325d41a9a0da8153142eff3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:04:46.627355 | Epoch 0\n",
      "Average training loss at batch  0 : 66.587\n",
      "Average training loss after epoch  0 : 4.903\n",
      "Average validation loss after epoch  0 : 0.203\n",
      "2019-12-08 10:05:15.779559 | Epoch 1\n",
      "Average training loss at batch  0 : 2.059\n",
      "Average training loss after epoch  1 : 0.696\n",
      "Average validation loss after epoch  1 : 0.089\n",
      "2019-12-08 10:05:50.621761 | Epoch 2\n",
      "Average training loss at batch  0 : 0.612\n",
      "Average training loss after epoch  2 : 0.277\n",
      "Average validation loss after epoch  2 : 0.061\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=4,upweight=25,lambda=0.1/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433121dcd06e4a2cb32162d2bbef9878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:06:30.263871 | Epoch 0\n",
      "Average training loss at batch  0 : 2.794\n",
      "Average training loss after epoch  0 : 1.502\n",
      "Average validation loss after epoch  0 : 4.793\n",
      "2019-12-08 10:06:31.793211 | Epoch 1\n",
      "Average training loss at batch  0 : 2.559\n",
      "Average training loss after epoch  1 : 1.466\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 10:06:33.319525 | Epoch 2\n",
      "Average training loss at batch  0 : 2.625\n",
      "Average training loss after epoch  2 : 1.470\n",
      "Average validation loss after epoch  2 : 4.787\n",
      "2019-12-08 10:06:34.976483 | Epoch 3\n",
      "Average training loss at batch  0 : 2.942\n",
      "Average training loss after epoch  3 : 1.470\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 10:06:36.475062 | Epoch 4\n",
      "Average training loss at batch  0 : 2.895\n",
      "Average training loss after epoch  4 : 1.472\n",
      "Average validation loss after epoch  4 : 4.787\n",
      "2019-12-08 10:06:38.003850 | Epoch 5\n",
      "Average training loss at batch  0 : 2.391\n",
      "Average training loss after epoch  5 : 1.469\n",
      "Average validation loss after epoch  5 : 4.787\n",
      "2019-12-08 10:06:39.564020 | Epoch 6\n",
      "Average training loss at batch  0 : 2.886\n",
      "Average training loss after epoch  6 : 1.461\n",
      "Average validation loss after epoch  6 : 4.784\n",
      "2019-12-08 10:06:41.035139 | Epoch 7\n",
      "Average training loss at batch  0 : 2.823\n",
      "Average training loss after epoch  7 : 1.472\n",
      "Average validation loss after epoch  7 : 4.779\n",
      "2019-12-08 10:06:42.559781 | Epoch 8\n",
      "Average training loss at batch  0 : 2.672\n",
      "Average training loss after epoch  8 : 1.465\n",
      "Average validation loss after epoch  8 : 4.774\n",
      "2019-12-08 10:06:44.149460 | Epoch 9\n",
      "Average training loss at batch  0 : 2.461\n",
      "Average training loss after epoch  9 : 1.458\n",
      "Average validation loss after epoch  9 : 4.768\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad467adeffc44477933c13d40882912f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:06:45.721113 | Epoch 0\n",
      "Average training loss at batch  0 : 2.553\n",
      "Average training loss after epoch  0 : 0.193\n",
      "Average validation loss after epoch  0 : 0.192\n",
      "2019-12-08 10:07:16.269739 | Epoch 1\n",
      "Average training loss at batch  0 : 0.102\n",
      "Average training loss after epoch  1 : 0.039\n",
      "Average validation loss after epoch  1 : 0.076\n",
      "2019-12-08 10:07:52.753262 | Epoch 2\n",
      "Average training loss at batch  0 : 0.022\n",
      "Average training loss after epoch  2 : 0.019\n",
      "Average validation loss after epoch  2 : 0.049\n",
      "2019-12-08 10:08:34.004238 | Epoch 3\n",
      "Average training loss at batch  0 : 0.024\n",
      "Average training loss after epoch  3 : 0.013\n",
      "Average validation loss after epoch  3 : 0.040\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=4,upweight=25,lambda=0.5/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335ecd0566ba46d18080df9c255c07ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:09:15.182646 | Epoch 0\n",
      "Average training loss at batch  0 : 2.829\n",
      "Average training loss after epoch  0 : 2.084\n",
      "Average validation loss after epoch  0 : 4.797\n",
      "2019-12-08 10:09:16.659660 | Epoch 1\n",
      "Average training loss at batch  0 : 3.533\n",
      "Average training loss after epoch  1 : 2.069\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 10:09:18.199562 | Epoch 2\n",
      "Average training loss at batch  0 : 3.299\n",
      "Average training loss after epoch  2 : 2.073\n",
      "Average validation loss after epoch  2 : 4.785\n",
      "2019-12-08 10:09:19.769308 | Epoch 3\n",
      "Average training loss at batch  0 : 3.454\n",
      "Average training loss after epoch  3 : 2.070\n",
      "Average validation loss after epoch  3 : 4.786\n",
      "2019-12-08 10:09:21.281965 | Epoch 4\n",
      "Average training loss at batch  0 : 3.380\n",
      "Average training loss after epoch  4 : 2.070\n",
      "Average validation loss after epoch  4 : 4.787\n",
      "2019-12-08 10:09:22.856618 | Epoch 5\n",
      "Average training loss at batch  0 : 3.661\n",
      "Average training loss after epoch  5 : 2.062\n",
      "Average validation loss after epoch  5 : 4.787\n",
      "2019-12-08 10:09:24.381631 | Epoch 6\n",
      "Average training loss at batch  0 : 3.618\n",
      "Average training loss after epoch  6 : 2.068\n",
      "Average validation loss after epoch  6 : 4.782\n",
      "2019-12-08 10:09:25.926103 | Epoch 7\n",
      "Average training loss at batch  0 : 3.274\n",
      "Average training loss after epoch  7 : 2.070\n",
      "Average validation loss after epoch  7 : 4.775\n",
      "2019-12-08 10:09:27.461228 | Epoch 8\n",
      "Average training loss at batch  0 : 4.098\n",
      "Average training loss after epoch  8 : 2.060\n",
      "Average validation loss after epoch  8 : 4.772\n",
      "2019-12-08 10:09:28.993822 | Epoch 9\n",
      "Average training loss at batch  0 : 2.994\n",
      "Average training loss after epoch  9 : 2.052\n",
      "Average validation loss after epoch  9 : 4.766\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabd5528bdaa4f53b1bcc3b7dcd9e61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:09:30.553401 | Epoch 0\n",
      "Average training loss at batch  0 : 3.274\n",
      "Average training loss after epoch  0 : 0.243\n",
      "Average validation loss after epoch  0 : 0.147\n",
      "2019-12-08 10:09:59.682361 | Epoch 1\n",
      "Average training loss at batch  0 : 0.090\n",
      "Average training loss after epoch  1 : 0.042\n",
      "Average validation loss after epoch  1 : 0.060\n",
      "2019-12-08 10:10:34.627560 | Epoch 2\n",
      "Average training loss at batch  0 : 0.024\n",
      "Average training loss after epoch  2 : 0.021\n",
      "Average validation loss after epoch  2 : 0.040\n",
      "2019-12-08 10:11:14.121492 | Epoch 3\n",
      "Average training loss at batch  0 : 0.059\n",
      "Average training loss after epoch  3 : 0.013\n",
      "Average validation loss after epoch  3 : 0.037\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=4,upweight=25,lambda=1/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123ed01fd0c64140b688774f00c3cf11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:11:55.193896 | Epoch 0\n",
      "Average training loss at batch  0 : 4.817\n",
      "Average training loss after epoch  0 : 2.840\n",
      "Average validation loss after epoch  0 : 4.794\n",
      "2019-12-08 10:11:56.726062 | Epoch 1\n",
      "Average training loss at batch  0 : 4.875\n",
      "Average training loss after epoch  1 : 2.813\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 10:11:58.259165 | Epoch 2\n",
      "Average training loss at batch  0 : 4.521\n",
      "Average training loss after epoch  2 : 2.811\n",
      "Average validation loss after epoch  2 : 4.786\n",
      "2019-12-08 10:11:59.799012 | Epoch 3\n",
      "Average training loss at batch  0 : 4.646\n",
      "Average training loss after epoch  3 : 2.811\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 10:12:01.340971 | Epoch 4\n",
      "Average training loss at batch  0 : 4.799\n",
      "Average training loss after epoch  4 : 2.817\n",
      "Average validation loss after epoch  4 : 4.788\n",
      "2019-12-08 10:12:02.875857 | Epoch 5\n",
      "Average training loss at batch  0 : 4.416\n",
      "Average training loss after epoch  5 : 2.814\n",
      "Average validation loss after epoch  5 : 4.785\n",
      "2019-12-08 10:12:04.414615 | Epoch 6\n",
      "Average training loss at batch  0 : 5.224\n",
      "Average training loss after epoch  6 : 2.814\n",
      "Average validation loss after epoch  6 : 4.781\n",
      "2019-12-08 10:12:05.967008 | Epoch 7\n",
      "Average training loss at batch  0 : 4.249\n",
      "Average training loss after epoch  7 : 2.810\n",
      "Average validation loss after epoch  7 : 4.777\n",
      "2019-12-08 10:12:07.513650 | Epoch 8\n",
      "Average training loss at batch  0 : 4.259\n",
      "Average training loss after epoch  8 : 2.809\n",
      "Average validation loss after epoch  8 : 4.771\n",
      "2019-12-08 10:12:09.065736 | Epoch 9\n",
      "Average training loss at batch  0 : 4.625\n",
      "Average training loss after epoch  9 : 2.810\n",
      "Average validation loss after epoch  9 : 4.765\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f596aac14f2a4f51b7e083cbeec01dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:12:10.569106 | Epoch 0\n",
      "Average training loss at batch  0 : 4.715\n",
      "Average training loss after epoch  0 : 0.324\n",
      "Average validation loss after epoch  0 : 0.145\n",
      "2019-12-08 10:12:39.795682 | Epoch 1\n",
      "Average training loss at batch  0 : 0.133\n",
      "Average training loss after epoch  1 : 0.051\n",
      "Average validation loss after epoch  1 : 0.061\n",
      "2019-12-08 10:13:14.750459 | Epoch 2\n",
      "Average training loss at batch  0 : 0.031\n",
      "Average training loss after epoch  2 : 0.024\n",
      "Average validation loss after epoch  2 : 0.043\n",
      "2019-12-08 10:13:54.217339 | Epoch 3\n",
      "Average training loss at batch  0 : 0.025\n",
      "Average training loss after epoch  3 : 0.015\n",
      "Average validation loss after epoch  3 : 0.035\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=4,upweight=25,lambda=5/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e04a6a228d41b9a783dc8277b93006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:14:35.396395 | Epoch 0\n",
      "Average training loss at batch  0 : 15.538\n",
      "Average training loss after epoch  0 : 8.787\n",
      "Average validation loss after epoch  0 : 4.794\n",
      "2019-12-08 10:14:36.925549 | Epoch 1\n",
      "Average training loss at batch  0 : 15.994\n",
      "Average training loss after epoch  1 : 8.778\n",
      "Average validation loss after epoch  1 : 4.787\n",
      "2019-12-08 10:14:38.466311 | Epoch 2\n",
      "Average training loss at batch  0 : 18.846\n",
      "Average training loss after epoch  2 : 8.784\n",
      "Average validation loss after epoch  2 : 4.787\n",
      "2019-12-08 10:14:39.997571 | Epoch 3\n",
      "Average training loss at batch  0 : 15.635\n",
      "Average training loss after epoch  3 : 8.803\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 10:14:41.504377 | Epoch 4\n",
      "Average training loss at batch  0 : 14.339\n",
      "Average training loss after epoch  4 : 8.784\n",
      "Average validation loss after epoch  4 : 4.787\n",
      "2019-12-08 10:14:43.041900 | Epoch 5\n",
      "Average training loss at batch  0 : 12.485\n",
      "Average training loss after epoch  5 : 8.797\n",
      "Average validation loss after epoch  5 : 4.785\n",
      "2019-12-08 10:14:44.575846 | Epoch 6\n",
      "Average training loss at batch  0 : 13.230\n",
      "Average training loss after epoch  6 : 8.790\n",
      "Average validation loss after epoch  6 : 4.782\n",
      "2019-12-08 10:14:46.111089 | Epoch 7\n",
      "Average training loss at batch  0 : 14.277\n",
      "Average training loss after epoch  7 : 8.798\n",
      "Average validation loss after epoch  7 : 4.775\n",
      "2019-12-08 10:14:47.661411 | Epoch 8\n",
      "Average training loss at batch  0 : 13.254\n",
      "Average training loss after epoch  8 : 8.799\n",
      "Average validation loss after epoch  8 : 4.771\n",
      "2019-12-08 10:14:49.184714 | Epoch 9\n",
      "Average training loss at batch  0 : 16.453\n",
      "Average training loss after epoch  9 : 8.799\n",
      "Average validation loss after epoch  9 : 4.764\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f16df49e2047f19f0d042949de1bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:14:50.736591 | Epoch 0\n",
      "Average training loss at batch  0 : 14.995\n",
      "Average training loss after epoch  0 : 1.074\n",
      "Average validation loss after epoch  0 : 0.175\n",
      "2019-12-08 10:15:19.852068 | Epoch 1\n",
      "Average training loss at batch  0 : 0.380\n",
      "Average training loss after epoch  1 : 0.154\n",
      "Average validation loss after epoch  1 : 0.075\n",
      "2019-12-08 10:15:54.791592 | Epoch 2\n",
      "Average training loss at batch  0 : 0.262\n",
      "Average training loss after epoch  2 : 0.066\n",
      "Average validation loss after epoch  2 : 0.052\n",
      "2019-12-08 10:16:34.474221 | Epoch 3\n",
      "Average training loss at batch  0 : 0.066\n",
      "Average training loss after epoch  3 : 0.038\n",
      "Average validation loss after epoch  3 : 0.043\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=4,upweight=25,lambda=10/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2a1b208dc94572b35f7de1cc85d50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:17:16.887782 | Epoch 0\n",
      "Average training loss at batch  0 : 27.620\n",
      "Average training loss after epoch  0 : 16.209\n",
      "Average validation loss after epoch  0 : 4.795\n",
      "2019-12-08 10:17:18.393388 | Epoch 1\n",
      "Average training loss at batch  0 : 29.636\n",
      "Average training loss after epoch  1 : 16.256\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 10:17:19.956523 | Epoch 2\n",
      "Average training loss at batch  0 : 29.715\n",
      "Average training loss after epoch  2 : 16.248\n",
      "Average validation loss after epoch  2 : 4.786\n",
      "2019-12-08 10:17:21.488195 | Epoch 3\n",
      "Average training loss at batch  0 : 25.621\n",
      "Average training loss after epoch  3 : 16.256\n",
      "Average validation loss after epoch  3 : 4.786\n",
      "2019-12-08 10:17:23.038492 | Epoch 4\n",
      "Average training loss at batch  0 : 23.760\n",
      "Average training loss after epoch  4 : 16.273\n",
      "Average validation loss after epoch  4 : 4.786\n",
      "2019-12-08 10:17:24.768700 | Epoch 5\n",
      "Average training loss at batch  0 : 26.794\n",
      "Average training loss after epoch  5 : 16.265\n",
      "Average validation loss after epoch  5 : 4.787\n",
      "2019-12-08 10:17:26.472989 | Epoch 6\n",
      "Average training loss at batch  0 : 26.920\n",
      "Average training loss after epoch  6 : 16.273\n",
      "Average validation loss after epoch  6 : 4.784\n",
      "2019-12-08 10:17:28.049880 | Epoch 7\n",
      "Average training loss at batch  0 : 23.939\n",
      "Average training loss after epoch  7 : 16.258\n",
      "Average validation loss after epoch  7 : 4.782\n",
      "2019-12-08 10:17:29.712986 | Epoch 8\n",
      "Average training loss at batch  0 : 28.973\n",
      "Average training loss after epoch  8 : 16.287\n",
      "Average validation loss after epoch  8 : 4.775\n",
      "2019-12-08 10:17:31.324455 | Epoch 9\n",
      "Average training loss at batch  0 : 30.319\n",
      "Average training loss after epoch  9 : 16.291\n",
      "Average validation loss after epoch  9 : 4.770\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770e9d99bf5e426fb66350c5fb32cb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:17:32.882915 | Epoch 0\n",
      "Average training loss at batch  0 : 29.029\n",
      "Average training loss after epoch  0 : 2.037\n",
      "Average validation loss after epoch  0 : 0.185\n",
      "2019-12-08 10:18:03.059390 | Epoch 1\n",
      "Average training loss at batch  0 : 1.275\n",
      "Average training loss after epoch  1 : 0.288\n",
      "Average validation loss after epoch  1 : 0.087\n",
      "2019-12-08 10:18:39.697735 | Epoch 2\n",
      "Average training loss at batch  0 : 0.418\n",
      "Average training loss after epoch  2 : 0.118\n",
      "Average validation loss after epoch  2 : 0.054\n",
      "2019-12-08 10:19:19.170984 | Epoch 3\n",
      "Average training loss at batch  0 : 0.188\n",
      "Average training loss after epoch  3 : 0.066\n",
      "Average validation loss after epoch  3 : 0.046\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=4,upweight=25,lambda=25/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edef036bdeef4e91908a5a15ae9753d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:20:02.791349 | Epoch 0\n",
      "Average training loss at batch  0 : 65.218\n",
      "Average training loss after epoch  0 : 38.517\n",
      "Average validation loss after epoch  0 : 4.795\n",
      "2019-12-08 10:20:04.439258 | Epoch 1\n",
      "Average training loss at batch  0 : 60.923\n",
      "Average training loss after epoch  1 : 38.628\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 10:20:06.027036 | Epoch 2\n",
      "Average training loss at batch  0 : 64.917\n",
      "Average training loss after epoch  2 : 38.625\n",
      "Average validation loss after epoch  2 : 4.787\n",
      "2019-12-08 10:20:07.605923 | Epoch 3\n",
      "Average training loss at batch  0 : 59.239\n",
      "Average training loss after epoch  3 : 38.702\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 10:20:09.391518 | Epoch 4\n",
      "Average training loss at batch  0 : 67.070\n",
      "Average training loss after epoch  4 : 38.648\n",
      "Average validation loss after epoch  4 : 4.786\n",
      "2019-12-08 10:20:10.990981 | Epoch 5\n",
      "Average training loss at batch  0 : 65.511\n",
      "Average training loss after epoch  5 : 38.738\n",
      "Average validation loss after epoch  5 : 4.782\n",
      "2019-12-08 10:20:12.552470 | Epoch 6\n",
      "Average training loss at batch  0 : 58.161\n",
      "Average training loss after epoch  6 : 38.705\n",
      "Average validation loss after epoch  6 : 4.778\n",
      "2019-12-08 10:20:14.075256 | Epoch 7\n",
      "Average training loss at batch  0 : 76.814\n",
      "Average training loss after epoch  7 : 38.757\n",
      "Average validation loss after epoch  7 : 4.772\n",
      "2019-12-08 10:20:15.611935 | Epoch 8\n",
      "Average training loss at batch  0 : 66.268\n",
      "Average training loss after epoch  8 : 38.719\n",
      "Average validation loss after epoch  8 : 4.767\n",
      "2019-12-08 10:20:17.137561 | Epoch 9\n",
      "Average training loss at batch  0 : 64.191\n",
      "Average training loss after epoch  9 : 38.817\n",
      "Average validation loss after epoch  9 : 4.757\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ec3c110b5d4a94990f8c632dc6a008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:20:18.678095 | Epoch 0\n",
      "Average training loss at batch  0 : 63.001\n",
      "Average training loss after epoch  0 : 4.993\n",
      "Average validation loss after epoch  0 : 0.202\n",
      "2019-12-08 10:20:48.847654 | Epoch 1\n",
      "Average training loss at batch  0 : 1.662\n",
      "Average training loss after epoch  1 : 0.703\n",
      "Average validation loss after epoch  1 : 0.091\n",
      "2019-12-08 10:21:26.908704 | Epoch 2\n",
      "Average training loss at batch  0 : 0.465\n",
      "Average training loss after epoch  2 : 0.284\n",
      "Average validation loss after epoch  2 : 0.065\n",
      "2019-12-08 10:22:06.730087 | Epoch 3\n",
      "Average training loss at batch  0 : 0.250\n",
      "Average training loss after epoch  3 : 0.160\n",
      "Average validation loss after epoch  3 : 0.055\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=5,upweight=25,lambda=0.1/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3bec0d8d0b44c48272f19aa303c108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:22:47.896397 | Epoch 0\n",
      "Average training loss at batch  0 : 2.712\n",
      "Average training loss after epoch  0 : 1.499\n",
      "Average validation loss after epoch  0 : 4.794\n",
      "2019-12-08 10:22:49.396815 | Epoch 1\n",
      "Average training loss at batch  0 : 2.124\n",
      "Average training loss after epoch  1 : 1.468\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 10:22:50.933850 | Epoch 2\n",
      "Average training loss at batch  0 : 2.198\n",
      "Average training loss after epoch  2 : 1.471\n",
      "Average validation loss after epoch  2 : 4.786\n",
      "2019-12-08 10:22:52.478817 | Epoch 3\n",
      "Average training loss at batch  0 : 2.449\n",
      "Average training loss after epoch  3 : 1.472\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 10:22:54.002732 | Epoch 4\n",
      "Average training loss at batch  0 : 2.299\n",
      "Average training loss after epoch  4 : 1.472\n",
      "Average validation loss after epoch  4 : 4.787\n",
      "2019-12-08 10:22:55.537108 | Epoch 5\n",
      "Average training loss at batch  0 : 2.715\n",
      "Average training loss after epoch  5 : 1.465\n",
      "Average validation loss after epoch  5 : 4.785\n",
      "2019-12-08 10:22:57.077849 | Epoch 6\n",
      "Average training loss at batch  0 : 2.484\n",
      "Average training loss after epoch  6 : 1.467\n",
      "Average validation loss after epoch  6 : 4.781\n",
      "2019-12-08 10:22:58.600057 | Epoch 7\n",
      "Average training loss at batch  0 : 2.611\n",
      "Average training loss after epoch  7 : 1.463\n",
      "Average validation loss after epoch  7 : 4.776\n",
      "2019-12-08 10:23:00.138651 | Epoch 8\n",
      "Average training loss at batch  0 : 2.554\n",
      "Average training loss after epoch  8 : 1.468\n",
      "Average validation loss after epoch  8 : 4.770\n",
      "2019-12-08 10:23:01.660034 | Epoch 9\n",
      "Average training loss at batch  0 : 2.736\n",
      "Average training loss after epoch  9 : 1.454\n",
      "Average validation loss after epoch  9 : 4.764\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ed5e8b943747048abe3012024607df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:23:03.225940 | Epoch 0\n",
      "Average training loss at batch  0 : 3.167\n",
      "Average training loss after epoch  0 : 0.194\n",
      "Average validation loss after epoch  0 : 0.180\n",
      "2019-12-08 10:23:32.410729 | Epoch 1\n",
      "Average training loss at batch  0 : 0.208\n",
      "Average training loss after epoch  1 : 0.040\n",
      "Average validation loss after epoch  1 : 0.076\n",
      "2019-12-08 10:24:07.359579 | Epoch 2\n",
      "Average training loss at batch  0 : 0.037\n",
      "Average training loss after epoch  2 : 0.020\n",
      "Average validation loss after epoch  2 : 0.047\n",
      "2019-12-08 10:24:46.814263 | Epoch 3\n",
      "Average training loss at batch  0 : 0.018\n",
      "Average training loss after epoch  3 : 0.012\n",
      "Average validation loss after epoch  3 : 0.036\n",
      "2019-12-08 10:25:27.698297 | Epoch 4\n",
      "Average training loss at batch  0 : 0.012\n",
      "Average training loss after epoch  4 : 0.009\n",
      "Average validation loss after epoch  4 : 0.032\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=5,upweight=25,lambda=0.5/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9700603d72b44b89c379e2f883c6a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:26:09.177248 | Epoch 0\n",
      "Average training loss at batch  0 : 3.546\n",
      "Average training loss after epoch  0 : 2.096\n",
      "Average validation loss after epoch  0 : 4.794\n",
      "2019-12-08 10:26:10.705184 | Epoch 1\n",
      "Average training loss at batch  0 : 2.811\n",
      "Average training loss after epoch  1 : 2.067\n",
      "Average validation loss after epoch  1 : 4.786\n",
      "2019-12-08 10:26:12.232976 | Epoch 2\n",
      "Average training loss at batch  0 : 3.632\n",
      "Average training loss after epoch  2 : 2.064\n",
      "Average validation loss after epoch  2 : 4.787\n",
      "2019-12-08 10:26:13.750198 | Epoch 3\n",
      "Average training loss at batch  0 : 3.420\n",
      "Average training loss after epoch  3 : 2.065\n",
      "Average validation loss after epoch  3 : 4.786\n",
      "2019-12-08 10:26:15.282436 | Epoch 4\n",
      "Average training loss at batch  0 : 3.499\n",
      "Average training loss after epoch  4 : 2.072\n",
      "Average validation loss after epoch  4 : 4.786\n",
      "2019-12-08 10:26:16.817377 | Epoch 5\n",
      "Average training loss at batch  0 : 3.766\n",
      "Average training loss after epoch  5 : 2.067\n",
      "Average validation loss after epoch  5 : 4.787\n",
      "2019-12-08 10:26:18.353318 | Epoch 6\n",
      "Average training loss at batch  0 : 3.339\n",
      "Average training loss after epoch  6 : 2.064\n",
      "Average validation loss after epoch  6 : 4.784\n",
      "2019-12-08 10:26:19.879744 | Epoch 7\n",
      "Average training loss at batch  0 : 3.382\n",
      "Average training loss after epoch  7 : 2.060\n",
      "Average validation loss after epoch  7 : 4.781\n",
      "2019-12-08 10:26:21.393688 | Epoch 8\n",
      "Average training loss at batch  0 : 3.376\n",
      "Average training loss after epoch  8 : 2.067\n",
      "Average validation loss after epoch  8 : 4.774\n",
      "2019-12-08 10:26:22.925847 | Epoch 9\n",
      "Average training loss at batch  0 : 3.418\n",
      "Average training loss after epoch  9 : 2.063\n",
      "Average validation loss after epoch  9 : 4.769\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e453b518e0f544a3a4831473ee262138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:26:24.468847 | Epoch 0\n",
      "Average training loss at batch  0 : 3.016\n",
      "Average training loss after epoch  0 : 0.241\n",
      "Average validation loss after epoch  0 : 0.151\n",
      "2019-12-08 10:26:53.628002 | Epoch 1\n",
      "Average training loss at batch  0 : 0.102\n",
      "Average training loss after epoch  1 : 0.042\n",
      "Average validation loss after epoch  1 : 0.064\n",
      "2019-12-08 10:27:28.410785 | Epoch 2\n",
      "Average training loss at batch  0 : 0.031\n",
      "Average training loss after epoch  2 : 0.021\n",
      "Average validation loss after epoch  2 : 0.046\n",
      "2019-12-08 10:28:08.827455 | Epoch 3\n",
      "Average training loss at batch  0 : 0.036\n",
      "Average training loss after epoch  3 : 0.013\n",
      "Average validation loss after epoch  3 : 0.044\n",
      "2019-12-08 10:28:50.926381 | Epoch 4\n",
      "Average training loss at batch  0 : 0.010\n",
      "Average training loss after epoch  4 : 0.010\n",
      "Average validation loss after epoch  4 : 0.041\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=5,upweight=25,lambda=1/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c93a798d0774ea2a035d9d3d39a7786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:29:33.685391 | Epoch 0\n",
      "Average training loss at batch  0 : 4.988\n",
      "Average training loss after epoch  0 : 2.833\n",
      "Average validation loss after epoch  0 : 4.795\n",
      "2019-12-08 10:29:35.256649 | Epoch 1\n",
      "Average training loss at batch  0 : 4.920\n",
      "Average training loss after epoch  1 : 2.814\n",
      "Average validation loss after epoch  1 : 4.788\n",
      "2019-12-08 10:29:36.813830 | Epoch 2\n",
      "Average training loss at batch  0 : 5.043\n",
      "Average training loss after epoch  2 : 2.812\n",
      "Average validation loss after epoch  2 : 4.787\n",
      "2019-12-08 10:29:38.386591 | Epoch 3\n",
      "Average training loss at batch  0 : 4.649\n",
      "Average training loss after epoch  3 : 2.811\n",
      "Average validation loss after epoch  3 : 4.788\n",
      "2019-12-08 10:29:39.955063 | Epoch 4\n",
      "Average training loss at batch  0 : 4.357\n",
      "Average training loss after epoch  4 : 2.819\n",
      "Average validation loss after epoch  4 : 4.786\n",
      "2019-12-08 10:29:41.519026 | Epoch 5\n",
      "Average training loss at batch  0 : 5.060\n",
      "Average training loss after epoch  5 : 2.815\n",
      "Average validation loss after epoch  5 : 4.784\n",
      "2019-12-08 10:29:43.065097 | Epoch 6\n",
      "Average training loss at batch  0 : 4.791\n",
      "Average training loss after epoch  6 : 2.812\n",
      "Average validation loss after epoch  6 : 4.781\n",
      "2019-12-08 10:29:44.607137 | Epoch 7\n",
      "Average training loss at batch  0 : 4.668\n",
      "Average training loss after epoch  7 : 2.817\n",
      "Average validation loss after epoch  7 : 4.774\n",
      "2019-12-08 10:29:46.170869 | Epoch 8\n",
      "Average training loss at batch  0 : 3.935\n",
      "Average training loss after epoch  8 : 2.804\n",
      "Average validation loss after epoch  8 : 4.770\n",
      "2019-12-08 10:29:47.710046 | Epoch 9\n",
      "Average training loss at batch  0 : 5.413\n",
      "Average training loss after epoch  9 : 2.814\n",
      "Average validation loss after epoch  9 : 4.763\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a7eb3aecfb4ed69fde9d3311bcf656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:29:49.331016 | Epoch 0\n",
      "Average training loss at batch  0 : 4.990\n",
      "Average training loss after epoch  0 : 0.321\n",
      "Average validation loss after epoch  0 : 0.151\n",
      "2019-12-08 10:30:19.971215 | Epoch 1\n",
      "Average training loss at batch  0 : 0.083\n",
      "Average training loss after epoch  1 : 0.051\n",
      "Average validation loss after epoch  1 : 0.058\n",
      "2019-12-08 10:30:56.501841 | Epoch 2\n",
      "Average training loss at batch  0 : 0.046\n",
      "Average training loss after epoch  2 : 0.024\n",
      "Average validation loss after epoch  2 : 0.040\n",
      "2019-12-08 10:31:36.988123 | Epoch 3\n",
      "Average training loss at batch  0 : 0.046\n",
      "Average training loss after epoch  3 : 0.016\n",
      "Average validation loss after epoch  3 : 0.033\n",
      "2019-12-08 10:32:19.483153 | Epoch 4\n",
      "Average training loss at batch  0 : 0.013\n",
      "Average training loss after epoch  4 : 0.011\n",
      "Average validation loss after epoch  4 : 0.030\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=5,upweight=25,lambda=5/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8257add7f459498185ff48b4b06a4d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:33:00.897314 | Epoch 0\n",
      "Average training loss at batch  0 : 14.638\n",
      "Average training loss after epoch  0 : 8.777\n",
      "Average validation loss after epoch  0 : 4.794\n",
      "2019-12-08 10:33:02.425574 | Epoch 1\n",
      "Average training loss at batch  0 : 14.993\n",
      "Average training loss after epoch  1 : 8.791\n",
      "Average validation loss after epoch  1 : 4.787\n",
      "2019-12-08 10:33:03.953743 | Epoch 2\n",
      "Average training loss at batch  0 : 15.606\n",
      "Average training loss after epoch  2 : 8.777\n",
      "Average validation loss after epoch  2 : 4.787\n",
      "2019-12-08 10:33:05.484470 | Epoch 3\n",
      "Average training loss at batch  0 : 12.767\n",
      "Average training loss after epoch  3 : 8.805\n",
      "Average validation loss after epoch  3 : 4.786\n",
      "2019-12-08 10:33:07.034632 | Epoch 4\n",
      "Average training loss at batch  0 : 15.411\n",
      "Average training loss after epoch  4 : 8.782\n",
      "Average validation loss after epoch  4 : 4.788\n",
      "2019-12-08 10:33:08.583376 | Epoch 5\n",
      "Average training loss at batch  0 : 15.838\n",
      "Average training loss after epoch  5 : 8.797\n",
      "Average validation loss after epoch  5 : 4.786\n",
      "2019-12-08 10:33:10.120273 | Epoch 6\n",
      "Average training loss at batch  0 : 11.837\n",
      "Average training loss after epoch  6 : 8.797\n",
      "Average validation loss after epoch  6 : 4.784\n",
      "2019-12-08 10:33:11.671063 | Epoch 7\n",
      "Average training loss at batch  0 : 14.159\n",
      "Average training loss after epoch  7 : 8.796\n",
      "Average validation loss after epoch  7 : 4.778\n",
      "2019-12-08 10:33:13.196718 | Epoch 8\n",
      "Average training loss at batch  0 : 16.832\n",
      "Average training loss after epoch  8 : 8.797\n",
      "Average validation loss after epoch  8 : 4.773\n",
      "2019-12-08 10:33:14.721274 | Epoch 9\n",
      "Average training loss at batch  0 : 16.202\n",
      "Average training loss after epoch  9 : 8.795\n",
      "Average validation loss after epoch  9 : 4.768\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c569b42d2247b88e45c1b64cb65366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:33:16.291645 | Epoch 0\n",
      "Average training loss at batch  0 : 12.767\n",
      "Average training loss after epoch  0 : 1.067\n",
      "Average validation loss after epoch  0 : 0.173\n",
      "2019-12-08 10:33:45.407496 | Epoch 1\n",
      "Average training loss at batch  0 : 0.430\n",
      "Average training loss after epoch  1 : 0.155\n",
      "Average validation loss after epoch  1 : 0.079\n",
      "2019-12-08 10:34:20.225519 | Epoch 2\n",
      "Average training loss at batch  0 : 0.300\n",
      "Average training loss after epoch  2 : 0.065\n",
      "Average validation loss after epoch  2 : 0.048\n",
      "2019-12-08 10:34:59.569746 | Epoch 3\n",
      "Average training loss at batch  0 : 0.053\n",
      "Average training loss after epoch  3 : 0.037\n",
      "Average validation loss after epoch  3 : 0.039\n",
      "2019-12-08 10:35:40.415587 | Epoch 4\n",
      "Average training loss at batch  0 : 0.034\n",
      "Average training loss after epoch  4 : 0.026\n",
      "Average validation loss after epoch  4 : 0.034\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=5,upweight=25,lambda=10/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6cc3bb15944cd6acb1552b9702ab6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:36:21.842378 | Epoch 0\n",
      "Average training loss at batch  0 : 27.761\n",
      "Average training loss after epoch  0 : 16.208\n",
      "Average validation loss after epoch  0 : 4.796\n",
      "2019-12-08 10:36:23.322977 | Epoch 1\n",
      "Average training loss at batch  0 : 32.895\n",
      "Average training loss after epoch  1 : 16.247\n",
      "Average validation loss after epoch  1 : 4.787\n",
      "2019-12-08 10:36:24.847643 | Epoch 2\n",
      "Average training loss at batch  0 : 24.863\n",
      "Average training loss after epoch  2 : 16.243\n",
      "Average validation loss after epoch  2 : 4.785\n",
      "2019-12-08 10:36:26.414326 | Epoch 3\n",
      "Average training loss at batch  0 : 25.417\n",
      "Average training loss after epoch  3 : 16.272\n",
      "Average validation loss after epoch  3 : 4.786\n",
      "2019-12-08 10:36:27.960201 | Epoch 4\n",
      "Average training loss at batch  0 : 26.184\n",
      "Average training loss after epoch  4 : 16.283\n",
      "Average validation loss after epoch  4 : 4.787\n",
      "2019-12-08 10:36:29.473248 | Epoch 5\n",
      "Average training loss at batch  0 : 21.863\n",
      "Average training loss after epoch  5 : 16.256\n",
      "Average validation loss after epoch  5 : 4.788\n",
      "2019-12-08 10:36:31.005435 | Epoch 6\n",
      "Average training loss at batch  0 : 28.839\n",
      "Average training loss after epoch  6 : 16.268\n",
      "Average validation loss after epoch  6 : 4.782\n",
      "2019-12-08 10:36:32.547557 | Epoch 7\n",
      "Average training loss at batch  0 : 26.344\n",
      "Average training loss after epoch  7 : 16.293\n",
      "Average validation loss after epoch  7 : 4.779\n",
      "2019-12-08 10:36:34.135852 | Epoch 8\n",
      "Average training loss at batch  0 : 25.389\n",
      "Average training loss after epoch  8 : 16.256\n",
      "Average validation loss after epoch  8 : 4.773\n",
      "2019-12-08 10:36:35.615806 | Epoch 9\n",
      "Average training loss at batch  0 : 31.135\n",
      "Average training loss after epoch  9 : 16.293\n",
      "Average validation loss after epoch  9 : 4.768\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2bc910f64a42ff996f301b0f7b1d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:36:37.160392 | Epoch 0\n",
      "Average training loss at batch  0 : 31.384\n",
      "Average training loss after epoch  0 : 2.046\n",
      "Average validation loss after epoch  0 : 0.182\n",
      "2019-12-08 10:37:06.278408 | Epoch 1\n",
      "Average training loss at batch  0 : 1.052\n",
      "Average training loss after epoch  1 : 0.290\n",
      "Average validation loss after epoch  1 : 0.086\n",
      "2019-12-08 10:37:41.501326 | Epoch 2\n",
      "Average training loss at batch  0 : 0.294\n",
      "Average training loss after epoch  2 : 0.119\n",
      "Average validation loss after epoch  2 : 0.057\n",
      "2019-12-08 10:38:21.897679 | Epoch 3\n",
      "Average training loss at batch  0 : 0.130\n",
      "Average training loss after epoch  3 : 0.067\n",
      "Average validation loss after epoch  3 : 0.049\n",
      "2019-12-08 10:39:02.760817 | Epoch 4\n",
      "Average training loss at batch  0 : 0.081\n",
      "Average training loss after epoch  4 : 0.046\n",
      "Average validation loss after epoch  4 : 0.041\n",
      "\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/baseline_semisupervised_frozen_glove/num_unfrozen_epochs=5,upweight=25,lambda=25/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82542dc2ac2d4a8696cfed3bf92930fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:39:49.587000 | Epoch 0\n",
      "Average training loss at batch  0 : 67.976\n",
      "Average training loss after epoch  0 : 38.505\n",
      "Average validation loss after epoch  0 : 4.796\n",
      "2019-12-08 10:39:51.246073 | Epoch 1\n",
      "Average training loss at batch  0 : 76.188\n",
      "Average training loss after epoch  1 : 38.585\n",
      "Average validation loss after epoch  1 : 4.789\n",
      "2019-12-08 10:39:52.888197 | Epoch 2\n",
      "Average training loss at batch  0 : 65.935\n",
      "Average training loss after epoch  2 : 38.702\n",
      "Average validation loss after epoch  2 : 4.787\n",
      "2019-12-08 10:39:54.543926 | Epoch 3\n",
      "Average training loss at batch  0 : 72.237\n",
      "Average training loss after epoch  3 : 38.703\n",
      "Average validation loss after epoch  3 : 4.787\n",
      "2019-12-08 10:39:56.165805 | Epoch 4\n",
      "Average training loss at batch  0 : 58.349\n",
      "Average training loss after epoch  4 : 38.636\n",
      "Average validation loss after epoch  4 : 4.787\n",
      "2019-12-08 10:39:57.851103 | Epoch 5\n",
      "Average training loss at batch  0 : 54.617\n",
      "Average training loss after epoch  5 : 38.723\n",
      "Average validation loss after epoch  5 : 4.779\n",
      "2019-12-08 10:39:59.418433 | Epoch 6\n",
      "Average training loss at batch  0 : 57.067\n",
      "Average training loss after epoch  6 : 38.723\n",
      "Average validation loss after epoch  6 : 4.775\n",
      "2019-12-08 10:40:01.092196 | Epoch 7\n",
      "Average training loss at batch  0 : 70.672\n",
      "Average training loss after epoch  7 : 38.759\n",
      "Average validation loss after epoch  7 : 4.770\n",
      "2019-12-08 10:40:02.729334 | Epoch 8\n",
      "Average training loss at batch  0 : 65.458\n",
      "Average training loss after epoch  8 : 38.788\n",
      "Average validation loss after epoch  8 : 4.764\n",
      "2019-12-08 10:40:04.430783 | Epoch 9\n",
      "Average training loss at batch  0 : 59.056\n",
      "Average training loss after epoch  9 : 38.812\n",
      "Average validation loss after epoch  9 : 4.745\n",
      "\n",
      "*** UNFREEZING MODEL ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a67a9c7bd34aab9285e1f5c09e38d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 10:40:06.209818 | Epoch 0\n",
      "Average training loss at batch  0 : 67.315\n",
      "Average training loss after epoch  0 : 4.996\n",
      "Average validation loss after epoch  0 : 0.217\n",
      "2019-12-08 10:40:36.672927 | Epoch 1\n",
      "Average training loss at batch  0 : 1.709\n",
      "Average training loss after epoch  1 : 0.698\n",
      "Average validation loss after epoch  1 : 0.088\n",
      "2019-12-08 10:41:12.278258 | Epoch 2\n",
      "Average training loss at batch  0 : 0.567\n",
      "Average training loss after epoch  2 : 0.279\n",
      "Average validation loss after epoch  2 : 0.060\n",
      "2019-12-08 10:41:51.708104 | Epoch 3\n",
      "Average training loss at batch  0 : 0.220\n",
      "Average training loss after epoch  3 : 0.157\n",
      "Average validation loss after epoch  3 : 0.051\n",
      "2019-12-08 10:42:32.560197 | Epoch 4\n",
      "Average training loss at batch  0 : 0.122\n",
      "Average training loss after epoch  4 : 0.106\n",
      "Average validation loss after epoch  4 : 0.040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_unfrozen_epochs_list = [3, 4, 5]\n",
    "upweights = [25]\n",
    "lambda_losses = [.1, .5, 1, 5, 10, 25]\n",
    "\n",
    "for num_unfrozen_epochs in num_unfrozen_epochs_list:\n",
    "    for upweight in upweights:\n",
    "        for lambda_loss in lambda_losses:\n",
    "            opts = {\n",
    "                'embedding_matrix': glove_embedding_index,\n",
    "                'num_unfrozen_epochs': num_unfrozen_epochs,\n",
    "                'upweight': upweight,\n",
    "                'lambda_loss': lambda_loss\n",
    "            }\n",
    "            train_config(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_config(opts,verbose=True):\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    #print(path_to_save)\n",
    "    \n",
    "    model = neuralNetBow_glove(opts) #change according to model inputs\n",
    "    model.load_state_dict(torch.load(path_to_save+'model_dict.pt',map_location=lambda storage, loc: storage))\n",
    "    model = model.to(current_device)\n",
    "    criterion = KMeansCriterion().to(current_device)\n",
    "    centroids = torch.load(path_to_save+'centroids',map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    TP_cluster, FP_cluster, results_dict = evaluation.main(model, centroids, val_loader, criterion, data_dir, current_device, verbose)\n",
    "    results_dict.update(opts)\n",
    "    return TP_cluster, FP_cluster, results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unfrozen_epochs_list = [0, 1, 2]\n",
    "upweights = [1, 5, 10, 25]\n",
    "lambda_losses = [.1, .5, 1, 5, 10, 25]\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "for num_unfrozen_epochs in num_unfrozen_epochs_list:\n",
    "    for upweight in upweights:\n",
    "        for lambda_loss in lambda_losses:\n",
    "            opts = {\n",
    "                'embedding_matrix': glove_embedding_index,\n",
    "                'num_unfrozen_epochs': num_unfrozen_epochs,\n",
    "                'upweight': upweight,\n",
    "                'lambda_loss': lambda_loss\n",
    "            }\n",
    "            _, _, results_dict = evaluate_config(opts,False)\n",
    "            results_df = results_df.append(results_dict,ignore_index=True)\n",
    "      \n",
    "# tried just 25 for the following:\n",
    "num_unfrozen_epochs_list = [3, 4, 5]\n",
    "upweights = [25]\n",
    "lambda_losses = [.1, .5, 1, 5, 10, 25]\n",
    "for num_unfrozen_epochs in num_unfrozen_epochs_list:\n",
    "    for upweight in upweights:\n",
    "        for lambda_loss in lambda_losses:\n",
    "            opts = {\n",
    "                'embedding_matrix': glove_embedding_index,\n",
    "                'num_unfrozen_epochs': num_unfrozen_epochs,\n",
    "                'upweight': upweight,\n",
    "                'lambda_loss': lambda_loss\n",
    "            }\n",
    "            _, _, results_dict = evaluate_config(opts,False)\n",
    "            results_df = results_df.append(results_dict,ignore_index=True)\n",
    "        \n",
    "results_df = results_df[['num_unfrozen_epochs','upweight','lambda_loss','Accuracy','F1 score','Precision','Recall',\n",
    "                        'TP_rate','FP_rate','FN_rate','TN_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_unfrozen_epochs</th>\n",
       "      <th>upweight</th>\n",
       "      <th>lambda_loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>TP_rate</th>\n",
       "      <th>FP_rate</th>\n",
       "      <th>FN_rate</th>\n",
       "      <th>TN_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.564989</td>\n",
       "      <td>0.677833</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.538217</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.785276</td>\n",
       "      <td>0.214724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.627055</td>\n",
       "      <td>0.722114</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.575441</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.715026</td>\n",
       "      <td>0.284974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.618121</td>\n",
       "      <td>0.717090</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>0.569497</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.268293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.627055</td>\n",
       "      <td>0.722114</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.575441</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.715026</td>\n",
       "      <td>0.284974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.619544</td>\n",
       "      <td>0.717890</td>\n",
       "      <td>0.968153</td>\n",
       "      <td>0.570436</td>\n",
       "      <td>0.968153</td>\n",
       "      <td>0.031847</td>\n",
       "      <td>0.729064</td>\n",
       "      <td>0.270936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.616030</td>\n",
       "      <td>0.715916</td>\n",
       "      <td>0.967638</td>\n",
       "      <td>0.568124</td>\n",
       "      <td>0.967638</td>\n",
       "      <td>0.032362</td>\n",
       "      <td>0.735577</td>\n",
       "      <td>0.264423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.620993</td>\n",
       "      <td>0.718704</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>0.571394</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.726368</td>\n",
       "      <td>0.273632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.549192</td>\n",
       "      <td>0.666870</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.528827</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.195946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.563797</td>\n",
       "      <td>0.674857</td>\n",
       "      <td>0.905371</td>\n",
       "      <td>0.537903</td>\n",
       "      <td>0.905371</td>\n",
       "      <td>0.094629</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571176</td>\n",
       "      <td>0.679652</td>\n",
       "      <td>0.909794</td>\n",
       "      <td>0.542436</td>\n",
       "      <td>0.909794</td>\n",
       "      <td>0.090206</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.232558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.578458</td>\n",
       "      <td>0.684087</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.547016</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.087179</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.244094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.578458</td>\n",
       "      <td>0.684087</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.547016</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.087179</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.244094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.571134</td>\n",
       "      <td>0.679296</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.091603</td>\n",
       "      <td>0.766129</td>\n",
       "      <td>0.233871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.554751</td>\n",
       "      <td>0.669465</td>\n",
       "      <td>0.901809</td>\n",
       "      <td>0.532318</td>\n",
       "      <td>0.901809</td>\n",
       "      <td>0.098191</td>\n",
       "      <td>0.792308</td>\n",
       "      <td>0.207692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.557883</td>\n",
       "      <td>0.670644</td>\n",
       "      <td>0.900249</td>\n",
       "      <td>0.534358</td>\n",
       "      <td>0.900249</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>0.784483</td>\n",
       "      <td>0.215517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.564536</td>\n",
       "      <td>0.674650</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.538485</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.097015</td>\n",
       "      <td>0.773913</td>\n",
       "      <td>0.226087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.579775</td>\n",
       "      <td>0.684478</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>0.547951</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>0.088384</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.247934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.578102</td>\n",
       "      <td>0.682858</td>\n",
       "      <td>0.908416</td>\n",
       "      <td>0.547031</td>\n",
       "      <td>0.908416</td>\n",
       "      <td>0.091584</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.247788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.585170</td>\n",
       "      <td>0.687856</td>\n",
       "      <td>0.914141</td>\n",
       "      <td>0.551371</td>\n",
       "      <td>0.914141</td>\n",
       "      <td>0.085859</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.256198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.552922</td>\n",
       "      <td>0.668432</td>\n",
       "      <td>0.901299</td>\n",
       "      <td>0.531190</td>\n",
       "      <td>0.901299</td>\n",
       "      <td>0.098701</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.204545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.566841</td>\n",
       "      <td>0.675624</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>0.540007</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.231481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.573934</td>\n",
       "      <td>0.679877</td>\n",
       "      <td>0.904878</td>\n",
       "      <td>0.544488</td>\n",
       "      <td>0.904878</td>\n",
       "      <td>0.095122</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.242991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.588155</td>\n",
       "      <td>0.689173</td>\n",
       "      <td>0.913151</td>\n",
       "      <td>0.553427</td>\n",
       "      <td>0.913151</td>\n",
       "      <td>0.086849</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.583764</td>\n",
       "      <td>0.686363</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.550636</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.256637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.595502</td>\n",
       "      <td>0.694025</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.558091</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.726496</td>\n",
       "      <td>0.273504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.638402</td>\n",
       "      <td>0.726048</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.584399</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.318471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.749688</td>\n",
       "      <td>0.790382</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.775541</td>\n",
       "      <td>0.806059</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>0.709582</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.618182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.753591</td>\n",
       "      <td>0.792650</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>0.684196</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>0.058036</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.737844</td>\n",
       "      <td>0.782190</td>\n",
       "      <td>0.941441</td>\n",
       "      <td>0.669020</td>\n",
       "      <td>0.941441</td>\n",
       "      <td>0.058559</td>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.534247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.742054</td>\n",
       "      <td>0.785315</td>\n",
       "      <td>0.943567</td>\n",
       "      <td>0.672523</td>\n",
       "      <td>0.943567</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.540541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.534307</td>\n",
       "      <td>0.657948</td>\n",
       "      <td>0.895775</td>\n",
       "      <td>0.519912</td>\n",
       "      <td>0.895775</td>\n",
       "      <td>0.104225</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.172840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.548402</td>\n",
       "      <td>0.666415</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.528346</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.805369</td>\n",
       "      <td>0.194631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553270</td>\n",
       "      <td>0.669205</td>\n",
       "      <td>0.903743</td>\n",
       "      <td>0.531318</td>\n",
       "      <td>0.903743</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.202797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.560342</td>\n",
       "      <td>0.673848</td>\n",
       "      <td>0.908356</td>\n",
       "      <td>0.535579</td>\n",
       "      <td>0.908356</td>\n",
       "      <td>0.091644</td>\n",
       "      <td>0.787671</td>\n",
       "      <td>0.212329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.562937</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.537190</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.783217</td>\n",
       "      <td>0.216783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.562522</td>\n",
       "      <td>0.674769</td>\n",
       "      <td>0.907652</td>\n",
       "      <td>0.536989</td>\n",
       "      <td>0.907652</td>\n",
       "      <td>0.092348</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.517471</td>\n",
       "      <td>0.647053</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.510074</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.849673</td>\n",
       "      <td>0.150327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.538609</td>\n",
       "      <td>0.659773</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.522549</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.817518</td>\n",
       "      <td>0.182482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.550865</td>\n",
       "      <td>0.666718</td>\n",
       "      <td>0.898477</td>\n",
       "      <td>0.530005</td>\n",
       "      <td>0.898477</td>\n",
       "      <td>0.101523</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.203252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.568019</td>\n",
       "      <td>0.677548</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.540503</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.771654</td>\n",
       "      <td>0.228346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.582869</td>\n",
       "      <td>0.686565</td>\n",
       "      <td>0.913706</td>\n",
       "      <td>0.549871</td>\n",
       "      <td>0.913706</td>\n",
       "      <td>0.086294</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.252033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.586670</td>\n",
       "      <td>0.689425</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.552157</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.255814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.534430</td>\n",
       "      <td>0.657154</td>\n",
       "      <td>0.892388</td>\n",
       "      <td>0.520065</td>\n",
       "      <td>0.892388</td>\n",
       "      <td>0.107612</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.555452</td>\n",
       "      <td>0.669004</td>\n",
       "      <td>0.898515</td>\n",
       "      <td>0.532887</td>\n",
       "      <td>0.898515</td>\n",
       "      <td>0.101485</td>\n",
       "      <td>0.787611</td>\n",
       "      <td>0.212389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.579775</td>\n",
       "      <td>0.684478</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>0.547951</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>0.088384</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.247934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.606549</td>\n",
       "      <td>0.701014</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.565291</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.290598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.612073</td>\n",
       "      <td>0.704533</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.700855</td>\n",
       "      <td>0.299145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.606549</td>\n",
       "      <td>0.701014</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.565291</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.290598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.640394</td>\n",
       "      <td>0.726720</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.586037</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.043716</td>\n",
       "      <td>0.675497</td>\n",
       "      <td>0.324503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.734813</td>\n",
       "      <td>0.780565</td>\n",
       "      <td>0.943311</td>\n",
       "      <td>0.665712</td>\n",
       "      <td>0.943311</td>\n",
       "      <td>0.056689</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.782992</td>\n",
       "      <td>0.812256</td>\n",
       "      <td>0.938865</td>\n",
       "      <td>0.715737</td>\n",
       "      <td>0.938865</td>\n",
       "      <td>0.061135</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.627119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.757811</td>\n",
       "      <td>0.795498</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.688381</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.057906</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.573529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.749688</td>\n",
       "      <td>0.790382</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.730588</td>\n",
       "      <td>0.777454</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.662252</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.639110</td>\n",
       "      <td>0.727416</td>\n",
       "      <td>0.963068</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.963068</td>\n",
       "      <td>0.036932</td>\n",
       "      <td>0.684848</td>\n",
       "      <td>0.315152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.749688</td>\n",
       "      <td>0.790382</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784268</td>\n",
       "      <td>0.812844</td>\n",
       "      <td>0.936957</td>\n",
       "      <td>0.717767</td>\n",
       "      <td>0.936957</td>\n",
       "      <td>0.063043</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.734168</td>\n",
       "      <td>0.779784</td>\n",
       "      <td>0.941309</td>\n",
       "      <td>0.665574</td>\n",
       "      <td>0.941309</td>\n",
       "      <td>0.058691</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.527027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.700812</td>\n",
       "      <td>0.757404</td>\n",
       "      <td>0.934091</td>\n",
       "      <td>0.636927</td>\n",
       "      <td>0.934091</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.467532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.719388</td>\n",
       "      <td>0.769874</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.652482</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.639767</td>\n",
       "      <td>0.727306</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.585118</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.318750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.740878</td>\n",
       "      <td>0.783481</td>\n",
       "      <td>0.937639</td>\n",
       "      <td>0.672856</td>\n",
       "      <td>0.937639</td>\n",
       "      <td>0.062361</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.544118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.753776</td>\n",
       "      <td>0.791750</td>\n",
       "      <td>0.936123</td>\n",
       "      <td>0.685958</td>\n",
       "      <td>0.936123</td>\n",
       "      <td>0.063877</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.703962</td>\n",
       "      <td>0.759373</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.639647</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.065760</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.684425</td>\n",
       "      <td>0.746917</td>\n",
       "      <td>0.931350</td>\n",
       "      <td>0.623456</td>\n",
       "      <td>0.931350</td>\n",
       "      <td>0.068650</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.684441</td>\n",
       "      <td>0.748081</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.622532</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.431818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.634659</td>\n",
       "      <td>0.724389</td>\n",
       "      <td>0.960227</td>\n",
       "      <td>0.581555</td>\n",
       "      <td>0.960227</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.309091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.753663</td>\n",
       "      <td>0.793044</td>\n",
       "      <td>0.943946</td>\n",
       "      <td>0.683739</td>\n",
       "      <td>0.943946</td>\n",
       "      <td>0.056054</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>0.563380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.749416</td>\n",
       "      <td>0.788135</td>\n",
       "      <td>0.932166</td>\n",
       "      <td>0.682656</td>\n",
       "      <td>0.932166</td>\n",
       "      <td>0.067834</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.703962</td>\n",
       "      <td>0.759373</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.639647</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.065760</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.699319</td>\n",
       "      <td>0.757612</td>\n",
       "      <td>0.939815</td>\n",
       "      <td>0.634585</td>\n",
       "      <td>0.939815</td>\n",
       "      <td>0.060185</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.458824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.654753</td>\n",
       "      <td>0.727983</td>\n",
       "      <td>0.923963</td>\n",
       "      <td>0.600592</td>\n",
       "      <td>0.923963</td>\n",
       "      <td>0.076037</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.385542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_unfrozen_epochs  upweight  lambda_loss  Accuracy  F1 score  Precision  \\\n",
       "0   0.0                  1.0       0.1          0.564214  0.677387  0.915014    \n",
       "1   0.0                  1.0       0.5          0.564214  0.677387  0.915014    \n",
       "2   0.0                  1.0       1.0          0.564214  0.677387  0.915014    \n",
       "3   0.0                  1.0       5.0          0.564214  0.677387  0.915014    \n",
       "4   0.0                  1.0       10.0         0.564214  0.677387  0.915014    \n",
       "5   0.0                  1.0       25.0         0.564989  0.677833  0.915254    \n",
       "6   0.0                  5.0       0.1          0.560509  0.674914  0.912429    \n",
       "7   0.0                  5.0       0.5          0.560509  0.674914  0.912429    \n",
       "8   0.0                  5.0       1.0          0.560509  0.674914  0.912429    \n",
       "9   0.0                  5.0       5.0          0.560509  0.674914  0.912429    \n",
       "10  0.0                  5.0       10.0         0.560509  0.674914  0.912429    \n",
       "11  0.0                  5.0       25.0         0.560509  0.674914  0.912429    \n",
       "12  0.0                  10.0      0.1          0.565596  0.678531  0.916905    \n",
       "13  0.0                  10.0      0.5          0.565596  0.678531  0.916905    \n",
       "14  0.0                  10.0      1.0          0.565596  0.678531  0.916905    \n",
       "15  0.0                  10.0      5.0          0.565596  0.678531  0.916905    \n",
       "16  0.0                  10.0      10.0         0.565596  0.678531  0.916905    \n",
       "17  0.0                  10.0      25.0         0.565596  0.678531  0.916905    \n",
       "18  0.0                  25.0      0.1          0.627055  0.722114  0.969136    \n",
       "19  0.0                  25.0      0.5          0.618121  0.717090  0.967949    \n",
       "20  0.0                  25.0      1.0          0.627055  0.722114  0.969136    \n",
       "21  0.0                  25.0      5.0          0.619544  0.717890  0.968153    \n",
       "22  0.0                  25.0      10.0         0.616030  0.715916  0.967638    \n",
       "23  0.0                  25.0      25.0         0.620993  0.718704  0.968354    \n",
       "24  1.0                  1.0       0.1          0.549192  0.666870  0.902439    \n",
       "25  1.0                  1.0       0.5          0.563797  0.674857  0.905371    \n",
       "26  1.0                  1.0       1.0          0.571176  0.679652  0.909794    \n",
       "27  1.0                  1.0       5.0          0.578458  0.684087  0.912821    \n",
       "28  1.0                  1.0       10.0         0.578458  0.684087  0.912821    \n",
       "29  1.0                  1.0       25.0         0.571134  0.679296  0.908397    \n",
       "30  1.0                  5.0       0.1          0.554751  0.669465  0.901809    \n",
       "31  1.0                  5.0       0.5          0.557883  0.670644  0.900249    \n",
       "32  1.0                  5.0       1.0          0.564536  0.674650  0.902985    \n",
       "33  1.0                  5.0       5.0          0.579775  0.684478  0.911616    \n",
       "34  1.0                  5.0       10.0         0.578102  0.682858  0.908416    \n",
       "35  1.0                  5.0       25.0         0.585170  0.687856  0.914141    \n",
       "36  1.0                  10.0      0.1          0.552922  0.668432  0.901299    \n",
       "37  1.0                  10.0      0.5          0.566841  0.675624  0.902200    \n",
       "38  1.0                  10.0      1.0          0.573934  0.679877  0.904878    \n",
       "39  1.0                  10.0      5.0          0.588155  0.689173  0.913151    \n",
       "40  1.0                  10.0      10.0         0.583764  0.686363  0.910891    \n",
       "41  1.0                  10.0      25.0         0.595502  0.694025  0.917500    \n",
       "42  1.0                  25.0      0.1          0.638402  0.726048  0.958333    \n",
       "43  1.0                  25.0      0.5          0.749688  0.790382  0.943820    \n",
       "44  1.0                  25.0      1.0          0.775541  0.806059  0.932900    \n",
       "45  1.0                  25.0      5.0          0.753591  0.792650  0.941964    \n",
       "46  1.0                  25.0      10.0         0.737844  0.782190  0.941441    \n",
       "47  1.0                  25.0      25.0         0.742054  0.785315  0.943567    \n",
       "48  2.0                  1.0       0.1          0.534307  0.657948  0.895775    \n",
       "49  2.0                  1.0       0.5          0.548402  0.666415  0.902174    \n",
       "50  2.0                  1.0       1.0          0.553270  0.669205  0.903743    \n",
       "51  2.0                  1.0       5.0          0.560342  0.673848  0.908356    \n",
       "52  2.0                  1.0       10.0         0.562937  0.675325  0.909091    \n",
       "53  2.0                  1.0       25.0         0.562522  0.674769  0.907652    \n",
       "54  2.0                  5.0       0.1          0.517471  0.647053  0.884615    \n",
       "55  2.0                  5.0       0.5          0.538609  0.659773  0.894737    \n",
       "56  2.0                  5.0       1.0          0.550865  0.666718  0.898477    \n",
       "57  2.0                  5.0       5.0          0.568019  0.677548  0.907692    \n",
       "58  2.0                  5.0       10.0         0.582869  0.686565  0.913706    \n",
       "59  2.0                  5.0       25.0         0.586670  0.689425  0.917526    \n",
       "60  2.0                  10.0      0.1          0.534430  0.657154  0.892388    \n",
       "61  2.0                  10.0      0.5          0.555452  0.669004  0.898515    \n",
       "62  2.0                  10.0      1.0          0.579775  0.684478  0.911616    \n",
       "63  2.0                  10.0      5.0          0.606549  0.701014  0.922500    \n",
       "64  2.0                  10.0      10.0         0.612073  0.704533  0.925000    \n",
       "65  2.0                  10.0      25.0         0.606549  0.701014  0.922500    \n",
       "66  2.0                  25.0      0.1          0.640394  0.726720  0.956284    \n",
       "67  2.0                  25.0      0.5          0.734813  0.780565  0.943311    \n",
       "68  2.0                  25.0      1.0          0.782992  0.812256  0.938865    \n",
       "69  2.0                  25.0      5.0          0.757811  0.795498  0.942094    \n",
       "70  2.0                  25.0      10.0         0.749688  0.790382  0.943820    \n",
       "71  2.0                  25.0      25.0         0.730588  0.777454  0.941176    \n",
       "72  3.0                  25.0      0.1          0.639110  0.727416  0.963068    \n",
       "73  3.0                  25.0      0.5          0.749688  0.790382  0.943820    \n",
       "74  3.0                  25.0      1.0          0.784268  0.812844  0.936957    \n",
       "75  3.0                  25.0      5.0          0.734168  0.779784  0.941309    \n",
       "76  3.0                  25.0      10.0         0.700812  0.757404  0.934091    \n",
       "77  3.0                  25.0      25.0         0.719388  0.769874  0.938776    \n",
       "78  4.0                  25.0      0.1          0.639767  0.727306  0.960784    \n",
       "79  4.0                  25.0      0.5          0.740878  0.783481  0.937639    \n",
       "80  4.0                  25.0      1.0          0.753776  0.791750  0.936123    \n",
       "81  4.0                  25.0      5.0          0.703962  0.759373  0.934240    \n",
       "82  4.0                  25.0      10.0         0.684425  0.746917  0.931350    \n",
       "83  4.0                  25.0      25.0         0.684441  0.748081  0.937063    \n",
       "84  5.0                  25.0      0.1          0.634659  0.724389  0.960227    \n",
       "85  5.0                  25.0      0.5          0.753663  0.793044  0.943946    \n",
       "86  5.0                  25.0      1.0          0.749416  0.788135  0.932166    \n",
       "87  5.0                  25.0      5.0          0.703962  0.759373  0.934240    \n",
       "88  5.0                  25.0      10.0         0.699319  0.757612  0.939815    \n",
       "89  5.0                  25.0      25.0         0.654753  0.727983  0.923963    \n",
       "\n",
       "      Recall   TP_rate   FP_rate   FN_rate   TN_rate  \n",
       "0   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "1   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "2   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "3   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "4   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "5   0.538217  0.915254  0.084746  0.785276  0.214724  \n",
       "6   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "7   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "8   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "9   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "10  0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "11  0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "12  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "13  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "14  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "15  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "16  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "17  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "18  0.575441  0.969136  0.030864  0.715026  0.284974  \n",
       "19  0.569497  0.967949  0.032051  0.731707  0.268293  \n",
       "20  0.575441  0.969136  0.030864  0.715026  0.284974  \n",
       "21  0.570436  0.968153  0.031847  0.729064  0.270936  \n",
       "22  0.568124  0.967638  0.032362  0.735577  0.264423  \n",
       "23  0.571394  0.968354  0.031646  0.726368  0.273632  \n",
       "24  0.528827  0.902439  0.097561  0.804054  0.195946  \n",
       "25  0.537903  0.905371  0.094629  0.777778  0.222222  \n",
       "26  0.542436  0.909794  0.090206  0.767442  0.232558  \n",
       "27  0.547016  0.912821  0.087179  0.755906  0.244094  \n",
       "28  0.547016  0.912821  0.087179  0.755906  0.244094  \n",
       "29  0.542480  0.908397  0.091603  0.766129  0.233871  \n",
       "30  0.532318  0.901809  0.098191  0.792308  0.207692  \n",
       "31  0.534358  0.900249  0.099751  0.784483  0.215517  \n",
       "32  0.538485  0.902985  0.097015  0.773913  0.226087  \n",
       "33  0.547951  0.911616  0.088384  0.752066  0.247934  \n",
       "34  0.547031  0.908416  0.091584  0.752212  0.247788  \n",
       "35  0.551371  0.914141  0.085859  0.743802  0.256198  \n",
       "36  0.531190  0.901299  0.098701  0.795455  0.204545  \n",
       "37  0.540007  0.902200  0.097800  0.768519  0.231481  \n",
       "38  0.544488  0.904878  0.095122  0.757009  0.242991  \n",
       "39  0.553427  0.913151  0.086849  0.736842  0.263158  \n",
       "40  0.550636  0.910891  0.089109  0.743363  0.256637  \n",
       "41  0.558091  0.917500  0.082500  0.726496  0.273504  \n",
       "42  0.584399  0.958333  0.041667  0.681529  0.318471  \n",
       "43  0.679856  0.943820  0.056180  0.444444  0.555556  \n",
       "44  0.709582  0.932900  0.067100  0.381818  0.618182  \n",
       "45  0.684196  0.941964  0.058036  0.434783  0.565217  \n",
       "46  0.669020  0.941441  0.058559  0.465753  0.534247  \n",
       "47  0.672523  0.943567  0.056433  0.459459  0.540541  \n",
       "48  0.519912  0.895775  0.104225  0.827160  0.172840  \n",
       "49  0.528346  0.902174  0.097826  0.805369  0.194631  \n",
       "50  0.531318  0.903743  0.096257  0.797203  0.202797  \n",
       "51  0.535579  0.908356  0.091644  0.787671  0.212329  \n",
       "52  0.537190  0.909091  0.090909  0.783217  0.216783  \n",
       "53  0.536989  0.907652  0.092348  0.782609  0.217391  \n",
       "54  0.510074  0.884615  0.115385  0.849673  0.150327  \n",
       "55  0.522549  0.894737  0.105263  0.817518  0.182482  \n",
       "56  0.530005  0.898477  0.101523  0.796748  0.203252  \n",
       "57  0.540503  0.907692  0.092308  0.771654  0.228346  \n",
       "58  0.549871  0.913706  0.086294  0.747967  0.252033  \n",
       "59  0.552157  0.917526  0.082474  0.744186  0.255814  \n",
       "60  0.520065  0.892388  0.107612  0.823529  0.176471  \n",
       "61  0.532887  0.898515  0.101485  0.787611  0.212389  \n",
       "62  0.547951  0.911616  0.088384  0.752066  0.247934  \n",
       "63  0.565291  0.922500  0.077500  0.709402  0.290598  \n",
       "64  0.568932  0.925000  0.075000  0.700855  0.299145  \n",
       "65  0.565291  0.922500  0.077500  0.709402  0.290598  \n",
       "66  0.586037  0.956284  0.043716  0.675497  0.324503  \n",
       "67  0.665712  0.943311  0.056689  0.473684  0.526316  \n",
       "68  0.715737  0.938865  0.061135  0.372881  0.627119  \n",
       "69  0.688381  0.942094  0.057906  0.426471  0.573529  \n",
       "70  0.679856  0.943820  0.056180  0.444444  0.555556  \n",
       "71  0.662252  0.941176  0.058824  0.480000  0.520000  \n",
       "72  0.584416  0.963068  0.036932  0.684848  0.315152  \n",
       "73  0.679856  0.943820  0.056180  0.444444  0.555556  \n",
       "74  0.717767  0.936957  0.063043  0.368421  0.631579  \n",
       "75  0.665574  0.941309  0.058691  0.472973  0.527027  \n",
       "76  0.636927  0.934091  0.065909  0.532468  0.467532  \n",
       "77  0.652482  0.938776  0.061224  0.500000  0.500000  \n",
       "78  0.585118  0.960784  0.039216  0.681250  0.318750  \n",
       "79  0.672856  0.937639  0.062361  0.455882  0.544118  \n",
       "80  0.685958  0.936123  0.063877  0.428571  0.571429  \n",
       "81  0.639647  0.934240  0.065760  0.526316  0.473684  \n",
       "82  0.623456  0.931350  0.068650  0.562500  0.437500  \n",
       "83  0.622532  0.937063  0.062937  0.568182  0.431818  \n",
       "84  0.581555  0.960227  0.039773  0.690909  0.309091  \n",
       "85  0.683739  0.943946  0.056054  0.436620  0.563380  \n",
       "86  0.682656  0.932166  0.067834  0.433333  0.566667  \n",
       "87  0.639647  0.934240  0.065760  0.526316  0.473684  \n",
       "88  0.634585  0.939815  0.060185  0.541176  0.458824  \n",
       "89  0.600592  0.923963  0.076037  0.614458  0.385542  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best params: num_unfrozen=3, upweight=25, lambda_loss = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_unfrozen_epochs</th>\n",
       "      <th>upweight</th>\n",
       "      <th>lambda_loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>TP_rate</th>\n",
       "      <th>FP_rate</th>\n",
       "      <th>FN_rate</th>\n",
       "      <th>TN_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784268</td>\n",
       "      <td>0.812844</td>\n",
       "      <td>0.936957</td>\n",
       "      <td>0.717767</td>\n",
       "      <td>0.936957</td>\n",
       "      <td>0.063043</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.782992</td>\n",
       "      <td>0.812256</td>\n",
       "      <td>0.938865</td>\n",
       "      <td>0.715737</td>\n",
       "      <td>0.938865</td>\n",
       "      <td>0.061135</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.627119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.775541</td>\n",
       "      <td>0.806059</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>0.709582</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.618182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.757811</td>\n",
       "      <td>0.795498</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.688381</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.057906</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.573529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.753663</td>\n",
       "      <td>0.793044</td>\n",
       "      <td>0.943946</td>\n",
       "      <td>0.683739</td>\n",
       "      <td>0.943946</td>\n",
       "      <td>0.056054</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>0.563380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.753591</td>\n",
       "      <td>0.792650</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>0.684196</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>0.058036</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.753776</td>\n",
       "      <td>0.791750</td>\n",
       "      <td>0.936123</td>\n",
       "      <td>0.685958</td>\n",
       "      <td>0.936123</td>\n",
       "      <td>0.063877</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.749688</td>\n",
       "      <td>0.790382</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.749688</td>\n",
       "      <td>0.790382</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.749688</td>\n",
       "      <td>0.790382</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.749416</td>\n",
       "      <td>0.788135</td>\n",
       "      <td>0.932166</td>\n",
       "      <td>0.682656</td>\n",
       "      <td>0.932166</td>\n",
       "      <td>0.067834</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.742054</td>\n",
       "      <td>0.785315</td>\n",
       "      <td>0.943567</td>\n",
       "      <td>0.672523</td>\n",
       "      <td>0.943567</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.540541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.740878</td>\n",
       "      <td>0.783481</td>\n",
       "      <td>0.937639</td>\n",
       "      <td>0.672856</td>\n",
       "      <td>0.937639</td>\n",
       "      <td>0.062361</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.544118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.737844</td>\n",
       "      <td>0.782190</td>\n",
       "      <td>0.941441</td>\n",
       "      <td>0.669020</td>\n",
       "      <td>0.941441</td>\n",
       "      <td>0.058559</td>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.534247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.734813</td>\n",
       "      <td>0.780565</td>\n",
       "      <td>0.943311</td>\n",
       "      <td>0.665712</td>\n",
       "      <td>0.943311</td>\n",
       "      <td>0.056689</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.734168</td>\n",
       "      <td>0.779784</td>\n",
       "      <td>0.941309</td>\n",
       "      <td>0.665574</td>\n",
       "      <td>0.941309</td>\n",
       "      <td>0.058691</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.527027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.730588</td>\n",
       "      <td>0.777454</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.662252</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.719388</td>\n",
       "      <td>0.769874</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.652482</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.703962</td>\n",
       "      <td>0.759373</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.639647</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.065760</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.703962</td>\n",
       "      <td>0.759373</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.639647</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.065760</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.699319</td>\n",
       "      <td>0.757612</td>\n",
       "      <td>0.939815</td>\n",
       "      <td>0.634585</td>\n",
       "      <td>0.939815</td>\n",
       "      <td>0.060185</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.458824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.700812</td>\n",
       "      <td>0.757404</td>\n",
       "      <td>0.934091</td>\n",
       "      <td>0.636927</td>\n",
       "      <td>0.934091</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.467532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.684441</td>\n",
       "      <td>0.748081</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.622532</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.431818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.684425</td>\n",
       "      <td>0.746917</td>\n",
       "      <td>0.931350</td>\n",
       "      <td>0.623456</td>\n",
       "      <td>0.931350</td>\n",
       "      <td>0.068650</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.654753</td>\n",
       "      <td>0.727983</td>\n",
       "      <td>0.923963</td>\n",
       "      <td>0.600592</td>\n",
       "      <td>0.923963</td>\n",
       "      <td>0.076037</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.385542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.639110</td>\n",
       "      <td>0.727416</td>\n",
       "      <td>0.963068</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.963068</td>\n",
       "      <td>0.036932</td>\n",
       "      <td>0.684848</td>\n",
       "      <td>0.315152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.639767</td>\n",
       "      <td>0.727306</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.585118</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.318750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.640394</td>\n",
       "      <td>0.726720</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.586037</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.043716</td>\n",
       "      <td>0.675497</td>\n",
       "      <td>0.324503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.638402</td>\n",
       "      <td>0.726048</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.584399</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.318471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.634659</td>\n",
       "      <td>0.724389</td>\n",
       "      <td>0.960227</td>\n",
       "      <td>0.581555</td>\n",
       "      <td>0.960227</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.309091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.627055</td>\n",
       "      <td>0.722114</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.575441</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.715026</td>\n",
       "      <td>0.284974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.627055</td>\n",
       "      <td>0.722114</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.575441</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.715026</td>\n",
       "      <td>0.284974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.620993</td>\n",
       "      <td>0.718704</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>0.571394</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.726368</td>\n",
       "      <td>0.273632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.619544</td>\n",
       "      <td>0.717890</td>\n",
       "      <td>0.968153</td>\n",
       "      <td>0.570436</td>\n",
       "      <td>0.968153</td>\n",
       "      <td>0.031847</td>\n",
       "      <td>0.729064</td>\n",
       "      <td>0.270936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.618121</td>\n",
       "      <td>0.717090</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>0.569497</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.268293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.616030</td>\n",
       "      <td>0.715916</td>\n",
       "      <td>0.967638</td>\n",
       "      <td>0.568124</td>\n",
       "      <td>0.967638</td>\n",
       "      <td>0.032362</td>\n",
       "      <td>0.735577</td>\n",
       "      <td>0.264423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.612073</td>\n",
       "      <td>0.704533</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.700855</td>\n",
       "      <td>0.299145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.606549</td>\n",
       "      <td>0.701014</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.565291</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.290598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.606549</td>\n",
       "      <td>0.701014</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.565291</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.290598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.595502</td>\n",
       "      <td>0.694025</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.558091</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.726496</td>\n",
       "      <td>0.273504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.586670</td>\n",
       "      <td>0.689425</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.552157</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.255814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.588155</td>\n",
       "      <td>0.689173</td>\n",
       "      <td>0.913151</td>\n",
       "      <td>0.553427</td>\n",
       "      <td>0.913151</td>\n",
       "      <td>0.086849</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.585170</td>\n",
       "      <td>0.687856</td>\n",
       "      <td>0.914141</td>\n",
       "      <td>0.551371</td>\n",
       "      <td>0.914141</td>\n",
       "      <td>0.085859</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.256198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.582869</td>\n",
       "      <td>0.686565</td>\n",
       "      <td>0.913706</td>\n",
       "      <td>0.549871</td>\n",
       "      <td>0.913706</td>\n",
       "      <td>0.086294</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.252033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.583764</td>\n",
       "      <td>0.686363</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.550636</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.256637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.579775</td>\n",
       "      <td>0.684478</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>0.547951</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>0.088384</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.247934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.579775</td>\n",
       "      <td>0.684478</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>0.547951</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>0.088384</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.247934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.578458</td>\n",
       "      <td>0.684087</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.547016</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.087179</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.244094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.578458</td>\n",
       "      <td>0.684087</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.547016</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.087179</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.244094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.578102</td>\n",
       "      <td>0.682858</td>\n",
       "      <td>0.908416</td>\n",
       "      <td>0.547031</td>\n",
       "      <td>0.908416</td>\n",
       "      <td>0.091584</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.247788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.573934</td>\n",
       "      <td>0.679877</td>\n",
       "      <td>0.904878</td>\n",
       "      <td>0.544488</td>\n",
       "      <td>0.904878</td>\n",
       "      <td>0.095122</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.242991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571176</td>\n",
       "      <td>0.679652</td>\n",
       "      <td>0.909794</td>\n",
       "      <td>0.542436</td>\n",
       "      <td>0.909794</td>\n",
       "      <td>0.090206</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.232558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.571134</td>\n",
       "      <td>0.679296</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.091603</td>\n",
       "      <td>0.766129</td>\n",
       "      <td>0.233871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.564989</td>\n",
       "      <td>0.677833</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.538217</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.785276</td>\n",
       "      <td>0.214724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.568019</td>\n",
       "      <td>0.677548</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.540503</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.771654</td>\n",
       "      <td>0.228346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.566841</td>\n",
       "      <td>0.675624</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>0.540007</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.231481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.562937</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.537190</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.783217</td>\n",
       "      <td>0.216783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.563797</td>\n",
       "      <td>0.674857</td>\n",
       "      <td>0.905371</td>\n",
       "      <td>0.537903</td>\n",
       "      <td>0.905371</td>\n",
       "      <td>0.094629</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.562522</td>\n",
       "      <td>0.674769</td>\n",
       "      <td>0.907652</td>\n",
       "      <td>0.536989</td>\n",
       "      <td>0.907652</td>\n",
       "      <td>0.092348</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.564536</td>\n",
       "      <td>0.674650</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.538485</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.097015</td>\n",
       "      <td>0.773913</td>\n",
       "      <td>0.226087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.560342</td>\n",
       "      <td>0.673848</td>\n",
       "      <td>0.908356</td>\n",
       "      <td>0.535579</td>\n",
       "      <td>0.908356</td>\n",
       "      <td>0.091644</td>\n",
       "      <td>0.787671</td>\n",
       "      <td>0.212329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.557883</td>\n",
       "      <td>0.670644</td>\n",
       "      <td>0.900249</td>\n",
       "      <td>0.534358</td>\n",
       "      <td>0.900249</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>0.784483</td>\n",
       "      <td>0.215517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.554751</td>\n",
       "      <td>0.669465</td>\n",
       "      <td>0.901809</td>\n",
       "      <td>0.532318</td>\n",
       "      <td>0.901809</td>\n",
       "      <td>0.098191</td>\n",
       "      <td>0.792308</td>\n",
       "      <td>0.207692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553270</td>\n",
       "      <td>0.669205</td>\n",
       "      <td>0.903743</td>\n",
       "      <td>0.531318</td>\n",
       "      <td>0.903743</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.202797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.555452</td>\n",
       "      <td>0.669004</td>\n",
       "      <td>0.898515</td>\n",
       "      <td>0.532887</td>\n",
       "      <td>0.898515</td>\n",
       "      <td>0.101485</td>\n",
       "      <td>0.787611</td>\n",
       "      <td>0.212389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.552922</td>\n",
       "      <td>0.668432</td>\n",
       "      <td>0.901299</td>\n",
       "      <td>0.531190</td>\n",
       "      <td>0.901299</td>\n",
       "      <td>0.098701</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.204545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.549192</td>\n",
       "      <td>0.666870</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.528827</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.195946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.550865</td>\n",
       "      <td>0.666718</td>\n",
       "      <td>0.898477</td>\n",
       "      <td>0.530005</td>\n",
       "      <td>0.898477</td>\n",
       "      <td>0.101523</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.203252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.548402</td>\n",
       "      <td>0.666415</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.528346</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.805369</td>\n",
       "      <td>0.194631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.538609</td>\n",
       "      <td>0.659773</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.522549</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.817518</td>\n",
       "      <td>0.182482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.534307</td>\n",
       "      <td>0.657948</td>\n",
       "      <td>0.895775</td>\n",
       "      <td>0.519912</td>\n",
       "      <td>0.895775</td>\n",
       "      <td>0.104225</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.172840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.534430</td>\n",
       "      <td>0.657154</td>\n",
       "      <td>0.892388</td>\n",
       "      <td>0.520065</td>\n",
       "      <td>0.892388</td>\n",
       "      <td>0.107612</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.517471</td>\n",
       "      <td>0.647053</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.510074</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.849673</td>\n",
       "      <td>0.150327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_unfrozen_epochs  upweight  lambda_loss  Accuracy  F1 score  Precision  \\\n",
       "74  3.0                  25.0      1.0          0.784268  0.812844  0.936957    \n",
       "68  2.0                  25.0      1.0          0.782992  0.812256  0.938865    \n",
       "44  1.0                  25.0      1.0          0.775541  0.806059  0.932900    \n",
       "69  2.0                  25.0      5.0          0.757811  0.795498  0.942094    \n",
       "85  5.0                  25.0      0.5          0.753663  0.793044  0.943946    \n",
       "45  1.0                  25.0      5.0          0.753591  0.792650  0.941964    \n",
       "80  4.0                  25.0      1.0          0.753776  0.791750  0.936123    \n",
       "73  3.0                  25.0      0.5          0.749688  0.790382  0.943820    \n",
       "43  1.0                  25.0      0.5          0.749688  0.790382  0.943820    \n",
       "70  2.0                  25.0      10.0         0.749688  0.790382  0.943820    \n",
       "86  5.0                  25.0      1.0          0.749416  0.788135  0.932166    \n",
       "47  1.0                  25.0      25.0         0.742054  0.785315  0.943567    \n",
       "79  4.0                  25.0      0.5          0.740878  0.783481  0.937639    \n",
       "46  1.0                  25.0      10.0         0.737844  0.782190  0.941441    \n",
       "67  2.0                  25.0      0.5          0.734813  0.780565  0.943311    \n",
       "75  3.0                  25.0      5.0          0.734168  0.779784  0.941309    \n",
       "71  2.0                  25.0      25.0         0.730588  0.777454  0.941176    \n",
       "77  3.0                  25.0      25.0         0.719388  0.769874  0.938776    \n",
       "81  4.0                  25.0      5.0          0.703962  0.759373  0.934240    \n",
       "87  5.0                  25.0      5.0          0.703962  0.759373  0.934240    \n",
       "88  5.0                  25.0      10.0         0.699319  0.757612  0.939815    \n",
       "76  3.0                  25.0      10.0         0.700812  0.757404  0.934091    \n",
       "83  4.0                  25.0      25.0         0.684441  0.748081  0.937063    \n",
       "82  4.0                  25.0      10.0         0.684425  0.746917  0.931350    \n",
       "89  5.0                  25.0      25.0         0.654753  0.727983  0.923963    \n",
       "72  3.0                  25.0      0.1          0.639110  0.727416  0.963068    \n",
       "78  4.0                  25.0      0.1          0.639767  0.727306  0.960784    \n",
       "66  2.0                  25.0      0.1          0.640394  0.726720  0.956284    \n",
       "42  1.0                  25.0      0.1          0.638402  0.726048  0.958333    \n",
       "84  5.0                  25.0      0.1          0.634659  0.724389  0.960227    \n",
       "20  0.0                  25.0      1.0          0.627055  0.722114  0.969136    \n",
       "18  0.0                  25.0      0.1          0.627055  0.722114  0.969136    \n",
       "23  0.0                  25.0      25.0         0.620993  0.718704  0.968354    \n",
       "21  0.0                  25.0      5.0          0.619544  0.717890  0.968153    \n",
       "19  0.0                  25.0      0.5          0.618121  0.717090  0.967949    \n",
       "22  0.0                  25.0      10.0         0.616030  0.715916  0.967638    \n",
       "64  2.0                  10.0      10.0         0.612073  0.704533  0.925000    \n",
       "65  2.0                  10.0      25.0         0.606549  0.701014  0.922500    \n",
       "63  2.0                  10.0      5.0          0.606549  0.701014  0.922500    \n",
       "41  1.0                  10.0      25.0         0.595502  0.694025  0.917500    \n",
       "59  2.0                  5.0       25.0         0.586670  0.689425  0.917526    \n",
       "39  1.0                  10.0      5.0          0.588155  0.689173  0.913151    \n",
       "35  1.0                  5.0       25.0         0.585170  0.687856  0.914141    \n",
       "58  2.0                  5.0       10.0         0.582869  0.686565  0.913706    \n",
       "40  1.0                  10.0      10.0         0.583764  0.686363  0.910891    \n",
       "62  2.0                  10.0      1.0          0.579775  0.684478  0.911616    \n",
       "33  1.0                  5.0       5.0          0.579775  0.684478  0.911616    \n",
       "27  1.0                  1.0       5.0          0.578458  0.684087  0.912821    \n",
       "28  1.0                  1.0       10.0         0.578458  0.684087  0.912821    \n",
       "34  1.0                  5.0       10.0         0.578102  0.682858  0.908416    \n",
       "38  1.0                  10.0      1.0          0.573934  0.679877  0.904878    \n",
       "26  1.0                  1.0       1.0          0.571176  0.679652  0.909794    \n",
       "29  1.0                  1.0       25.0         0.571134  0.679296  0.908397    \n",
       "16  0.0                  10.0      10.0         0.565596  0.678531  0.916905    \n",
       "17  0.0                  10.0      25.0         0.565596  0.678531  0.916905    \n",
       "15  0.0                  10.0      5.0          0.565596  0.678531  0.916905    \n",
       "13  0.0                  10.0      0.5          0.565596  0.678531  0.916905    \n",
       "12  0.0                  10.0      0.1          0.565596  0.678531  0.916905    \n",
       "14  0.0                  10.0      1.0          0.565596  0.678531  0.916905    \n",
       "5   0.0                  1.0       25.0         0.564989  0.677833  0.915254    \n",
       "57  2.0                  5.0       5.0          0.568019  0.677548  0.907692    \n",
       "2   0.0                  1.0       1.0          0.564214  0.677387  0.915014    \n",
       "3   0.0                  1.0       5.0          0.564214  0.677387  0.915014    \n",
       "4   0.0                  1.0       10.0         0.564214  0.677387  0.915014    \n",
       "0   0.0                  1.0       0.1          0.564214  0.677387  0.915014    \n",
       "1   0.0                  1.0       0.5          0.564214  0.677387  0.915014    \n",
       "37  1.0                  10.0      0.5          0.566841  0.675624  0.902200    \n",
       "52  2.0                  1.0       10.0         0.562937  0.675325  0.909091    \n",
       "11  0.0                  5.0       25.0         0.560509  0.674914  0.912429    \n",
       "6   0.0                  5.0       0.1          0.560509  0.674914  0.912429    \n",
       "7   0.0                  5.0       0.5          0.560509  0.674914  0.912429    \n",
       "8   0.0                  5.0       1.0          0.560509  0.674914  0.912429    \n",
       "9   0.0                  5.0       5.0          0.560509  0.674914  0.912429    \n",
       "10  0.0                  5.0       10.0         0.560509  0.674914  0.912429    \n",
       "25  1.0                  1.0       0.5          0.563797  0.674857  0.905371    \n",
       "53  2.0                  1.0       25.0         0.562522  0.674769  0.907652    \n",
       "32  1.0                  5.0       1.0          0.564536  0.674650  0.902985    \n",
       "51  2.0                  1.0       5.0          0.560342  0.673848  0.908356    \n",
       "31  1.0                  5.0       0.5          0.557883  0.670644  0.900249    \n",
       "30  1.0                  5.0       0.1          0.554751  0.669465  0.901809    \n",
       "50  2.0                  1.0       1.0          0.553270  0.669205  0.903743    \n",
       "61  2.0                  10.0      0.5          0.555452  0.669004  0.898515    \n",
       "36  1.0                  10.0      0.1          0.552922  0.668432  0.901299    \n",
       "24  1.0                  1.0       0.1          0.549192  0.666870  0.902439    \n",
       "56  2.0                  5.0       1.0          0.550865  0.666718  0.898477    \n",
       "49  2.0                  1.0       0.5          0.548402  0.666415  0.902174    \n",
       "55  2.0                  5.0       0.5          0.538609  0.659773  0.894737    \n",
       "48  2.0                  1.0       0.1          0.534307  0.657948  0.895775    \n",
       "60  2.0                  10.0      0.1          0.534430  0.657154  0.892388    \n",
       "54  2.0                  5.0       0.1          0.517471  0.647053  0.884615    \n",
       "\n",
       "      Recall   TP_rate   FP_rate   FN_rate   TN_rate  \n",
       "74  0.717767  0.936957  0.063043  0.368421  0.631579  \n",
       "68  0.715737  0.938865  0.061135  0.372881  0.627119  \n",
       "44  0.709582  0.932900  0.067100  0.381818  0.618182  \n",
       "69  0.688381  0.942094  0.057906  0.426471  0.573529  \n",
       "85  0.683739  0.943946  0.056054  0.436620  0.563380  \n",
       "45  0.684196  0.941964  0.058036  0.434783  0.565217  \n",
       "80  0.685958  0.936123  0.063877  0.428571  0.571429  \n",
       "73  0.679856  0.943820  0.056180  0.444444  0.555556  \n",
       "43  0.679856  0.943820  0.056180  0.444444  0.555556  \n",
       "70  0.679856  0.943820  0.056180  0.444444  0.555556  \n",
       "86  0.682656  0.932166  0.067834  0.433333  0.566667  \n",
       "47  0.672523  0.943567  0.056433  0.459459  0.540541  \n",
       "79  0.672856  0.937639  0.062361  0.455882  0.544118  \n",
       "46  0.669020  0.941441  0.058559  0.465753  0.534247  \n",
       "67  0.665712  0.943311  0.056689  0.473684  0.526316  \n",
       "75  0.665574  0.941309  0.058691  0.472973  0.527027  \n",
       "71  0.662252  0.941176  0.058824  0.480000  0.520000  \n",
       "77  0.652482  0.938776  0.061224  0.500000  0.500000  \n",
       "81  0.639647  0.934240  0.065760  0.526316  0.473684  \n",
       "87  0.639647  0.934240  0.065760  0.526316  0.473684  \n",
       "88  0.634585  0.939815  0.060185  0.541176  0.458824  \n",
       "76  0.636927  0.934091  0.065909  0.532468  0.467532  \n",
       "83  0.622532  0.937063  0.062937  0.568182  0.431818  \n",
       "82  0.623456  0.931350  0.068650  0.562500  0.437500  \n",
       "89  0.600592  0.923963  0.076037  0.614458  0.385542  \n",
       "72  0.584416  0.963068  0.036932  0.684848  0.315152  \n",
       "78  0.585118  0.960784  0.039216  0.681250  0.318750  \n",
       "66  0.586037  0.956284  0.043716  0.675497  0.324503  \n",
       "42  0.584399  0.958333  0.041667  0.681529  0.318471  \n",
       "84  0.581555  0.960227  0.039773  0.690909  0.309091  \n",
       "20  0.575441  0.969136  0.030864  0.715026  0.284974  \n",
       "18  0.575441  0.969136  0.030864  0.715026  0.284974  \n",
       "23  0.571394  0.968354  0.031646  0.726368  0.273632  \n",
       "21  0.570436  0.968153  0.031847  0.729064  0.270936  \n",
       "19  0.569497  0.967949  0.032051  0.731707  0.268293  \n",
       "22  0.568124  0.967638  0.032362  0.735577  0.264423  \n",
       "64  0.568932  0.925000  0.075000  0.700855  0.299145  \n",
       "65  0.565291  0.922500  0.077500  0.709402  0.290598  \n",
       "63  0.565291  0.922500  0.077500  0.709402  0.290598  \n",
       "41  0.558091  0.917500  0.082500  0.726496  0.273504  \n",
       "59  0.552157  0.917526  0.082474  0.744186  0.255814  \n",
       "39  0.553427  0.913151  0.086849  0.736842  0.263158  \n",
       "35  0.551371  0.914141  0.085859  0.743802  0.256198  \n",
       "58  0.549871  0.913706  0.086294  0.747967  0.252033  \n",
       "40  0.550636  0.910891  0.089109  0.743363  0.256637  \n",
       "62  0.547951  0.911616  0.088384  0.752066  0.247934  \n",
       "33  0.547951  0.911616  0.088384  0.752066  0.247934  \n",
       "27  0.547016  0.912821  0.087179  0.755906  0.244094  \n",
       "28  0.547016  0.912821  0.087179  0.755906  0.244094  \n",
       "34  0.547031  0.908416  0.091584  0.752212  0.247788  \n",
       "38  0.544488  0.904878  0.095122  0.757009  0.242991  \n",
       "26  0.542436  0.909794  0.090206  0.767442  0.232558  \n",
       "29  0.542480  0.908397  0.091603  0.766129  0.233871  \n",
       "16  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "17  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "15  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "13  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "12  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "14  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "5   0.538217  0.915254  0.084746  0.785276  0.214724  \n",
       "57  0.540503  0.907692  0.092308  0.771654  0.228346  \n",
       "2   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "3   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "4   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "0   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "1   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "37  0.540007  0.902200  0.097800  0.768519  0.231481  \n",
       "52  0.537190  0.909091  0.090909  0.783217  0.216783  \n",
       "11  0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "6   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "7   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "8   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "9   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "10  0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "25  0.537903  0.905371  0.094629  0.777778  0.222222  \n",
       "53  0.536989  0.907652  0.092348  0.782609  0.217391  \n",
       "32  0.538485  0.902985  0.097015  0.773913  0.226087  \n",
       "51  0.535579  0.908356  0.091644  0.787671  0.212329  \n",
       "31  0.534358  0.900249  0.099751  0.784483  0.215517  \n",
       "30  0.532318  0.901809  0.098191  0.792308  0.207692  \n",
       "50  0.531318  0.903743  0.096257  0.797203  0.202797  \n",
       "61  0.532887  0.898515  0.101485  0.787611  0.212389  \n",
       "36  0.531190  0.901299  0.098701  0.795455  0.204545  \n",
       "24  0.528827  0.902439  0.097561  0.804054  0.195946  \n",
       "56  0.530005  0.898477  0.101523  0.796748  0.203252  \n",
       "49  0.528346  0.902174  0.097826  0.805369  0.194631  \n",
       "55  0.522549  0.894737  0.105263  0.817518  0.182482  \n",
       "48  0.519912  0.895775  0.104225  0.827160  0.172840  \n",
       "60  0.520065  0.892388  0.107612  0.823529  0.176471  \n",
       "54  0.510074  0.884615  0.115385  0.849673  0.150327  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(['F1 score'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_unfrozen_epochs</th>\n",
       "      <th>upweight</th>\n",
       "      <th>lambda_loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>TP_rate</th>\n",
       "      <th>FP_rate</th>\n",
       "      <th>FN_rate</th>\n",
       "      <th>TN_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.627055</td>\n",
       "      <td>0.722114</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.575441</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.715026</td>\n",
       "      <td>0.284974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.627055</td>\n",
       "      <td>0.722114</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.575441</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.715026</td>\n",
       "      <td>0.284974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.620993</td>\n",
       "      <td>0.718704</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>0.571394</td>\n",
       "      <td>0.968354</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.726368</td>\n",
       "      <td>0.273632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.619544</td>\n",
       "      <td>0.717890</td>\n",
       "      <td>0.968153</td>\n",
       "      <td>0.570436</td>\n",
       "      <td>0.968153</td>\n",
       "      <td>0.031847</td>\n",
       "      <td>0.729064</td>\n",
       "      <td>0.270936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.618121</td>\n",
       "      <td>0.717090</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>0.569497</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.268293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.616030</td>\n",
       "      <td>0.715916</td>\n",
       "      <td>0.967638</td>\n",
       "      <td>0.568124</td>\n",
       "      <td>0.967638</td>\n",
       "      <td>0.032362</td>\n",
       "      <td>0.735577</td>\n",
       "      <td>0.264423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.916905</td>\n",
       "      <td>0.083095</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.564989</td>\n",
       "      <td>0.677833</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.538217</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.785276</td>\n",
       "      <td>0.214724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.564214</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.537738</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.535513</td>\n",
       "      <td>0.912429</td>\n",
       "      <td>0.087571</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.208589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_unfrozen_epochs  upweight  lambda_loss  Accuracy  F1 score  Precision  \\\n",
       "20  0.0                  25.0      1.0          0.627055  0.722114  0.969136    \n",
       "18  0.0                  25.0      0.1          0.627055  0.722114  0.969136    \n",
       "23  0.0                  25.0      25.0         0.620993  0.718704  0.968354    \n",
       "21  0.0                  25.0      5.0          0.619544  0.717890  0.968153    \n",
       "19  0.0                  25.0      0.5          0.618121  0.717090  0.967949    \n",
       "22  0.0                  25.0      10.0         0.616030  0.715916  0.967638    \n",
       "13  0.0                  10.0      0.5          0.565596  0.678531  0.916905    \n",
       "17  0.0                  10.0      25.0         0.565596  0.678531  0.916905    \n",
       "16  0.0                  10.0      10.0         0.565596  0.678531  0.916905    \n",
       "15  0.0                  10.0      5.0          0.565596  0.678531  0.916905    \n",
       "14  0.0                  10.0      1.0          0.565596  0.678531  0.916905    \n",
       "12  0.0                  10.0      0.1          0.565596  0.678531  0.916905    \n",
       "5   0.0                  1.0       25.0         0.564989  0.677833  0.915254    \n",
       "1   0.0                  1.0       0.5          0.564214  0.677387  0.915014    \n",
       "4   0.0                  1.0       10.0         0.564214  0.677387  0.915014    \n",
       "3   0.0                  1.0       5.0          0.564214  0.677387  0.915014    \n",
       "2   0.0                  1.0       1.0          0.564214  0.677387  0.915014    \n",
       "0   0.0                  1.0       0.1          0.564214  0.677387  0.915014    \n",
       "11  0.0                  5.0       25.0         0.560509  0.674914  0.912429    \n",
       "10  0.0                  5.0       10.0         0.560509  0.674914  0.912429    \n",
       "9   0.0                  5.0       5.0          0.560509  0.674914  0.912429    \n",
       "8   0.0                  5.0       1.0          0.560509  0.674914  0.912429    \n",
       "7   0.0                  5.0       0.5          0.560509  0.674914  0.912429    \n",
       "6   0.0                  5.0       0.1          0.560509  0.674914  0.912429    \n",
       "\n",
       "      Recall   TP_rate   FP_rate   FN_rate   TN_rate  \n",
       "20  0.575441  0.969136  0.030864  0.715026  0.284974  \n",
       "18  0.575441  0.969136  0.030864  0.715026  0.284974  \n",
       "23  0.571394  0.968354  0.031646  0.726368  0.273632  \n",
       "21  0.570436  0.968153  0.031847  0.729064  0.270936  \n",
       "19  0.569497  0.967949  0.032051  0.731707  0.268293  \n",
       "22  0.568124  0.967638  0.032362  0.735577  0.264423  \n",
       "13  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "17  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "16  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "15  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "14  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "12  0.538526  0.916905  0.083095  0.785714  0.214286  \n",
       "5   0.538217  0.915254  0.084746  0.785276  0.214724  \n",
       "1   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "4   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "3   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "2   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "0   0.537738  0.915014  0.084986  0.786585  0.213415  \n",
       "11  0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "10  0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "9   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "8   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "7   0.535513  0.912429  0.087571  0.791411  0.208589  \n",
       "6   0.535513  0.912429  0.087571  0.791411  0.208589  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df.num_unfrozen_epochs==0].sort_values(['F1 score'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_unfrozen_epochs</th>\n",
       "      <th>upweight</th>\n",
       "      <th>lambda_loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>TP_rate</th>\n",
       "      <th>FP_rate</th>\n",
       "      <th>FN_rate</th>\n",
       "      <th>TN_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784268</td>\n",
       "      <td>0.812844</td>\n",
       "      <td>0.936957</td>\n",
       "      <td>0.717767</td>\n",
       "      <td>0.936957</td>\n",
       "      <td>0.063043</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.749688</td>\n",
       "      <td>0.790382</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.734168</td>\n",
       "      <td>0.779784</td>\n",
       "      <td>0.941309</td>\n",
       "      <td>0.665574</td>\n",
       "      <td>0.941309</td>\n",
       "      <td>0.058691</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.527027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.719388</td>\n",
       "      <td>0.769874</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.652482</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.700812</td>\n",
       "      <td>0.757404</td>\n",
       "      <td>0.934091</td>\n",
       "      <td>0.636927</td>\n",
       "      <td>0.934091</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.467532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.639110</td>\n",
       "      <td>0.727416</td>\n",
       "      <td>0.963068</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.963068</td>\n",
       "      <td>0.036932</td>\n",
       "      <td>0.684848</td>\n",
       "      <td>0.315152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_unfrozen_epochs  upweight  lambda_loss  Accuracy  F1 score  Precision  \\\n",
       "74  3.0                  25.0      1.0          0.784268  0.812844  0.936957    \n",
       "73  3.0                  25.0      0.5          0.749688  0.790382  0.943820    \n",
       "75  3.0                  25.0      5.0          0.734168  0.779784  0.941309    \n",
       "77  3.0                  25.0      25.0         0.719388  0.769874  0.938776    \n",
       "76  3.0                  25.0      10.0         0.700812  0.757404  0.934091    \n",
       "72  3.0                  25.0      0.1          0.639110  0.727416  0.963068    \n",
       "\n",
       "      Recall   TP_rate   FP_rate   FN_rate   TN_rate  \n",
       "74  0.717767  0.936957  0.063043  0.368421  0.631579  \n",
       "73  0.679856  0.943820  0.056180  0.444444  0.555556  \n",
       "75  0.665574  0.941309  0.058691  0.472973  0.527027  \n",
       "77  0.652482  0.938776  0.061224  0.500000  0.500000  \n",
       "76  0.636927  0.934091  0.065909  0.532468  0.467532  \n",
       "72  0.584416  0.963068  0.036932  0.684848  0.315152  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[(results_df.num_unfrozen_epochs==3) & (results_df.upweight == 25)].sort_values(['F1 score'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
