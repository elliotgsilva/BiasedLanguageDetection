{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# ## KAGGLE ONLY\nfrom shutil import copyfile\ncopyfile(src=\"../input/inputs/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\ncopyfile(src=\"../input/eval-script6/evaluation.py\", dst=\"../working/evaluation.py\")\ncopyfile(src=\"../input/inputs/train_dataloader.p\", dst=\"../working/train_dataloader.p\")\ncopyfile(src=\"../input/inputs/centroids_dataloader.p\", dst=\"../working/ground_truth_dataloader.p\")\ncopyfile(src=\"../input/dictionary/dictionary_2.p\", dst=\"../working/dictionary.p\")\n\ncopyfile(src=\"../input/bert-inputs/bert_train_unlabeled_dataloader.p\", dst=\"../working/train_unlabeled_dataloader.p\")\ncopyfile(src=\"../input/bert-inputs/bert_train_labeled_dataloader.p\", dst=\"../working/train_labeled_dataloader.p\")\ncopyfile(src=\"../input/bert-inputs/bert_val_dataloader.p\", dst=\"../working/val_dataloader.p\")\n\n!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"zno22FtJPX9z","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n#from datasets import get_mnist_dataset, get_data_loader\n#from utils import *\n#from models import *\n\nimport pickle as pkl\nimport os\nimport datetime as dt\nimport pandas as pd\nimport random\n\nfrom generate_dataloaders import *\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport evaluation\nimport importlib\nimportlib.reload(evaluation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import bert stuff\n\nfrom transformers import (\n    BertModel,\n    BertTokenizer\n)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"oaJEVd0wPX94"},"cell_type":"markdown","source":"## Get Dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1029\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\nnp.random.seed(seed)  # Numpy module.\nrandom.seed(seed)  # Python random module.\ntorch.manual_seed(seed)\ntorch.backends.cudnn.enabled = False \ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\ndef _init_fn(worker_id):\n    np.random.seed(int(seed))","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"6nLzh007PX98","trusted":true},"cell_type":"code","source":"path = os.getcwd()\ndata_dir = path + '/'\n# data_dir = path +'/data/' #Uncomment for local system","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Verify filenames are consistent*"},{"metadata":{"colab":{},"colab_type":"code","id":"yq-jDGFIPX99","trusted":true},"cell_type":"code","source":"train_loader_labelled = pkl.load(open(data_dir + 'train_labeled_dataloader.p','rb'))\ntrain_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabeled_dataloader.p','rb'))\nval_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%conda install pytorch torchvision -c pytorch\n## if torch.__version__ is not 1.3.1, run this cell then restart kernel","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Lzz8lwNQPX-B","outputId":"690cb77f-2525-4c5a-ea14-a162716e34d3","trusted":true},"cell_type":"code","source":"print(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"Cvt6N9QCPX-X"},"cell_type":"markdown","source":"## Defining a Bert Class"},{"metadata":{"colab_type":"text","id":"puweJhdxPX-Y"},"cell_type":"markdown","source":"NOTE: Data loader is defined as:\n- tuple: (tokens, flagged_index, problematic)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTSequenceClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)\n        self.X = nn.Linear(bert.config.hidden_size, 10)\n        self.W = nn.Linear(10, num_classes)\n        self.num_classes = num_classes\n        \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        h, _, attn = self.bert(input_ids=input_ids, \n                               attention_mask=attention_mask, \n                               token_type_ids=token_type_ids)\n        h_cls = h[:, 0]\n        X_output = F.relu(self.X(h_cls))\n        \n        logits = self.W(X_output)\n        \n        return logits, attn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's print what bert's architecture looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert.parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First performing fully supervised learning using the labelled set to train new vector representations"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\nif num_gpus > 0:\n    current_device = 'cuda'\nelse:\n    current_device = 'cpu'\n\nmodel = BERTSequenceClassifier(num_classes = 2).to(current_device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(reduction='sum',ignore_index=-1).to(current_device)\noptimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=2e-05, eps=1e-08, amsgrad = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Supervised model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_supervised_model(model, criterion, optimizer, train_loader_labelled, valid_loader, num_epochs=10, path_to_save=None, print_every = 1000):\n\n    train_losses=[]\n    val_losses=[]\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 0:\n        current_device = 'cuda'\n    else:\n        current_device = 'cpu'\n    \n    for epoch in range(num_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.train()\n        total_epoch_loss = 0\n        \n        for i,(input_ids_labelled, attention_mask_labelled, token_type_ids_labelled, labels) in tqdm(enumerate(train_loader_labelled)):\n            \n            input_ids_labelled = input_ids_labelled.to(current_device)\n            attention_mask_labelled = attention_mask_labelled.to(current_device)\n            token_type_ids_labelled = token_type_ids_labelled.to(current_device)\n            labels = labels.to(current_device)\n\n            # forward pass and compute loss\n            logits, attn = model(input_ids_labelled, attention_mask_labelled, token_type_ids_labelled)\n            \n            loss = criterion(logits, labels)\n        \n            # run update step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            #Add loss to the epoch loss\n            total_epoch_loss += loss.data\n\n            if i % print_every == 0:\n                losses = loss/len(input_ids_labelled)\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= len(train_loader_labelled.dataset)\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(valid_loader):\n            model.eval()\n            \n            input_ids = input_ids.to(current_device)\n            attention_mask = attention_mask.to(current_device)\n            token_type_ids = token_type_ids.to(current_device)\n            labels = labels.to(current_device)\n            \n            # forward pass and compute loss\n            logits,attn = model(input_ids, attention_mask, token_type_ids)\n            \n            loss = criterion(logits, labels)\n            \n            #Add loss to the validation loss\n            total_validation_loss += loss.data\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"num_classes\":model.num_classes}\n            torch.save(model.state_dict(), path_to_save+'model_dict_labelled.pt')\n            torch.save(train_losses, path_to_save+'train_losses_labelled')\n            torch.save(val_losses, path_to_save+'val_losses_labelled')\n            torch.save(opts, path_to_save+'opts_labelled')\n        \n    return model, train_losses, val_losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = os.getcwd()\n# model_folder = 'bert_model/'\n# model_dir = path + '/models/' + model_folder\nmodel_dir = path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_loader_labelled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model, train_losses, val_losses = train_supervised_model(model, criterion, optimizer, train_loader_labelled, val_loader, num_epochs=4, path_to_save=model_dir)\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"SGsqcnEtPX-a"},"cell_type":"markdown","source":"### Clustering Stuff"},{"metadata":{"colab":{},"colab_type":"code","id":"MrgIYm8JPX-b","trusted":true},"cell_type":"code","source":"class KMeansCriterion(nn.Module):\n    \n    def __init__(self, lmbda):\n        super().__init__()\n        self.lmbda = lmbda\n    \n    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n        if labelled:\n            num_reviews = len(cluster_assignments)\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n            loss = self.lmbda * cluster_distances.sum()\n        else:\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances, cluster_assignments = distances.min(1)\n            loss = self.lmbda * cluster_distances.sum()\n        return loss, cluster_assignments","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"-TJohK2aPX-d","trusted":true},"cell_type":"code","source":"def centroid_init(k, d, dataloader, model, current_device):\n    ## Here we ideally don't want to do randomized/zero initialization\n    centroid_sums = torch.zeros(k, d).to(current_device)\n    centroid_counts = torch.zeros(k).to(current_device)\n    for (input_ids, attention_mask, token_type_ids, labels) in dataloader:\n        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n        cluster_assignments = labels.to(current_device)\n        \n        model.eval()\n        sentence_embed = model(input_ids.to(current_device), attention_mask.to(current_device), token_type_ids.to(current_device))\n    \n        update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n                        cluster_assignments.detach(), sentence_embed[0].to(current_device).detach())\n    \n    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n    return centroid_means.clone()\n\ndef update_clusters(centroid_sums, centroid_counts,\n                    cluster_assignments, embeddings):\n    k = centroid_sums.size(0)\n\n    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n    centroid_counts.add_(bin_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataloader stuff"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n    try:\n        input_ids, attention_mask, token_type_ids, labels = next(train_loader_labelled_iter)\n    except StopIteration:\n        train_loader_labelled_iter = iter(train_loader_labelled)\n        input_ids, attention_mask, token_type_ids, labels = next(train_loader_labelled_iter)\n\n    return input_ids, attention_mask, token_type_ids, labels, train_loader_labelled_iter\n\n\ndef loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n    try:\n        input_ids, attention_mask, token_type_ids, labels = next(train_loader_unlabelled_iter)\n    except StopIteration:\n        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n        input_ids, attention_mask, token_type_ids, labels = next(train_loader_unlabelled_iter)\n\n    return input_ids, attention_mask, token_type_ids, labels, train_loader_unlabelled_iter","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"u3wynM7fPX-h"},"cell_type":"markdown","source":"### Training Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_clusters(model, centroids, criterion, optimizer, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=10, num_batches = 1000, path_to_save=None, print_every = 1000):\n\n    train_loader_labelled_iter = iter(train_loader_labelled)\n    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n\n    train_losses=[]\n    val_losses=[]\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 0:\n        current_device = 'cuda'\n    else:\n        current_device = 'cpu'\n    \n    for epoch in range(num_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.eval() # we're only clustering, not training model\n        k, d = centroids.size()\n        centroid_sums = torch.zeros_like(centroids).to(current_device)\n        centroid_counts = torch.zeros(k).to(current_device)\n        total_epoch_loss = 0\n        \n        for i in tqdm(range(int(num_batches))):\n            input_ids_labelled, attention_mask_labelled, token_type_ids_labelled, labels, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n            input_ids_unlabelled, attention_mask_unlabelled, token_type_ids_unlabelled, _, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n\n            input_ids_labelled = input_ids_labelled.to(current_device)\n            attention_mask_labelled = attention_mask_labelled.to(current_device)\n            token_type_ids_labelled = token_type_ids_labelled.to(current_device)\n            labels = labels.to(current_device)\n            \n            input_ids_unlabelled = input_ids_unlabelled.to(current_device)\n            attention_mask_unlabelled = attention_mask_unlabelled.to(current_device)\n            token_type_ids_unlabelled = token_type_ids_unlabelled.to(current_device)\n\n            # forward pass and compute loss\n            sentence_embed_labelled,attn = model(input_ids_labelled, attention_mask_labelled, token_type_ids_labelled)\n            sentence_embed_unlabelled,attn = model(input_ids_unlabelled, attention_mask_unlabelled, token_type_ids_unlabelled)\n            \n            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n    \n            total_batch_loss = cluster_loss_labelled.data + cluster_loss_unlabelled.data\n            \n#             #Add loss to the epoch loss\n            total_epoch_loss += total_batch_loss.data\n\n#             # store centroid sums and counts in memory for later centering\n            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n                            cluster_assignments_labelled.detach(), sentence_embed_labelled.detach())\n    \n            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n                            cluster_assignments_unlabelled.detach(), sentence_embed_unlabelled.detach())\n\n            if i % print_every == 0:\n                losses = total_batch_loss/(len(input_ids_labelled)+ len(input_ids_unlabelled))\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # update centroids based on assignments from autoencoders\n        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(valid_loader):\n            model.eval()\n            input_ids = input_ids.to(current_device)\n            attention_mask = attention_mask.to(current_device)\n            token_type_ids = token_type_ids.to(current_device)\n            labels = labels.to(current_device)\n            \n            # forward pass and compute loss\n            sentence_embed,attn = model(input_ids, attention_mask, token_type_ids)\n            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n            \n            #Add loss to the validation loss\n            total_validation_loss += cluster_loss.data\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"num_classes\":model.num_classes}\n            torch.save(model.state_dict(), path_to_save+'model_dict_unlabelled.pt')\n            torch.save(centroids, path_to_save+'centroids_unlabelled')\n            torch.save(train_losses, path_to_save+'train_losses_unlabelled')\n            torch.save(val_losses, path_to_save+'val_losses_unlabelled')\n            torch.save(opts, path_to_save+'opts_unlabelled')\n            \n        \n    return model, centroids, train_losses, val_losses","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"0pBet75ZPX-m","trusted":true},"cell_type":"code","source":"unsupervised_model = model\nunsupervised_model.W = nn.Identity()","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"mTFO2vp-PX-o","trusted":true},"cell_type":"code","source":"# centroids = centroid_init(2, unsupervised_model.bert.config.hidden_size, train_loader_labelled, unsupervised_model, current_device)\ncentroids = centroid_init(2, 10, train_loader_labelled, unsupervised_model, current_device)\ncriterion = KMeansCriterion(1).to(current_device)\noptimizer = optim.Adam([p for p in unsupervised_model.parameters() if p.requires_grad], lr=2e-05, eps=1e-08, amsgrad = True)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Xya2NiqcPX-q","outputId":"59b3072e-c567-4e18-a242-ba8298f08e58","scrolled":true,"trusted":true},"cell_type":"code","source":"centroids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = os.getcwd()\n# model_folder = 'bert_model/'\n# model_dir = path + '/models/' + model_folder\nmodel_dir = path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_batches = int(len(train_loader_unlabelled.dataset)/train_loader_unlabelled.batch_size)+1\nnum_batches","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"colab_type":"code","executionInfo":{"elapsed":8813,"status":"error","timestamp":1573355511003,"user":{"displayName":"Eileen Cho","photoUrl":"","userId":"03381570147993013394"},"user_tz":300},"id":"rgwMd27mPX-u","outputId":"063ebc41-be3c-4474-d92c-7bd0680bb366","scrolled":true,"trusted":true},"cell_type":"code","source":"unsupervised_model, bert_centroids, bert_train_losses, bert_val_losses = train_clusters(unsupervised_model, centroids, criterion, optimizer, train_loader_labelled,train_loader_unlabelled, val_loader, num_epochs=2, num_batches=num_batches, path_to_save=model_dir)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(bert_centroids, model_dir+'centroids_unlabelled')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Only needed for Kaggle\n\n# from IPython.display import FileLink, FileLinks \n# FileLinks('.') #lists all downloadable files on server","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Model"},{"metadata":{},"cell_type":"markdown","source":"## Supervised Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\nif num_gpus > 0:\n    current_device = 'cuda'\nelse:\n    current_device = 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## This cell will change for each model\nmodel_folder = 'bert_model/'\n\ncriterion = nn.CrossEntropyLoss(reduction='sum')\ncriterion = criterion.to(current_device)\n\npath = os.getcwd()\n# model_dir = path + '/models/' + model_folder\nmodel_dir = path\n\nopts = torch.load(model_dir+'opts_labelled')\nmodel = BERTSequenceClassifier(opts['num_classes']) #change here depending on model\nmodel.load_state_dict(torch.load(model_dir+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\nmodel = model.to(current_device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_centroids = torch.tensor([])\n\nTP_cluster, FP_cluster=evaluation.bert(model, empty_centroids, val_loader, criterion, data_dir, current_device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP_cluster[TP_cluster[\"original\"] == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unsupervised Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"## This cell will change for each model\nmodel_folder = 'bert_model/'\n\ncriterion = KMeansCriterion(1)\ncriterion = criterion.to(current_device)\n\npath = os.getcwd()\n# model_dir = path + '/models/' + model_folder\nmodel_dir = path\n\nopts = torch.load(model_dir+'opts_labelled')\nmodel = BERTSequenceClassifier(opts['num_classes']) #change here depending on model\nmodel.load_state_dict(torch.load(model_dir+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\nmodel = model.to(current_device)\nmodel.W = nn.Identity()\ncentroids = torch.load(model_dir+'centroids_unlabelled',map_location=lambda storage, loc: storage)\ncentroids = centroids.to(current_device)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP_cluster, FP_cluster=evaluation.bert(model, centroids, val_loader, criterion, data_dir, current_device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP_clusterer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#FP_cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#FP_cluster[FP_cluster.original == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"model_tuning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}