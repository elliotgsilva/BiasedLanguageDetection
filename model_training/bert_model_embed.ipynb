{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERT on our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to train BERT Base and BERT Large models on our training set. It makes use of the BERT dataloaders generated by the bert_dataloders notebook present in the data_prep folder.  It also uses our custom functions found in generate_dataloaders.py and evaluation.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we were unable to run BERT on our local system due to lack of computational power and hence used Kaggle to run it. Thus in order to run this notebook, please upload this notebook on Kaggle, along with the following data files in a custom folder named \"data\":\n",
    "- bert_train_labeled_dataloader.p\n",
    "- bert_train_unlabeled_dataloader.p\n",
    "- bert_val_dataloader.p\n",
    "- bert_test_dataloader.p\n",
    "\n",
    "The following scripts also need to be uploaded in another custom folder named \"scripts\":\n",
    "- generate_dataloaders.py\n",
    "- evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell sets up everything in Kaggle \n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(src=\"../input/scripts/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\n",
    "copyfile(src=\"../input/scripts/evaluation.py\", dst=\"../working/evaluation.py\")\n",
    "\n",
    "copyfile(src=\"../input/data/bert_train_unlabeled_dataloader.p\", dst=\"../working/train_unlabeled_dataloader.p\")\n",
    "copyfile(src=\"../input/data/bert_train_labeled_dataloader.p\", dst=\"../working/train_labeled_dataloader.p\")\n",
    "copyfile(src=\"../input/data/bert_val_dataloader.p\", dst=\"../working/val_dataloader.p\")\n",
    "copyfile(src=\"../input/data/bert_test_dataloader.p\", dst=\"../working/test_dataloader.p\")\n",
    "\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zno22FtJPX9z"
   },
   "outputs": [],
   "source": [
    "## Import all relevant libraries and scripts\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from generate_dataloaders import *\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import evaluation\n",
    "import importlib\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import BERT models and required functions\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaJEVd0wPX94"
   },
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1029\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nLzh007PX98"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Verify filenames are consistent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yq-jDGFIPX99"
   },
   "outputs": [],
   "source": [
    "train_loader_labelled = pkl.load(open(data_dir + 'train_labeled_dataloader.p','rb'))\n",
    "train_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabeled_dataloader.p','rb'))\n",
    "val_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))\n",
    "test_loader = pkl.load(open(data_dir + 'test_dataloader.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install pytorch torchvision -c pytorch\n",
    "## if torch.__version__ is not 1.3.1, run this cell then restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzz8lwNQPX-B",
    "outputId": "690cb77f-2525-4c5a-ea14-a162716e34d3"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvt6N9QCPX-X"
   },
   "source": [
    "## Defining our BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our BERT model consists of the pretrained BERT model (base/larged) followed by two linear layers. The first linear layer reduces the vector space to a smaller dimension space. The second linear layer is only used during the supervised learning phase to generate probabilities of the review being in each class. In the unsupervised phase, we replace the second linear layer with an identity layer and perform clustering on the vector representations generated by the first linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the cell below currently uses bert-base-cased model. In order to use the BERT large model, we should comment the line where we import BERT base model and uncomment the line where we use BERT large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSequenceClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Define which BERT model we want to use. Please uncomment one of the following lines and comment the other\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True) #Uncomment this line to use BERT base\n",
    "#         self.bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True) #Uncomment this line to use BERT large  \n",
    "    \n",
    "        self.X = nn.Linear(bert.config.hidden_size, 50)\n",
    "        self.W = nn.Linear(50, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        h, _, attn = self.bert(input_ids=input_ids, \n",
    "                               attention_mask=attention_mask, \n",
    "                               token_type_ids=token_type_ids)\n",
    "        h_cls = h[:, 0]\n",
    "        X_output = F.relu(self.X(h_cls))\n",
    "        \n",
    "        logits = self.W(X_output)\n",
    "        \n",
    "        return logits, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform Fully-Supervised Learning with Labelled Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "model = BERTSequenceClassifier(num_classes = 2).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum',ignore_index=-1).to(current_device)\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=2e-05, eps=1e-08, amsgrad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print what BERT's architecture looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method is used to perform fully supervised learning with our labelled train dataset. This model is used to train vector representations for each review and then we perform classification on it to ensure that we get meaningful representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, criterion, optimizer, train_loader_labelled, valid_loader, num_epochs=10, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i,(input_ids_labelled, attention_mask_labelled, token_type_ids_labelled, labels) in tqdm(enumerate(train_loader_labelled)):\n",
    "            \n",
    "            input_ids_labelled = input_ids_labelled.to(current_device)\n",
    "            attention_mask_labelled = attention_mask_labelled.to(current_device)\n",
    "            token_type_ids_labelled = token_type_ids_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            logits, attn = model(input_ids_labelled, attention_mask_labelled, token_type_ids_labelled)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add loss to the epoch loss\n",
    "            total_epoch_loss += loss.data\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = loss/len(input_ids_labelled)\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= len(train_loader_labelled.dataset)\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            \n",
    "            input_ids = input_ids.to(current_device)\n",
    "            attention_mask = attention_mask.to(current_device)\n",
    "            token_type_ids = token_type_ids.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            logits,attn = model(input_ids, attention_mask, token_type_ids)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"num_classes\":model.num_classes}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict_labelled.pt')\n",
    "            torch.save(train_losses, path_to_save+'train_losses_labelled')\n",
    "            torch.save(val_losses, path_to_save+'val_losses_labelled')\n",
    "            torch.save(opts, path_to_save+'opts_labelled')\n",
    "        \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "model_dir = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is used to train our supervised model. We observed that the BERT base model overfits after around 3 epochs. And the BERT lage model overfits after just 1 epoch on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_losses, val_losses = train_supervised_model(model, criterion, optimizer, train_loader_labelled, val_loader, num_epochs=3, path_to_save=model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform Unsupervised Learning (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, in this phase we replace the last layer with an identity layer and perform clustering on the vector represenations generated after the first linear layer. We use both the labelled and the unlabelled datasets for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGsqcnEtPX-a"
   },
   "source": [
    "### Define important functions that will be used during clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeansCriterion method is used to calculate the clustering loss. The centroid_init method initializes the centroids and the update_clusters method is used to store the sum of distances of all points in a cluster from the cluster center and is later used to update the new cluster center location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrgIYm8JPX-b"
   },
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self, lmbda):\n",
    "        super().__init__()\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n",
    "        if labelled:\n",
    "            num_reviews = len(cluster_assignments)\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n",
    "            loss = self.lmbda * cluster_distances.sum()\n",
    "        else:\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances, cluster_assignments = distances.min(1)\n",
    "            loss = self.lmbda * cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TJohK2aPX-d"
   },
   "outputs": [],
   "source": [
    "def centroid_init(k, d, dataloader, model, current_device):\n",
    "    \n",
    "    centroid_sums = torch.zeros(k, d).to(current_device)\n",
    "    centroid_counts = torch.zeros(k).to(current_device)\n",
    "    for (input_ids, attention_mask, token_type_ids, labels) in dataloader:\n",
    "        cluster_assignments = labels.to(current_device)\n",
    "        \n",
    "        model.eval()\n",
    "        sentence_embed = model(input_ids.to(current_device), attention_mask.to(current_device), token_type_ids.to(current_device))\n",
    "    \n",
    "        update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                        cluster_assignments.detach(), sentence_embed[0].to(current_device).detach())\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n",
    "    centroid_counts.add_(bin_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n",
    "    try:\n",
    "        input_ids, attention_mask, token_type_ids, labels = next(train_loader_labelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "        input_ids, attention_mask, token_type_ids, labels = next(train_loader_labelled_iter)\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels, train_loader_labelled_iter\n",
    "\n",
    "\n",
    "def loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n",
    "    try:\n",
    "        input_ids, attention_mask, token_type_ids, labels = next(train_loader_unlabelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "        input_ids, attention_mask, token_type_ids, labels = next(train_loader_unlabelled_iter)\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels, train_loader_unlabelled_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3wynM7fPX-h"
   },
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clusters(model, centroids, criterion, optimizer, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=10, num_batches = 1000, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.eval() # we're only clustering, not training model\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i in tqdm(range(int(num_batches))):\n",
    "            input_ids_labelled, attention_mask_labelled, token_type_ids_labelled, labels, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n",
    "            input_ids_unlabelled, attention_mask_unlabelled, token_type_ids_unlabelled, _, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n",
    "\n",
    "            input_ids_labelled = input_ids_labelled.to(current_device)\n",
    "            attention_mask_labelled = attention_mask_labelled.to(current_device)\n",
    "            token_type_ids_labelled = token_type_ids_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            \n",
    "            input_ids_unlabelled = input_ids_unlabelled.to(current_device)\n",
    "            attention_mask_unlabelled = attention_mask_unlabelled.to(current_device)\n",
    "            token_type_ids_unlabelled = token_type_ids_unlabelled.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed_labelled,attn = model(input_ids_labelled, attention_mask_labelled, token_type_ids_labelled)\n",
    "            sentence_embed_unlabelled,attn = model(input_ids_unlabelled, attention_mask_unlabelled, token_type_ids_unlabelled)\n",
    "            \n",
    "            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n",
    "            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n",
    "    \n",
    "            total_batch_loss = cluster_loss_labelled.data + cluster_loss_unlabelled.data\n",
    "            \n",
    "#             #Add loss to the epoch loss\n",
    "            total_epoch_loss += total_batch_loss.data\n",
    "\n",
    "#             # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_labelled.detach(), sentence_embed_labelled.detach())\n",
    "    \n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_unlabelled.detach(), sentence_embed_unlabelled.detach())\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = total_batch_loss/(len(input_ids_labelled)+ len(input_ids_unlabelled))\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            input_ids = input_ids.to(current_device)\n",
    "            attention_mask = attention_mask.to(current_device)\n",
    "            token_type_ids = token_type_ids.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed,attn = model(input_ids, attention_mask, token_type_ids)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"num_classes\":model.num_classes}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict_unlabelled.pt')\n",
    "            torch.save(centroids, path_to_save+'centroids_unlabelled')\n",
    "            torch.save(train_losses, path_to_save+'train_losses_unlabelled')\n",
    "            torch.save(val_losses, path_to_save+'val_losses_unlabelled')\n",
    "            torch.save(opts, path_to_save+'opts_unlabelled')\n",
    "            \n",
    "        \n",
    "    return model, centroids, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pBet75ZPX-m"
   },
   "outputs": [],
   "source": [
    "unsupervised_model = model\n",
    "unsupervised_model.W = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTFO2vp-PX-o"
   },
   "outputs": [],
   "source": [
    "# centroids = centroid_init(2, unsupervised_model.bert.config.hidden_size, train_loader_labelled, unsupervised_model, current_device)\n",
    "centroids = centroid_init(2, 10, train_loader_labelled, unsupervised_model, current_device)\n",
    "criterion = KMeansCriterion(1).to(current_device)\n",
    "optimizer = optim.Adam([p for p in unsupervised_model.parameters() if p.requires_grad], lr=2e-05, eps=1e-08, amsgrad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xya2NiqcPX-q",
    "outputId": "59b3072e-c567-4e18-a242-ba8298f08e58",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "model_dir = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(len(train_loader_unlabelled.dataset)/train_loader_unlabelled.batch_size)+1\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8813,
     "status": "error",
     "timestamp": 1573355511003,
     "user": {
      "displayName": "Eileen Cho",
      "photoUrl": "",
      "userId": "03381570147993013394"
     },
     "user_tz": 300
    },
    "id": "rgwMd27mPX-u",
    "outputId": "063ebc41-be3c-4474-d92c-7bd0680bb366",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unsupervised_model, bert_centroids, bert_train_losses, bert_val_losses = train_clusters(unsupervised_model, centroids, criterion, optimizer, train_loader_labelled,train_loader_unlabelled, val_loader, num_epochs=2, num_batches=num_batches, path_to_save=model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_centroids, model_dir+'centroids_unlabelled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is used to display all the generated outputs from this notebook on Kaggle and we can click on any output to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks \n",
    "FileLinks('.') #lists all downloadable files on server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "criterion = criterion.to(current_device)\n",
    "\n",
    "path = os.getcwd()\n",
    "model_dir = path\n",
    "\n",
    "opts = torch.load(model_dir+'opts_labelled')\n",
    "model = BERTSequenceClassifier(opts['num_classes']) #change here depending on model\n",
    "model.load_state_dict(torch.load(model_dir+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\n",
    "model = model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_centroids = torch.tensor([])\n",
    "\n",
    "TP_cluster, FP_cluster=evaluation.bert(model, empty_centroids, val_loader, criterion, data_dir, current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_cluster[TP_cluster[\"original\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = KMeansCriterion(1)\n",
    "criterion = criterion.to(current_device)\n",
    "\n",
    "path = os.getcwd()\n",
    "model_dir = path\n",
    "\n",
    "opts = torch.load(model_dir+'opts_labelled')\n",
    "model = BERTSequenceClassifier(opts['num_classes']) #change here depending on model\n",
    "model.load_state_dict(torch.load(model_dir+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\n",
    "model = model.to(current_device)\n",
    "model.W = nn.Identity()\n",
    "centroids = torch.load(model_dir+'centroids_unlabelled',map_location=lambda storage, loc: storage)\n",
    "centroids = centroids.to(current_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_cluster, FP_cluster=evaluation.bert(model, centroids, val_loader, criterion, data_dir, current_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "criterion = criterion.to(current_device)\n",
    "\n",
    "path = os.getcwd()\n",
    "model_dir = path\n",
    "\n",
    "opts = torch.load(model_dir+'opts_labelled')\n",
    "model = BERTSequenceClassifier(opts['num_classes']) #change here depending on model\n",
    "model.load_state_dict(torch.load(model_dir+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\n",
    "model = model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_centroids = torch.tensor([])\n",
    "\n",
    "TP_cluster, FP_cluster=evaluation.bert(model, empty_centroids, test_loader, criterion, data_dir, current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_cluster[TP_cluster[\"original\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = KMeansCriterion(1)\n",
    "criterion = criterion.to(current_device)\n",
    "\n",
    "path = os.getcwd()\n",
    "model_dir = path\n",
    "\n",
    "opts = torch.load(model_dir+'opts_labelled')\n",
    "model = BERTSequenceClassifier(opts['num_classes']) #change here depending on model\n",
    "model.load_state_dict(torch.load(model_dir+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\n",
    "model = model.to(current_device)\n",
    "model.W = nn.Identity()\n",
    "centroids = torch.load(model_dir+'centroids_unlabelled',map_location=lambda storage, loc: storage)\n",
    "centroids = centroids.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_cluster, FP_cluster=evaluation.bert(model, centroids, test_loader, criterion, data_dir, current_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Embeddings for Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be used to save embeddings generated by our models which can be used to make UMAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = path + '/umap/' + model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an embedding on validation set including centroids\n",
    "val_embed_labelled = []\n",
    "val_labels_lst = []\n",
    "\n",
    "for i, (tokens, labels, flagged_indices) in enumerate(val_loader):\n",
    "    model.eval()\n",
    "    tokens = tokens.to(current_device)\n",
    "    labels = labels.to(current_device)\n",
    "    flagged_indices = flagged_indices.to(current_device)\n",
    "\n",
    "    # forward pass and compute loss\n",
    "    sentence_embed = model(tokens,flagged_indices)\n",
    "\n",
    "    val_embed_labelled+= sentence_embed.tolist()    \n",
    "    val_labels_lst+=labels.tolist()\n",
    "val_embed_labelled += centroids.tolist()\n",
    "val_labels_lst += [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an embedding on training set\n",
    "embed_labelled = []\n",
    "labels_lst = []\n",
    "\n",
    "for i, (tokens, labels, flagged_indices) in enumerate(train_loader_labelled):\n",
    "    model.eval()\n",
    "    tokens = tokens.to(current_device)\n",
    "    labels = labels.to(current_device)\n",
    "    flagged_indices = flagged_indices.to(current_device)\n",
    "\n",
    "    # forward pass and compute loss\n",
    "    sentence_embed = model(tokens,flagged_indices)\n",
    "\n",
    "    embed_labelled+= sentence_embed.tolist()    \n",
    "    labels_lst+=labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out1 = open(save_dir + \"val_embed_labelled.pickle\",\"wb\")\n",
    "pickle.dump(val_embed_labelled, pickle_out1)\n",
    "pickle_out1.close()\n",
    "\n",
    "pickle_out2 = open(save_dir + \"val_labels_lst.pickle\",\"wb\")\n",
    "pickle.dump(val_labels_lst, pickle_out2)\n",
    "pickle_out2.close()\n",
    "\n",
    "pickle_out3 = open(save_dir + \"embed_labelled.pickle\",\"wb\")\n",
    "pickle.dump(embed_labelled, pickle_out3)\n",
    "pickle_out3.close()\n",
    "\n",
    "pickle_out4 = open(save_dir + \"labels.pickle\",\"wb\")\n",
    "pickle.dump(labels_lst, pickle_out4)\n",
    "pickle_out4.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
