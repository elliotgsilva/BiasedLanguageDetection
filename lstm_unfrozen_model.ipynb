{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## KAGGLE ONLY\nfrom shutil import copyfile\ncopyfile(src=\"../input/scriptandpickle/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\ncopyfile(src=\"../input/scriptssss/model.py\", dst=\"../working/model.py\")\ncopyfile(src=\"../input/newevaluation/evaluation.py\", dst=\"../working/evaluation.py\")\n\ncopyfile(src=\"../input/newfiles/train_dataloader_lstm.p\", dst=\"../working/train_dataloader_lstm.p\")\ncopyfile(src=\"../input/newfiles/val_dataloader_lstm.p\", dst=\"../working/val_dataloader_lstm.p\")\ncopyfile(src=\"../input/newfiles/dictionary_lstm.p\", dst=\"../working/dictionary.p\")\ncopyfile(src=\"../input/newfiles/train_unlabeled_dataloader_lstm.p\", dst=\"../working/train_unlabelled_dataloader_lstm.p\")\ncopyfile(src=\"../input/newfiles/train_labeled_dataloader_lstm.p\", dst=\"../working/train_labelled_dataloader_lstm.p\")","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"'../working/train_labelled_dataloader_lstm.p'"},"metadata":{}}]},{"metadata":{"colab":{},"colab_type":"code","id":"zno22FtJPX9z","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n#from datasets import get_mnist_dataset, get_data_loader\n#from utils import *\n#from models import *\n\nimport pickle as pkl\nimport os\nimport datetime as dt\nimport pandas as pd\nimport random\n\nfrom generate_dataloaders import *\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport evaluation\nimport importlib\nimportlib.reload(evaluation)","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"<module 'evaluation' from '/kaggle/working/evaluation.py'>"},"metadata":{}}]},{"metadata":{"colab_type":"text","id":"oaJEVd0wPX94"},"cell_type":"markdown","source":"## Get Dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1029\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\nnp.random.seed(seed)  # Numpy module.\nrandom.seed(seed)  # Python random module.\ntorch.manual_seed(seed)\ntorch.backends.cudnn.enabled = False \ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\ndef _init_fn(worker_id):\n    np.random.seed(int(seed))","execution_count":3,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"6nLzh007PX98","trusted":true},"cell_type":"code","source":"path = os.getcwd()\ndata_dir = path + '/'\n#data_dir = path +'/data/' #Uncomment for local system","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Verify filenames are consistent*"},{"metadata":{"colab":{},"colab_type":"code","id":"yq-jDGFIPX99","trusted":true},"cell_type":"code","source":"train_loader_labelled = pkl.load(open(data_dir + 'train_labelled_dataloader_lstm.p','rb'))\ntrain_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabelled_dataloader_lstm.p','rb'))\nval_loader = pkl.load(open(data_dir + 'val_dataloader_lstm.p','rb'))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_dict = pkl.load(open(data_dir + 'dictionary.p','rb'))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%conda install pytorch torchvision -c pytorch\n## if torch.__version__ is not 1.3.1, run this cell then restart kernel","execution_count":7,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Lzz8lwNQPX-B","outputId":"690cb77f-2525-4c5a-ea14-a162716e34d3","trusted":true},"cell_type":"code","source":"print(torch.__version__)","execution_count":8,"outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## PRE TRAINED WORD EMBEDDINGS "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float16')","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(review_dict, embedding_index ,dim = 200):\n#     embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(review_dict.tokens), dim))\n    unknown_words = []\n    \n    for word, i in review_dict.ids.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LOCAL - 2nd line // KAGGLE -- 1st line\nglove_twitter = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt' #Change loc for local system\n#glove_twitter = data_dir + 'glove.twitter.27B.200d.txt'","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_index = load_embeddings(glove_twitter)","execution_count":13,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  This is separate from the ipykernel package so we can avoid doing imports until\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28be9dd0f6c4ce9974ea4153bcd6584"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embedding_index,unknown_words = build_matrix(review_dict, embedding_index)\ndel embedding_index","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(review_dict.tokens)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"16256"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(unknown_words)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"4428"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for word in unknown_words:\n#     print(word)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_dict.get_id('great')","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"34"},"metadata":{}}]},{"metadata":{"colab_type":"text","id":"Cvt6N9QCPX-X"},"cell_type":"markdown","source":"## Neural Network LSTM Class"},{"metadata":{"colab_type":"text","id":"puweJhdxPX-Y"},"cell_type":"markdown","source":"NOTE: Data loader is defined as:\n- tuple: (tokens, flagged_index, problematic)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def freeze_model(model):\n    for param in model.parameters():\n        param.requires_grad = False\n        \ndef unfreeze_model(model):\n    for param in model.parameters():\n        param.requires_grad = True","execution_count":19,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"W8BZ-QhNPX-Z","trusted":true},"cell_type":"code","source":"class LSTM_model(nn.Module):\n    \"\"\"\n    LSTM classification model using pretrained glove embeddings\n    \"\"\"\n    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n    def __init__(self, embedding_matrix, num_hidden_layers = 3, hidden_size = 100, num_classes = 2):\n        super(LSTM_model, self).__init__()\n        vocab_size = embedding_matrix.shape[0]\n        embed_size = embedding_matrix.shape[1]\n        self.embedding_matrix = embedding_matrix\n        \n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.num_hidden_layers = num_hidden_layers\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        \n        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)    \n        self.embed.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embed.weight.requires_grad = False\n        \n        self.lstm = nn.LSTM(self.embed_size,self.hidden_size,self.num_hidden_layers, batch_first=True,bidirectional= True,bias=True)\n        \n        self.projection = nn.Linear(2*self.hidden_size, self.num_classes, bias=True)\n\n    \n    def forward(self, tokens, flagged_index):\n        batch_size, num_tokens = tokens.shape\n        embedding = self.embed(tokens)\n#         print(embedding.shape) # below assumes \"batch_size x num_tokens x Emb_dim\" (VERIFY)\n        \n        lstm_output = self.lstm(embedding)\n        # lstm_output is a tuple containing lstm output and (hidden_state, lstm_cell). \n        # lstm_output[0] would be of shape \"batch_size x num_tokens x hidden_size\" (VERIFY)\n        \n        logits = self.projection(lstm_output[0])\n        # logits would be of shape \"batch_size x num_tokens x num_classes (2)\" (VERIFY)\n        \n        batch_size, _, __ = logits.shape\n        \n        #selecting the logit at the flagged index\n        relevant_logits = logits[list(range(batch_size)),flagged_index]\n        # relevant_logits would be of shape \"batch_size x num_classes (2)\" (VERIFY)\n        \n        return relevant_logits","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First performing fully supervised learning using the labelled set to train new vector representations"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\nif num_gpus > 0:\n    current_device = 'cuda'\nelse:\n    current_device = 'cpu'\n\nmodel = LSTM_model(glove_embedding_index, num_hidden_layers = 3, hidden_size = 100, num_classes = 2).to(current_device)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(reduction='sum')\n#optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Supervised model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_supervised_model(model, criterion, train_loader_labelled, valid_loader, num_frozen_epochs=8, num_unfrozen_epochs=0, path_to_save=None, print_every = 1000):\n\n    train_losses=[]\n    val_losses=[]\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 0:\n        current_device = 'cuda'\n    else:\n        current_device = 'cpu'\n    \n    num_first_epochs = num_frozen_epochs\n    num_second_epochs = num_unfrozen_epochs\n    \n    empty_centroids = torch.tensor([])\n    # freeze part    \n    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n    \n    for epoch in range(num_first_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.train()\n        total_epoch_loss = 0\n        \n        for i,(tokens_labelled, labels, flagged_indices_labelled) in tqdm(enumerate(train_loader_labelled)):\n            \n            tokens_labelled = tokens_labelled.to(current_device)\n            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n            labels = labels.to(current_device)\n\n            # forward pass and compute loss\n            logits = model(tokens_labelled,flagged_indices_labelled)\n            \n            loss = criterion(logits, labels)\n        \n            # run update step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            #Add loss to the epoch loss\n            total_epoch_loss += loss.detach()\n\n            if i % print_every == 0:\n                losses = loss/len(tokens_labelled)\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= len(train_loader_labelled.dataset)\n        total_epoch_loss = total_epoch_loss.detach()\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n            model.eval()\n            tokens = tokens.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices = flagged_indices.to(current_device)\n            \n            # forward pass and compute loss\n            logits = model(tokens,flagged_indices)\n            \n            loss = criterion(logits, labels)\n            \n            #Add loss to the validation loss\n            total_validation_loss += loss\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        print('Train result:')\n        TP_cluster, FP_cluster=evaluation.main(model, empty_centroids, train_loader_labelled, criterion, data_dir, current_device)\n        print()\n        print('Validation result:')\n        TP_cluster, FP_cluster=evaluation.main(model, empty_centroids, valid_loader, criterion, data_dir, current_device)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n                    \"num_hidden_layers\":model.num_hidden_layers,\\\n                    \"hidden_size\":model.hidden_size,\\\n                    \"num_classes\":model.num_classes}\n            torch.save(model.state_dict(), path_to_save+ str(epoch)+'_model_dict_labelled.pt')\n            torch.save(train_losses, path_to_save+ str(epoch)+'_train_losses_labelled')\n            torch.save(val_losses, path_to_save+ str(epoch)+'_val_losses_labelled')\n            torch.save(opts, path_to_save+str(epoch)+'_opts_labelled')\n\n    # unfreeze part\n    unfreeze_model(model)\n    \n    params_to_update = []\n    for name,param in model.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n    \n\n    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n    \n    for epoch in range(num_second_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.train()\n        total_epoch_loss = 0\n\n        for i,(tokens_labelled, labels, flagged_indices_labelled) in tqdm(enumerate(train_loader_labelled)):\n            \n            tokens_labelled = tokens_labelled.to(current_device)\n            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n            labels = labels.to(current_device)\n\n            # forward pass and compute loss\n            logits = model(tokens_labelled,flagged_indices_labelled)\n            \n            loss = criterion(logits, labels)\n        \n            # run update step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            #Add loss to the epoch loss\n            total_epoch_loss += loss.detach()\n\n            if i % print_every == 0:\n                losses = loss/len(tokens_labelled)\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= len(train_loader_labelled.dataset)\n        total_epoch_loss = total_epoch_loss.detach()\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n            model.eval()\n            tokens = tokens.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices = flagged_indices.to(current_device)\n            \n            # forward pass and compute loss\n            logits = model(tokens,flagged_indices)\n            \n            loss = criterion(logits, labels)\n            \n            #Add loss to the validation loss\n            total_validation_loss += loss\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        print('Train result:')\n        TP_cluster, FP_cluster=evaluation.main(model, empty_centroids, train_loader_labelled, criterion, data_dir, current_device)\n        print()\n        print('Validation result:')\n        TP_cluster, FP_cluster=evaluation.main(model, empty_centroids, valid_loader, criterion, data_dir, current_device)\n        \n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n                    \"num_hidden_layers\":model.num_hidden_layers,\\\n                    \"hidden_size\":model.hidden_size,\\\n                    \"num_classes\":model.num_classes}\n            torch.save(model.state_dict(), path_to_save+ str(epoch)+'_model_dict_labelled.pt')\n            torch.save(train_losses, path_to_save+ str(epoch)+'_train_losses_labelled')\n            torch.save(val_losses, path_to_save+ str(epoch)+'_val_losses_labelled')\n            torch.save(opts, path_to_save+str(epoch)+'_opts_labelled')\n\n    return model, train_losses, val_losses","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = os.getcwd()\nmodel_folder = 'lstm_unfrozen_model/'\nmodel_dir = path + '/' #+ model_folder","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_loader_labelled)","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"226"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model, train_losses, val_losses = train_supervised_model(model, criterion, train_loader_labelled, val_loader, num_frozen_epochs=3, num_unfrozen_epochs=10, path_to_save=model_dir)\n","execution_count":36,"outputs":[{"output_type":"stream","text":"2019-12-07 21:24:20.166308 | Epoch 0\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b6842d5d06d4121b917e32fb8cd2e02"}},"metadata":{}},{"output_type":"stream","text":"Average training loss at batch  0 : 0.016\n\nAverage training loss after epoch  0 : 0.065\nAverage validation loss after epoch  0 : 0.462\nTrain result:\nTotal examples in val loader: 7226\nAssigned to cluster 1: 3778\nTP_rate: 0.9499735309687666\nFP_rate: 0.05002646903123346\nFN_rate: 0.0069605568445475635\nTN_rate: 0.9930394431554525\n\n\nAccuracy: 0.9715064870621095\nPrecision: 0.9499735309687666\nRecall: 0.9927261898879022\nF1 score: 0.9708794352192728\nValidation result:\nTotal examples in val loader: 517\nAssigned to cluster 1: 478\nTP_rate: 0.9121338912133892\nFP_rate: 0.08786610878661087\nFN_rate: 0.41025641025641024\nTN_rate: 0.5897435897435898\n\n\nAccuracy: 0.7509387404784895\nPrecision: 0.9121338912133892\nRecall: 0.6897614797987993\nF1 score: 0.7855130041114243\n2019-12-07 21:24:34.786821 | Epoch 1\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1bfcf8475947799f0777005f6015e5"}},"metadata":{}},{"output_type":"stream","text":"Average training loss at batch  0 : 0.098\n\nAverage training loss after epoch  1 : 0.047\nAverage validation loss after epoch  1 : 0.567\nTrain result:\nTotal examples in val loader: 7226\nAssigned to cluster 1: 3662\nTP_rate: 0.9814309120699072\nFP_rate: 0.018569087930092845\nFN_rate: 0.005331088664421998\nTN_rate: 0.994668911335578\n\n\nAccuracy: 0.9880499117027426\nPrecision: 0.9814309120699072\nRecall: 0.9945973916096742\nF1 score: 0.9879702870370578\nValidation result:\nTotal examples in val loader: 517\nAssigned to cluster 1: 455\nTP_rate: 0.9384615384615385\nFP_rate: 0.06153846153846154\nFN_rate: 0.4032258064516129\nTN_rate: 0.5967741935483871\n\n\nAccuracy: 0.7676178660049628\nPrecision: 0.9384615384615385\nRecall: 0.6994636582208249\nF1 score: 0.8015259086574124\n2019-12-07 21:24:49.649500 | Epoch 2\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e397be4f7683433fa072c2bdad99141f"}},"metadata":{}},{"output_type":"stream","text":"Average training loss at batch  0 : 0.007\n\nAverage training loss after epoch  2 : 0.036\nAverage validation loss after epoch  2 : 0.535\nTrain result:\nTotal examples in val loader: 7226\nAssigned to cluster 1: 3563\nTP_rate: 0.9935447656469267\nFP_rate: 0.006455234353073253\nFN_rate: 0.01992901992901993\nTN_rate: 0.98007098007098\n\n\nAccuracy: 0.9868078728589533\nPrecision: 0.9935447656469267\nRecall: 0.9803359295399097\nF1 score: 0.986896152077517\nValidation result:\nTotal examples in val loader: 517\nAssigned to cluster 1: 437\nTP_rate: 0.9473684210526315\nFP_rate: 0.05263157894736842\nFN_rate: 0.475\nTN_rate: 0.525\n\n\nAccuracy: 0.7361842105263158\nPrecision: 0.9473684210526315\nRecall: 0.666049953746531\nF1 score: 0.7821835958718087\n2019-12-07 21:25:04.536092 | Epoch 0\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:103: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e17a1a5bee41acb606b644977e3e71"}},"metadata":{}},{"output_type":"stream","text":"Average training loss at batch  0 : 0.097\n\nAverage training loss after epoch  0 : 0.043\nAverage validation loss after epoch  0 : 0.656\nTrain result:\nTotal examples in val loader: 7226\nAssigned to cluster 1: 3597\nTP_rate: 0.9958298582151793\nFP_rate: 0.004170141784820684\nFN_rate: 0.008542298153761367\nTN_rate: 0.9914577018462386\n\n\nAccuracy: 0.993643780030709\nPrecision: 0.9958298582151793\nRecall: 0.991494887527902\nF1 score: 0.9936576449147989\nValidation result:\nTotal examples in val loader: 517\nAssigned to cluster 1: 456\nTP_rate: 0.9298245614035088\nFP_rate: 0.07017543859649122\nFN_rate: 0.45901639344262296\nTN_rate: 0.5409836065573771\n\n\nAccuracy: 0.735404083980443\nPrecision: 0.9298245614035088\nRecall: 0.6694967902257196\nF1 score: 0.7784733927281483\n2019-12-07 21:25:19.371246 | Epoch 1\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69cac20056c74be98a8ad24c26620ded"}},"metadata":{}},{"output_type":"stream","text":"Average training loss at batch  0 : 0.005\n\nAverage training loss after epoch  1 : 0.049\nAverage validation loss after epoch  1 : 0.482\nTrain result:\nTotal examples in val loader: 7226\nAssigned to cluster 1: 3600\nTP_rate: 0.9930555555555556\nFP_rate: 0.006944444444444444\nFN_rate: 0.010479867622724766\nTN_rate: 0.9895201323772752\n\n\nAccuracy: 0.9912878439664154\nPrecision: 0.9930555555555556\nRecall: 0.989557052615508\nF1 score: 0.9913032173698589\nValidation result:\nTotal examples in val loader: 517\nAssigned to cluster 1: 453\nTP_rate: 0.9359823399558499\nFP_rate: 0.0640176600441501\nFN_rate: 0.4375\nTN_rate: 0.5625\n\n\nAccuracy: 0.7492411699779249\nPrecision: 0.9359823399558499\nRecall: 0.681466599698644\nF1 score: 0.7886996454106844\n2019-12-07 21:25:34.151085 | Epoch 2\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c6b0ca16b86490ea65e06fd182251b5"}},"metadata":{}},{"output_type":"stream","text":"Average training loss at batch  0 : 0.098\n\nAverage training loss after epoch  2 : 0.036\nAverage validation loss after epoch  2 : 0.512\nTrain result:\nTotal examples in val loader: 7226\nAssigned to cluster 1: 3608\nTP_rate: 0.9925166297117517\nFP_rate: 0.007483370288248337\nFN_rate: 0.008844665561083471\nTN_rate: 0.9911553344389166\n\n\nAccuracy: 0.9918359820753342\nPrecision: 0.9925166297117517\nRecall: 0.9911673582723469\nF1 score: 0.9918415351151748\nValidation result:\nTotal examples in val loader: 517\nAssigned to cluster 1: 456\nTP_rate: 0.9385964912280702\nFP_rate: 0.06140350877192982\nFN_rate: 0.39344262295081966\nTN_rate: 0.6065573770491803\n\n\nAccuracy: 0.7725769341386253\nPrecision: 0.9385964912280702\nRecall: 0.704631328943107\nF1 score: 0.8049577603749152\n2019-12-07 21:25:48.958454 | Epoch 3\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4af06595ee34e1ba0c6347f794ee356"}},"metadata":{}},{"output_type":"stream","text":"Average training loss at batch  0 : 0.085\n\nAverage training loss after epoch  3 : 0.026\nAverage validation loss after epoch  3 : 0.727\nTrain result:\nTotal examples in val loader: 7226\nAssigned to cluster 1: 3620\nTP_rate: 0.9903314917127072\nFP_rate: 0.009668508287292817\nFN_rate: 0.00776483638380477\nTN_rate: 0.9922351636161952\n\n\nAccuracy: 0.9912833276644513\nPrecision: 0.9903314917127072\nRecall: 0.992220353722157\nF1 score: 0.991275022917586\nValidation result:\nTotal examples in val loader: 517\nAssigned to cluster 1: 464\nTP_rate: 0.9267241379310345\nFP_rate: 0.07327586206896551\nFN_rate: 0.41509433962264153\nTN_rate: 0.5849056603773585\n\n\nAccuracy: 0.7558148991541964\nPrecision: 0.9267241379310345\nRecall: 0.6906479180556396\nF1 score: 0.7914568501475951\n2019-12-07 21:26:03.695562 | Epoch 4\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b48f80c626641c783c3a6fef6b2e839"}},"metadata":{}},{"output_type":"stream","text":"Average training loss at batch  0 : 0.002\n\nAverage training loss after epoch  4 : 0.024\nAverage validation loss after epoch  4 : 0.731\nTrain result:\nTotal examples in val loader: 7226\nAssigned to cluster 1: 3621\nTP_rate: 0.9928196630764982\nFP_rate: 0.007180336923501795\nFN_rate: 0.0049930651872399446\nTN_rate: 0.99500693481276\n\n\nAccuracy: 0.9939132989446291\nPrecision: 0.9928196630764982\nRecall: 0.9949959896824244\nF1 score: 0.9939066350221317\nValidation result:\nTotal examples in val loader: 517\nAssigned to cluster 1: 465\nTP_rate: 0.9204301075268817\nFP_rate: 0.07956989247311828\nFN_rate: 0.46153846153846156\nTN_rate: 0.5384615384615384\n\n\nAccuracy: 0.72944582299421\nPrecision: 0.9204301075268817\nRecall: 0.6660282499401483\nF1 score: 0.772831446628238\n","name":"stdout"}]},{"metadata":{"colab_type":"text","id":"SGsqcnEtPX-a"},"cell_type":"markdown","source":"### Clustering Stuff"},{"metadata":{"colab":{},"colab_type":"code","id":"MrgIYm8JPX-b","trusted":true},"cell_type":"code","source":"class KMeansCriterion(nn.Module):\n    \n    def __init__(self, lmbda):\n        super().__init__()\n        self.lmbda = lmbda\n    \n    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n        if labelled:\n            num_reviews = len(cluster_assignments)\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n            loss = self.lmbda * cluster_distances.sum()\n        else:\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances, cluster_assignments = distances.min(1)\n            loss = self.lmbda * cluster_distances.sum()\n        return loss, cluster_assignments","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"-TJohK2aPX-d","trusted":true},"cell_type":"code","source":"def centroid_init(k, d, dataloader, model, current_device):\n    ## Here we ideally don't want to do randomized/zero initialization\n    centroid_sums = torch.zeros(k, d).to(current_device)\n    centroid_counts = torch.zeros(k).to(current_device)\n    for (tokens, labels, flagged_indices) in dataloader:\n        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n        cluster_assignments = labels.to(current_device)\n        \n        model.eval()\n        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n    \n        update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n                        cluster_assignments.detach(), sentence_embed.to(current_device).detach())\n    \n    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n    return centroid_means.clone()\n\ndef update_clusters(centroid_sums, centroid_counts,\n                    cluster_assignments, embeddings):\n    k = centroid_sums.size(0)\n\n    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n    centroid_counts.add_(bin_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataloader stuff"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n    try:\n        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n    except StopIteration:\n        train_loader_labelled_iter = iter(train_loader_labelled)\n        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n\n    return tokens, labels, flagged_indices, train_loader_labelled_iter\n\n\ndef loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n    try:\n        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n    except StopIteration:\n        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n\n    return tokens, labels, flagged_indices, train_loader_unlabelled_iter","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"u3wynM7fPX-h"},"cell_type":"markdown","source":"### Training Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_clusters(model, centroids, criterion, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=15, num_batches = 1000, path_to_save=None, print_every = 1000):\n\n    train_loader_labelled_iter = iter(train_loader_labelled)\n    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n\n    train_losses=[]\n    val_losses=[]\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 0:\n        current_device = 'cuda'\n    else:\n        current_device = 'cpu'\n    \n    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n    \n    for epoch in range(num_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.eval() # we're only clustering, not training model\n        k, d = centroids.size()\n        centroid_sums = torch.zeros_like(centroids).to(current_device)\n        centroid_counts = torch.zeros(k).to(current_device)\n        total_epoch_loss = 0\n        \n        for i in tqdm(range(int(num_batches))):\n            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n\n            tokens_labelled = tokens_labelled.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n            \n            tokens_unlabelled = tokens_unlabelled.to(current_device)\n            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n\n            # forward pass and compute loss\n            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n            \n            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n    \n            total_batch_loss = cluster_loss_labelled.data + cluster_loss_unlabelled.data\n            \n#             #Add loss to the epoch loss\n            total_epoch_loss += total_batch_loss.data\n\n#             # store centroid sums and counts in memory for later centering\n            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n                            cluster_assignments_labelled.detach(), sentence_embed_labelled.detach())\n    \n            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n                            cluster_assignments_unlabelled.detach(), sentence_embed_unlabelled.detach())\n\n            if i % print_every == 0:\n                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # update centroids based on assignments from autoencoders\n        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n            model.eval()\n            tokens = tokens.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices = flagged_indices.to(current_device)\n            \n            # forward pass and compute loss\n            sentence_embed = model(tokens,flagged_indices)\n            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n            \n            #Add loss to the validation loss\n            total_validation_loss += cluster_loss.data\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n                    \"num_hidden_layers\":model.num_hidden_layers,\\\n                    \"hidden_size\":model.hidden_size,\\\n                    \"num_classes\":model.num_classes}\n            torch.save(model.state_dict(), path_to_save+'model_dict_unlabelled.pt')\n            torch.save(centroids, path_to_save+'centroids_unlabelled')\n            torch.save(train_losses, path_to_save+'train_losses_unlabelled')\n            torch.save(val_losses, path_to_save+'val_losses_unlabelled')\n            torch.save(opts, path_to_save+'opts_unlabelled')\n        \n    return model, centroids, train_losses, val_losses","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"0pBet75ZPX-m","trusted":true},"cell_type":"code","source":"unsupervised_model = LSTM_model(glove_embedding_index, num_hidden_layers = 3, hidden_size = 100, num_classes = 2).to(current_device)\n\nunsupervised_model.projection = nn.Identity()","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"mTFO2vp-PX-o","trusted":true},"cell_type":"code","source":"centroids = centroid_init(2, 2*unsupervised_model.hidden_size, train_loader_labelled, unsupervised_model, current_device)\ncriterion = KMeansCriterion(1).to(current_device)\n#optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Xya2NiqcPX-q","outputId":"59b3072e-c567-4e18-a242-ba8298f08e58","scrolled":true,"trusted":true},"cell_type":"code","source":"centroids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = os.getcwd()\nmodel_folder = 'lstm_unfrozen_model/'\nmodel_dir = path + '/'#'/models/' + model_folder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_batches = int(len(train_loader_unlabelled.dataset)/train_loader_unlabelled.batch_size)+1\nnum_batches","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"colab_type":"code","executionInfo":{"elapsed":8813,"status":"error","timestamp":1573355511003,"user":{"displayName":"Eileen Cho","photoUrl":"","userId":"03381570147993013394"},"user_tz":300},"id":"rgwMd27mPX-u","outputId":"063ebc41-be3c-4474-d92c-7bd0680bb366","scrolled":true,"trusted":true},"cell_type":"code","source":"lstm_model, lstm_centroids, lstm_train_losses, lstm_val_losses = train_clusters(unsupervised_model, centroids, criterion, train_loader_labelled,train_loader_unlabelled, val_loader, num_epochs=3, num_batches=num_batches, path_to_save=model_dir)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(lstm_centroids, model_dir+'centroids_unlabelled')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Only needed for Kaggle\n\n# from IPython.display import FileLink, FileLinks \n# FileLinks('.') #lists all downloadable files on server","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Model"},{"metadata":{},"cell_type":"markdown","source":"## Supervised Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\nif num_gpus > 0:\n    current_device = 'cuda'\nelse:\n    current_device = 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## This cell will change for each model\nmodel_folder = 'lstm_model/'\n\ncriterion = nn.CrossEntropyLoss(reduction='sum')\ncriterion = criterion.to(current_device)\n\npath = os.getcwd()\nmodel_dir = ''#path #+ '/models/' + model_folder\n\nopts = torch.load(model_dir+'opts_labelled')\nmodel = LSTM_model(opts['embedding_matrix']) #change here depending on model\nmodel.load_state_dict(torch.load(model_dir+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\nmodel = model.to(current_device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_centroids = torch.tensor([])\n\nTP_cluster, FP_cluster=evaluation.main(model, empty_centroids, train_loader_labelled, criterion, data_dir, current_device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unsupervised Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"## This cell will change for each model\nmodel_folder = 'lstm_model/'\n\ncriterion = KMeansCriterion(1)\ncriterion = criterion.to(current_device)\n\npath = os.getcwd()\nmodel_dir = ''#path + '/models/' + model_folder\n\nopts = torch.load(model_dir+'opts_unlabelled')\nmodel = LSTM_model(opts['embedding_matrix']) #change here depending on model\nmodel.projection = nn.Identity()\nmodel.load_state_dict(torch.load(model_dir+'model_dict_unlabelled.pt',map_location=lambda storage, loc: storage))\nmodel = model.to(current_device)\ncentroids = torch.load(model_dir+'centroids_unlabelled',map_location=lambda storage, loc: storage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP_cluster, FP_cluster=evaluation.main(model, centroids, val_loader, criterion, data_dir, current_device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#TP_cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#FP_cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#FP_cluster[FP_cluster.original == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"model_tuning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}