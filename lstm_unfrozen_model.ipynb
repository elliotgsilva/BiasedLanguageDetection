{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## KAGGLE ONLY\nfrom shutil import copyfile\ncopyfile(src=\"../input/scriptandpickle/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\ncopyfile(src=\"../input/capstone/train_dataloader.p\", dst=\"../working/train_dataloader.p\")\ncopyfile(src=\"../input/capstone/val_dataloader.p\", dst=\"../working/val_dataloader.p\")\n#copyfile(src=\"../input/capstone/centroids_dataloader.p\", dst=\"../working/ground_truth_dataloader.p\")\ncopyfile(src=\"../input/scriptandpickle/dictionary.p\", dst=\"../working/dictionary.p\")\ncopyfile(src=\"../input/scriptssss/evaluation.py\", dst=\"../working/evaluation.py\")\ncopyfile(src=\"../input/scriptssss/model.py\", dst=\"../working/model.py\")\n\n\ncopyfile(src=\"../input/capstone/train_unlabeled_dataloader.p\", dst=\"../working/train_unlabelled_dataloader.p\")\ncopyfile(src=\"../input/capstone/train_labeled_dataloader.p\", dst=\"../working/train_labelled_dataloader.p\")","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"zno22FtJPX9z","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n#from datasets import get_mnist_dataset, get_data_loader\n#from utils import *\n#from models import *\n\nimport pickle as pkl\nimport os\nimport datetime as dt\nimport pandas as pd\nimport random\n\nfrom generate_dataloaders import *\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport evaluation\nimport importlib\nimportlib.reload(evaluation)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"oaJEVd0wPX94"},"cell_type":"markdown","source":"## Get Dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1029\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\nnp.random.seed(seed)  # Numpy module.\nrandom.seed(seed)  # Python random module.\ntorch.manual_seed(seed)\ntorch.backends.cudnn.enabled = False \ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\ndef _init_fn(worker_id):\n    np.random.seed(int(seed))","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"6nLzh007PX98","trusted":true},"cell_type":"code","source":"path = os.getcwd()\ndata_dir = path + '/'\n#data_dir = path +'/data/' #Uncomment for local system","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Verify filenames are consistent*"},{"metadata":{"colab":{},"colab_type":"code","id":"yq-jDGFIPX99","trusted":true},"cell_type":"code","source":"train_loader_labelled = pkl.load(open(data_dir + 'train_labelled_dataloader.p','rb'))\ntrain_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabelled_dataloader.p','rb'))\nval_loader = pkl.load(open(data_dir + 'val_dataloader.p','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_dict = pkl.load(open(data_dir + 'dictionary.p','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%conda install pytorch torchvision -c pytorch\n## if torch.__version__ is not 1.3.1, run this cell then restart kernel","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Lzz8lwNQPX-B","outputId":"690cb77f-2525-4c5a-ea14-a162716e34d3","trusted":true},"cell_type":"code","source":"print(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PRE TRAINED WORD EMBEDDINGS "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(review_dict, embedding_index ,dim = 200):\n#     embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(review_dict.tokens), dim))\n    unknown_words = []\n    \n    for word, i in review_dict.ids.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LOCAL - 2nd line // KAGGLE -- 1st line\nglove_twitter = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt' #Change loc for local system\n#glove_twitter = data_dir + 'glove.twitter.27B.200d.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_index = load_embeddings(glove_twitter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embedding_index,unknown_words = build_matrix(review_dict, embedding_index)\ndel embedding_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(review_dict.tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(unknown_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for word in unknown_words:\n#     print(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_dict.get_id('great')","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"Cvt6N9QCPX-X"},"cell_type":"markdown","source":"## Neural Network LSTM Class"},{"metadata":{"colab_type":"text","id":"puweJhdxPX-Y"},"cell_type":"markdown","source":"NOTE: Data loader is defined as:\n- tuple: (tokens, flagged_index, problematic)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def freeze_model(model):\n    for param in model.parameters():\n        param.requires_grad = False\n        \ndef unfreeze_model(model):\n    for param in model.parameters():\n        param.requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"W8BZ-QhNPX-Z","trusted":true},"cell_type":"code","source":"class LSTM_model(nn.Module):\n    \"\"\"\n    LSTM classification model using pretrained glove embeddings\n    \"\"\"\n    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n    def __init__(self, embedding_matrix, num_hidden_layers = 3, hidden_size = 100, num_classes = 2):\n        super(LSTM_model, self).__init__()\n        vocab_size = embedding_matrix.shape[0]\n        embed_size = embedding_matrix.shape[1]\n        self.embedding_matrix = embedding_matrix\n        \n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.num_hidden_layers = num_hidden_layers\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        \n        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)    \n        self.embed.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embed.weight.requires_grad = False\n        \n        self.lstm = nn.LSTM(self.embed_size,self.hidden_size,self.num_hidden_layers, batch_first=True,bidirectional= True,bias=True)\n        \n        self.projection = nn.Linear(2*self.hidden_size, self.num_classes, bias=True)\n\n    \n    def forward(self, tokens, flagged_index):\n        batch_size, num_tokens = tokens.shape\n        embedding = self.embed(tokens)\n#         print(embedding.shape) # below assumes \"batch_size x num_tokens x Emb_dim\" (VERIFY)\n        \n        lstm_output = self.lstm(embedding)\n        # lstm_output is a tuple containing lstm output and (hidden_state, lstm_cell). \n        # lstm_output[0] would be of shape \"batch_size x num_tokens x hidden_size\" (VERIFY)\n        \n        logits = self.projection(lstm_output[0])\n        # logits would be of shape \"batch_size x num_tokens x num_classes (2)\" (VERIFY)\n        \n        batch_size, _, __ = logits.shape\n        \n        #selecting the logit at the flagged index\n        relevant_logits = logits[list(range(batch_size)),flagged_index]\n        # relevant_logits would be of shape \"batch_size x num_classes (2)\" (VERIFY)\n        \n        return relevant_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First performing fully supervised learning using the labelled set to train new vector representations"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\nif num_gpus > 0:\n    current_device = 'cuda'\nelse:\n    current_device = 'cpu'\n\nmodel = LSTM_model(glove_embedding_index, num_hidden_layers = 3, hidden_size = 100, num_classes = 2).to(current_device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(reduction='sum')\n#optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Supervised model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_supervised_model(model, criterion, train_loader_labelled, valid_loader, num_frozen_epochs=10, num_unfrozen_epochs=10, path_to_save=None, print_every = 1000):\n\n    train_losses=[]\n    val_losses=[]\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 0:\n        current_device = 'cuda'\n    else:\n        current_device = 'cpu'\n    \n    num_first_epochs = num_frozen_epochs\n    num_second_epochs = num_unfrozen_epochs\n    \n\n    # freeze part    \n    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n    \n    for epoch in range(num_first_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.train()\n        total_epoch_loss = 0\n        \n        for i,(tokens_labelled, labels, flagged_indices_labelled) in tqdm(enumerate(train_loader_labelled)):\n            \n            tokens_labelled = tokens_labelled.to(current_device)\n            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n            labels = labels.to(current_device)\n\n            # forward pass and compute loss\n            logits = model(tokens_labelled,flagged_indices_labelled)\n            \n            loss = criterion(logits, labels)\n        \n            # run update step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            #Add loss to the epoch loss\n            total_epoch_loss += loss.detach()\n\n            if i % print_every == 0:\n                losses = loss/len(tokens_labelled)\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= len(train_loader_labelled.dataset)\n        total_epoch_loss = total_epoch_loss.detach()\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n            model.eval()\n            tokens = tokens.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices = flagged_indices.to(current_device)\n            \n            # forward pass and compute loss\n            logits = model(tokens,flagged_indices)\n            \n            loss = criterion(logits, labels)\n            \n            #Add loss to the validation loss\n            total_validation_loss += loss\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n                    \"num_hidden_layers\":model.num_hidden_layers,\\\n                    \"hidden_size\":model.hidden_size,\\\n                    \"num_classes\":model.num_classes}\n            torch.save(model.state_dict(), path_to_save+'model_dict_labelled.pt')\n            torch.save(train_losses, path_to_save+'train_losses_labelled')\n            torch.save(val_losses, path_to_save+'val_losses_labelled')\n            torch.save(opts, path_to_save+'opts_labelled')\n\n    # unfreeze part\n    unfreeze_model(model)\n    \n    params_to_update = []\n    for name,param in model.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n    \n\n    optimizer = torch.optim.SGD(params_to_update, lr=0.0001)\n    \n    for epoch in range(num_second_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.train()\n        total_epoch_loss = 0\n\n        for i,(tokens_labelled, labels, flagged_indices_labelled) in tqdm(enumerate(train_loader_labelled)):\n            \n            tokens_labelled = tokens_labelled.to(current_device)\n            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n            labels = labels.to(current_device)\n\n            # forward pass and compute loss\n            logits = model(tokens_labelled,flagged_indices_labelled)\n            \n            loss = criterion(logits, labels)\n        \n            # run update step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            #Add loss to the epoch loss\n            total_epoch_loss += loss.detach()\n\n            if i % print_every == 0:\n                losses = loss/len(tokens_labelled)\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= len(train_loader_labelled.dataset)\n        total_epoch_loss = total_epoch_loss.detach()\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n            model.eval()\n            tokens = tokens.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices = flagged_indices.to(current_device)\n            \n            # forward pass and compute loss\n            logits = model(tokens,flagged_indices)\n            \n            loss = criterion(logits, labels)\n            \n            #Add loss to the validation loss\n            total_validation_loss += loss\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n                    \"num_hidden_layers\":model.num_hidden_layers,\\\n                    \"hidden_size\":model.hidden_size,\\\n                    \"num_classes\":model.num_classes}\n            torch.save(model.state_dict(), path_to_save+'model_dict_labelled.pt')\n            torch.save(train_losses, path_to_save+'train_losses_labelled')\n            torch.save(val_losses, path_to_save+'val_losses_labelled')\n            torch.save(opts, path_to_save+'opts_labelled')\n\n    return model, train_losses, val_losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = os.getcwd()\nmodel_folder = 'lstm_unfrozen_model/'\nmodel_dir = path + '/' #+ model_folder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_loader_labelled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model, train_losses, val_losses = train_supervised_model(model, criterion, train_loader_labelled, val_loader, num_frozen_epochs=10, num_unfrozen_epochs=10, path_to_save=model_dir)\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"SGsqcnEtPX-a"},"cell_type":"markdown","source":"### Clustering Stuff"},{"metadata":{"colab":{},"colab_type":"code","id":"MrgIYm8JPX-b","trusted":false},"cell_type":"code","source":"class KMeansCriterion(nn.Module):\n    \n    def __init__(self, lmbda):\n        super().__init__()\n        self.lmbda = lmbda\n    \n    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n        if labelled:\n            num_reviews = len(cluster_assignments)\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n            loss = self.lmbda * cluster_distances.sum()\n        else:\n            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n            cluster_distances, cluster_assignments = distances.min(1)\n            loss = self.lmbda * cluster_distances.sum()\n        return loss, cluster_assignments","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"-TJohK2aPX-d","trusted":false},"cell_type":"code","source":"def centroid_init(k, d, dataloader, model, current_device):\n    ## Here we ideally don't want to do randomized/zero initialization\n    centroid_sums = torch.zeros(k, d).to(current_device)\n    centroid_counts = torch.zeros(k).to(current_device)\n    for (tokens, labels, flagged_indices) in dataloader:\n        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n        cluster_assignments = labels.to(current_device)\n        \n        model.eval()\n        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n    \n        update_clusters(centroid_sums, centroid_counts,\n                        cluster_assignments, sentence_embed.to(current_device))\n    \n    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n    return centroid_means.clone()\n\ndef update_clusters(centroid_sums, centroid_counts,\n                    cluster_assignments, embeddings):\n    k = centroid_sums.size(0)\n\n    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n    centroid_counts.add_(bin_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataloader stuff"},{"metadata":{"trusted":false},"cell_type":"code","source":"def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n    try:\n        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n    except StopIteration:\n        train_loader_labelled_iter = iter(train_loader_labelled)\n        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n\n    return tokens, labels, flagged_indices, train_loader_labelled_iter\n\n\ndef loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n    try:\n        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n    except StopIteration:\n        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n\n    return tokens, labels, flagged_indices, train_loader_unlabelled_iter","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"u3wynM7fPX-h"},"cell_type":"markdown","source":"### Training Function"},{"metadata":{"trusted":false},"cell_type":"code","source":"def train_clusters(model, centroids, criterion, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=15 num_batches = 1000, path_to_save=None, print_every = 1000):\n\n    train_loader_labelled_iter = iter(train_loader_labelled)\n    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n\n    train_losses=[]\n    val_losses=[]\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 0:\n        current_device = 'cuda'\n    else:\n        current_device = 'cpu'\n    \n    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n    \n    for epoch in range(num_epochs):\n        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n        model.eval() # we're only clustering, not training model\n        k, d = centroids.size()\n        centroid_sums = torch.zeros_like(centroids).to(current_device)\n        centroid_counts = torch.zeros(k).to(current_device)\n        total_epoch_loss = 0\n        \n        for i in tqdm(range(int(num_batches))):\n            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n\n            tokens_labelled = tokens_labelled.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n            \n            tokens_unlabelled = tokens_unlabelled.to(current_device)\n            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n\n            # forward pass and compute loss\n            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n            \n            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n    \n            total_batch_loss = cluster_loss_labelled.data + cluster_loss_unlabelled.data\n            \n#             #Add loss to the epoch loss\n            total_epoch_loss += total_batch_loss.data\n\n#             # store centroid sums and counts in memory for later centering\n            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n                            cluster_assignments_labelled.detach(), sentence_embed_labelled.detach())\n    \n            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n                            cluster_assignments_unlabelled.detach(), sentence_embed_unlabelled.detach())\n\n            if i % print_every == 0:\n                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n                print('Average training loss at batch ',i,': %.3f' % losses)\n            \n        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n        train_losses.append(total_epoch_loss)\n        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n        \n        # update centroids based on assignments from autoencoders\n        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n        \n        # calculate validation loss after every epoch\n        total_validation_loss = 0\n        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n            model.eval()\n            tokens = tokens.to(current_device)\n            labels = labels.to(current_device)\n            flagged_indices = flagged_indices.to(current_device)\n            \n            # forward pass and compute loss\n            sentence_embed = model(tokens,flagged_indices)\n            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n            \n            #Add loss to the validation loss\n            total_validation_loss += cluster_loss.data\n\n        total_validation_loss /= len(valid_loader.dataset)\n        val_losses.append(total_validation_loss)\n        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n        \n        if path_to_save == None:\n            pass\n        else:\n            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n                    \"num_hidden_layers\":model.num_hidden_layers,\\\n                    \"hidden_size\":model.hidden_size,\\\n                    \"num_classes\":model.num_classes}\n            torch.save(model.state_dict(), path_to_save+'model_dict_unlabelled.pt')\n            torch.save(centroids, path_to_save+'centroids_unlabelled')\n            torch.save(train_losses, path_to_save+'train_losses_unlabelled')\n            torch.save(val_losses, path_to_save+'val_losses_unlabelled')\n            torch.save(opts, path_to_save+'opts_unlabelled')\n        \n    return model, centroids, train_losses, val_losses","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"0pBet75ZPX-m","trusted":false},"cell_type":"code","source":"unsupervised_model = model\nunsupervised_model.projection = nn.Identity()","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"mTFO2vp-PX-o","trusted":false},"cell_type":"code","source":"centroids = centroid_init(2, 2*unsupervised_model.hidden_size, train_loader_labelled, unsupervised_model, current_device)\ncriterion = KMeansCriterion(1).to(current_device)\n#optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Xya2NiqcPX-q","outputId":"59b3072e-c567-4e18-a242-ba8298f08e58","scrolled":true,"trusted":false},"cell_type":"code","source":"centroids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"path = os.getcwd()\nmodel_folder = 'lstm_unfrozen_model/'\nmodel_dir = path + '/models/' + model_folder","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"num_batches = int(len(train_loader_unlabelled.dataset)/train_loader_unlabelled.batch_size)+1\nnum_batches","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"colab_type":"code","executionInfo":{"elapsed":8813,"status":"error","timestamp":1573355511003,"user":{"displayName":"Eileen Cho","photoUrl":"","userId":"03381570147993013394"},"user_tz":300},"id":"rgwMd27mPX-u","outputId":"063ebc41-be3c-4474-d92c-7bd0680bb366","scrolled":true,"trusted":false},"cell_type":"code","source":"lstm_model, lstm_centroids, lstm_train_losses, lstm_val_losses = train_clusters(unsupervised_model, centroids, criterion, train_loader_labelled,train_loader_unlabelled, val_loader, num_epochs=15, num_batches=num_batches, path_to_save=model_dir)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.save(lstm_centroids, model_dir+'centroids_unlabelled')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# #Only needed for Kaggle\n\n# from IPython.display import FileLink, FileLinks \n# FileLinks('.') #lists all downloadable files on server","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Model"},{"metadata":{},"cell_type":"markdown","source":"## Supervised Evaluation"},{"metadata":{"trusted":false},"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\nif num_gpus > 0:\n    current_device = 'cuda'\nelse:\n    current_device = 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## This cell will change for each model\nmodel_folder = 'lstm_model/'\n\ncriterion = nn.CrossEntropyLoss(reduction='sum')\ncriterion = criterion.to(current_device)\n\npath = os.getcwd()\nmodel_dir = path + '/models/' + model_folder\n\nopts = torch.load(model_dir+'opts_labelled')\nmodel = LSTM_model(opts['embedding_matrix']) #change here depending on model\nmodel.load_state_dict(torch.load(model_dir+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\nmodel = model.to(current_device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"TP_cluster, FP_cluster=evaluation.main(model, None, val_loader, criterion, data_dir, current_device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unsupervised Evaluation"},{"metadata":{"trusted":false},"cell_type":"code","source":"## This cell will change for each model\nmodel_folder = 'lstm_model/'\n\ncriterion = KMeansCriterion(1)\ncriterion = criterion.to(current_device)\n\npath = os.getcwd()\nmodel_dir = path + '/models/' + model_folder\n\nopts = torch.load(model_dir+'opts_unlabelled')\nmodel = LSTM_model(opts['embedding_matrix']) #change here depending on model\nmodel.projection = nn.Identity()\nmodel.load_state_dict(torch.load(model_dir+'model_dict_unlabelled.pt',map_location=lambda storage, loc: storage))\nmodel = model.to(current_device)\ncentroids = torch.load(model_dir+'centroids_unlabelled',map_location=lambda storage, loc: storage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"TP_cluster, FP_cluster=evaluation.main(model, centroids, val_loader, criterion, data_dir, current_device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#TP_cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#FP_cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#FP_cluster[FP_cluster.original == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"model_tuning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}