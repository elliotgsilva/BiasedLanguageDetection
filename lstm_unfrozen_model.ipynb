{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## KAGGLE ONLY\n",
    "# from shutil import copyfile\n",
    "# copyfile(src=\"../input/scriptandpickle/generate_dataloaders.py\", dst=\"../working/generate_dataloaders.py\")\n",
    "# copyfile(src=\"../input/scriptssss/model.py\", dst=\"../working/model.py\")\n",
    "# copyfile(src=\"../input/newevaluation/evaluation.py\", dst=\"../working/evaluation.py\")\n",
    "\n",
    "# copyfile(src=\"../input/newfiles/train_dataloader_lstm.p\", dst=\"../working/train_dataloader_lstm.p\")\n",
    "# copyfile(src=\"../input/newfiles/val_dataloader_lstm.p\", dst=\"../working/val_dataloader_lstm.p\")\n",
    "# copyfile(src=\"../input/newfiles/dictionary_lstm.p\", dst=\"../working/dictionary.p\")\n",
    "# copyfile(src=\"../input/newfiles/train_unlabeled_dataloader_lstm.p\", dst=\"../working/train_unlabelled_dataloader_lstm.p\")\n",
    "# copyfile(src=\"../input/newfiles/train_labeled_dataloader_lstm.p\", dst=\"../working/train_labelled_dataloader_lstm.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zno22FtJPX9z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation' from '/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/evaluation.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from datasets import get_mnist_dataset, get_data_loader\n",
    "#from utils import *\n",
    "#from models import *\n",
    "\n",
    "import pickle as pkl\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from generate_dataloaders import *\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import evaluation\n",
    "import importlib\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaJEVd0wPX94"
   },
   "source": [
    "## Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1029\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nLzh007PX98"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = path + '/'\n",
    "data_dir = path +'/data/' #Uncomment for local system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Verify filenames are consistent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yq-jDGFIPX99"
   },
   "outputs": [],
   "source": [
    "train_loader_labelled = pkl.load(open(data_dir + 'train_labeled_dataloader_lstm.p','rb'))\n",
    "train_loader_unlabelled = pkl.load(open(data_dir + 'train_unlabeled_dataloader_lstm.p','rb'))\n",
    "val_loader = pkl.load(open(data_dir + 'val_dataloader_lstm.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict = pkl.load(open(data_dir + 'dictionary.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install pytorch torchvision -c pytorch\n",
    "## if torch.__version__ is not 1.3.1, run this cell then restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzz8lwNQPX-B",
    "outputId": "690cb77f-2525-4c5a-ea14-a162716e34d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE TRAINED WORD EMBEDDINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(review_dict, embedding_index ,dim = 200):\n",
    "#     embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(review_dict.tokens), dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in review_dict.ids.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOCAL - 2nd line // KAGGLE -- 1st line\n",
    "#glove_twitter = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt' #Change loc for local system\n",
    "glove_twitter = data_dir + 'glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7cbb56036a4ea982424b6aad02add6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_index = load_embeddings(glove_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_index,unknown_words = build_matrix(review_dict, embedding_index)\n",
    "del embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16256"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_dict.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4428"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in unknown_words:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict.get_id('great')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvt6N9QCPX-X"
   },
   "source": [
    "## Neural Network LSTM Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puweJhdxPX-Y"
   },
   "source": [
    "NOTE: Data loader is defined as:\n",
    "- tuple: (tokens, flagged_index, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8BZ-QhNPX-Z"
   },
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM classification model using pretrained glove embeddings\n",
    "    \"\"\"\n",
    "    # NOTE: we can't use linear layer until we take weighted average, otherwise it will\n",
    "    # remember certain positions incorrectly (ie, 4th word has bigger weights vs 7th word)\n",
    "    def __init__(self, opts):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.embedding_matrix = opts['embedding_matrix']\n",
    "        self.vocab_size = self.embedding_matrix.shape[0]\n",
    "        self.embed_size = self.embedding_matrix.shape[1]\n",
    "\n",
    "        self.num_hidden_layers = opts['num_hidden_layers']\n",
    "        self.hidden_size = opts['hidden_size']\n",
    "        self.dropout = opts['dropout']\n",
    "        self.num_classes = 2\n",
    "        self.lambda_loss = opts['lambda_loss']\n",
    "        \n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=0)    \n",
    "        self.embed.weight = nn.Parameter(torch.tensor(self.embedding_matrix, dtype=torch.float32))\n",
    "        self.embed.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size, self.num_hidden_layers, batch_first=True, dropout=self.dropout, bidirectional=True, bias=True)\n",
    "        \n",
    "        self.projection = nn.Linear(2*self.hidden_size, self.num_classes, bias=True)\n",
    "\n",
    "    \n",
    "    def forward(self, tokens, flagged_index):\n",
    "        batch_size, num_tokens = tokens.shape\n",
    "        embedding = self.embed(tokens)\n",
    "#         print(embedding.shape) # below assumes \"batch_size x num_tokens x Emb_dim\" (VERIFY)\n",
    "        \n",
    "        lstm_output = self.lstm(embedding)\n",
    "        # lstm_output is a tuple containing lstm output and (hidden_state, lstm_cell). \n",
    "        # lstm_output[0] would be of shape \"batch_size x num_tokens x hidden_size\" (VERIFY)\n",
    "        \n",
    "        logits = self.projection(lstm_output[0])\n",
    "        # logits would be of shape \"batch_size x num_tokens x num_classes (2)\" (VERIFY)\n",
    "        \n",
    "        batch_size, _, __ = logits.shape\n",
    "        \n",
    "        #selecting the logit at the flagged index\n",
    "        relevant_logits = logits[list(range(batch_size)),flagged_index]\n",
    "        # relevant_logits would be of shape \"batch_size x num_classes (2)\" (VERIFY)\n",
    "        \n",
    "        return relevant_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First performing fully supervised learning using the labelled set to train new vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "opts = {\n",
    "    'embedding_matrix': glove_embedding_index,\n",
    "    'num_hidden_layers': 3,\n",
    "    'hidden_size': 100,\n",
    "    'num_unfrozen_epochs': 0,\n",
    "    'dropout': .1\n",
    "}\n",
    "model = LSTM_model(opts).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "#optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_model(model, criterion, train_loader_labelled, valid_loader, num_frozen_epochs=10, num_unfrozen_epochs=0, path_to_save=None, print_every=1000, debug_mode=False):\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    empty_centroids = torch.tensor([])\n",
    "    # freeze part    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    \n",
    "    for epoch in range(num_frozen_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i,(tokens_labelled, labels, flagged_indices_labelled) in tqdm(enumerate(train_loader_labelled)):\n",
    "            \n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            logits = model(tokens_labelled,flagged_indices_labelled)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add loss to the epoch loss\n",
    "            total_epoch_loss += loss.detach()\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = loss/len(tokens_labelled)\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= len(train_loader_labelled.dataset)\n",
    "        total_epoch_loss = total_epoch_loss.detach()\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            logits = model(tokens,flagged_indices)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += loss\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        if debug_mode:\n",
    "            print('Train result:')\n",
    "            TP_cluster, FP_cluster, _ =evaluation.main(model, empty_centroids, train_loader_labelled, criterion, data_dir, current_device)\n",
    "            print()\n",
    "            print('Validation result:')\n",
    "            TP_cluster, FP_cluster, _ =evaluation.main(model, empty_centroids, valid_loader, criterion, data_dir, current_device)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n",
    "                    \"num_hidden_layers\":model.num_hidden_layers,\\\n",
    "                    \"hidden_size\":model.hidden_size,\\\n",
    "                    \"num_classes\":model.num_classes}\n",
    "            torch.save(model.state_dict(), path_to_save + 'model_dict_labelled.pt')\n",
    "            torch.save(train_losses, path_to_save + 'train_losses_labelled')\n",
    "            torch.save(val_losses, path_to_save + 'val_losses_labelled')\n",
    "            torch.save(opts, path_to_save + 'opts_labelled')\n",
    "\n",
    "    # unfreeze part\n",
    "    unfreeze_model(model)\n",
    "    print(\"*** UNFREEZING ***\")    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    \n",
    "    for epoch in range(num_unfrozen_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.train()\n",
    "        total_epoch_loss = 0\n",
    "\n",
    "        for i,(tokens_labelled, labels, flagged_indices_labelled) in tqdm(enumerate(train_loader_labelled)):\n",
    "            \n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            logits = model(tokens_labelled,flagged_indices_labelled)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "        \n",
    "            # run update step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add loss to the epoch loss\n",
    "            total_epoch_loss += loss.detach()\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = loss/len(tokens_labelled)\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= len(train_loader_labelled.dataset)\n",
    "        total_epoch_loss = total_epoch_loss.detach()\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            logits = model(tokens,flagged_indices)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += loss\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        if debug_mode:\n",
    "            print('Train result:')\n",
    "            TP_cluster, FP_cluster, _ =evaluation.main(model, empty_centroids, train_loader_labelled, criterion, data_dir, current_device)\n",
    "            print()\n",
    "            print('Validation result:')\n",
    "            TP_cluster, FP_cluster, _ =evaluation.main(model, empty_centroids, valid_loader, criterion, data_dir, current_device)\n",
    "        \n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n",
    "                    \"num_hidden_layers\":model.num_hidden_layers,\\\n",
    "                    \"hidden_size\":model.hidden_size,\\\n",
    "                    \"num_classes\":model.num_classes}\n",
    "            torch.save(model.state_dict(), path_to_save + 'model_dict_labelled.pt')\n",
    "            torch.save(train_losses, path_to_save + 'train_losses_labelled')\n",
    "            torch.save(val_losses, path_to_save + 'val_losses_labelled')\n",
    "            torch.save(opts, path_to_save + 'opts_labelled')\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGsqcnEtPX-a"
   },
   "source": [
    "### Clustering Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrgIYm8JPX-b"
   },
   "outputs": [],
   "source": [
    "class KMeansCriterion(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, embeddings, centroids, labelled = False,  cluster_assignments = None):\n",
    "        if labelled:\n",
    "            num_reviews = len(cluster_assignments)\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances = distances[list(range(num_reviews)),cluster_assignments]\n",
    "            loss = cluster_distances.sum()\n",
    "        else:\n",
    "            distances = torch.sum((embeddings[:, None, :] - centroids)**2, 2)\n",
    "            cluster_distances, cluster_assignments = distances.min(1)\n",
    "            loss = cluster_distances.sum()\n",
    "        return loss, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TJohK2aPX-d"
   },
   "outputs": [],
   "source": [
    "def centroid_init(k, d, dataloader, model, current_device):\n",
    "    ## Here we ideally don't want to do randomized/zero initialization\n",
    "    centroid_sums = torch.zeros(k, d).to(current_device)\n",
    "    centroid_counts = torch.zeros(k).to(current_device)\n",
    "    for (tokens, labels, flagged_indices) in dataloader:\n",
    "        # cluster_assignments = torch.LongTensor(tokens.size(0)).random_(k)\n",
    "        cluster_assignments = labels.to(current_device)\n",
    "        \n",
    "        model.eval()\n",
    "        sentence_embed = model(tokens.to(current_device),flagged_indices.to(current_device))\n",
    "    \n",
    "        update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                        cluster_assignments.detach(), sentence_embed.to(current_device).detach())\n",
    "    \n",
    "    centroid_means = centroid_sums / centroid_counts[:, None].to(current_device)\n",
    "    return centroid_means.clone()\n",
    "\n",
    "def update_clusters(centroid_sums, centroid_counts,\n",
    "                    cluster_assignments, embeddings):\n",
    "    k = centroid_sums.size(0)\n",
    "\n",
    "    centroid_sums.index_add_(0, cluster_assignments, embeddings)\n",
    "    bin_counts = torch.bincount(cluster_assignments,minlength=k).type(torch.FloatTensor).to(current_device)\n",
    "    centroid_counts.add_(bin_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_labelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_labelled_iter\n",
    "\n",
    "\n",
    "def loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled):\n",
    "    try:\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "        tokens, labels, flagged_indices = next(train_loader_unlabelled_iter)\n",
    "\n",
    "    return tokens, labels, flagged_indices, train_loader_unlabelled_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3wynM7fPX-h"
   },
   "source": [
    "## Unsupervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clusters(model, centroids, criterion, train_loader_labelled, train_loader_unlabelled, valid_loader, num_epochs=15, num_batches = 1000, path_to_save=None, print_every = 1000):\n",
    "\n",
    "    train_loader_labelled_iter = iter(train_loader_labelled)\n",
    "    train_loader_unlabelled_iter = iter(train_loader_unlabelled)\n",
    "    lambda_loss = model.lambda_loss\n",
    "\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        current_device = 'cuda'\n",
    "    else:\n",
    "        current_device = 'cpu'\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01, amsgrad=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('{} | Epoch {}'.format(dt.datetime.now(), epoch))\n",
    "        model.eval() # we're only clustering, not training model\n",
    "        k, d = centroids.size()\n",
    "        centroid_sums = torch.zeros_like(centroids).to(current_device)\n",
    "        centroid_counts = torch.zeros(k).to(current_device)\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        for i in tqdm(range(int(num_batches))):\n",
    "            tokens_labelled, labels, flagged_indices_labelled, train_loader_labelled_iter = loadLabelledBatch(train_loader_labelled_iter, train_loader_labelled)\n",
    "            tokens_unlabelled, _, flagged_indices_unlabelled, train_loader_unlabelled_iter = loadUnlabelledBatch(train_loader_unlabelled_iter, train_loader_unlabelled)\n",
    "\n",
    "            tokens_labelled = tokens_labelled.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices_labelled = flagged_indices_labelled.to(current_device)\n",
    "            \n",
    "            tokens_unlabelled = tokens_unlabelled.to(current_device)\n",
    "            flagged_indices_unlabelled = flagged_indices_unlabelled.to(current_device)\n",
    "\n",
    "            # forward pass and compute loss\n",
    "            sentence_embed_labelled = model(tokens_labelled,flagged_indices_labelled)\n",
    "            sentence_embed_unlabelled = model(tokens_unlabelled,flagged_indices_unlabelled)\n",
    "            \n",
    "            cluster_loss_unlabelled, cluster_assignments_unlabelled = criterion(sentence_embed_unlabelled, centroids.detach())\n",
    "            cluster_loss_labelled, cluster_assignments_labelled = criterion(sentence_embed_labelled, centroids.detach(), labelled = True, cluster_assignments = labels)\n",
    "    \n",
    "            total_batch_loss = cluster_loss_unlabelled.data + lambda_loss * cluster_loss_labelled.data\n",
    "            \n",
    "#             #Add loss to the epoch loss\n",
    "            total_epoch_loss += total_batch_loss.data\n",
    "\n",
    "#             # store centroid sums and counts in memory for later centering\n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_labelled.detach(), sentence_embed_labelled.detach())\n",
    "    \n",
    "            update_clusters(centroid_sums.detach(), centroid_counts.detach(),\n",
    "                            cluster_assignments_unlabelled.detach(), sentence_embed_unlabelled.detach())\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                losses = total_batch_loss/(len(tokens_labelled)+ len(tokens_unlabelled))\n",
    "                print('Average training loss at batch ',i,': %.3f' % losses)\n",
    "            \n",
    "        total_epoch_loss /= (len(train_loader_labelled.dataset)+len(train_loader_unlabelled.dataset))\n",
    "        train_losses.append(total_epoch_loss)\n",
    "        print('Average training loss after epoch ',epoch,': %.3f' % total_epoch_loss)\n",
    "        \n",
    "        # update centroids based on assignments from autoencoders\n",
    "        centroids = centroid_sums / (centroid_counts[:, None] + 1).to(current_device)\n",
    "        \n",
    "        # calculate validation loss after every epoch\n",
    "        total_validation_loss = 0\n",
    "        for i, (tokens, labels, flagged_indices) in enumerate(valid_loader):\n",
    "            model.eval()\n",
    "            tokens = tokens.to(current_device)\n",
    "            labels = labels.to(current_device)\n",
    "            flagged_indices = flagged_indices.to(current_device)\n",
    "            \n",
    "            # forward pass and compute loss\n",
    "            sentence_embed = model(tokens,flagged_indices)\n",
    "            cluster_loss, cluster_assignments = criterion(sentence_embed, centroids)\n",
    "            \n",
    "            #Add loss to the validation loss\n",
    "            total_validation_loss += cluster_loss.data\n",
    "\n",
    "        total_validation_loss /= len(valid_loader.dataset)\n",
    "        val_losses.append(total_validation_loss)\n",
    "        print('Average validation loss after epoch ',epoch,': %.3f' % total_validation_loss)\n",
    "        \n",
    "        if path_to_save == None:\n",
    "            pass\n",
    "        else:\n",
    "            opts = {\"embedding_matrix\":model.embedding_matrix,\\\n",
    "                    \"num_hidden_layers\":model.num_hidden_layers,\\\n",
    "                    \"hidden_size\":model.hidden_size,\\\n",
    "                    \"num_classes\":model.num_classes}\n",
    "            torch.save(model.state_dict(), path_to_save+'model_dict_unlabelled.pt')\n",
    "            torch.save(centroids, path_to_save+'centroids_unlabelled')\n",
    "            torch.save(train_losses, path_to_save+'train_losses_unlabelled')\n",
    "            torch.save(val_losses, path_to_save+'val_losses_unlabelled')\n",
    "            torch.save(opts, path_to_save+'opts_unlabelled')\n",
    "        \n",
    "    return model, centroids, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_directory(opts):\n",
    "    path = os.getcwd()\n",
    "    model_folder = 'lstm_unfrozen_model/'\n",
    "    model_dir = path + '/models/' + model_folder\n",
    "    \n",
    "    # subfolder for each hyperparam config\n",
    "    num_unfrozen_epochs = opts['num_unfrozen_epochs']\n",
    "    num_hidden_layers = opts['num_hidden_layers']\n",
    "    hidden_size = opts['hidden_size']\n",
    "    dropout = opts['dropout']\n",
    "    lambda_loss = opts['lambda_loss']\n",
    "    subfolder = \"num_unfrozen_epochs=\"+str(num_unfrozen_epochs) \\\n",
    "                + \",num_hidden_layers=\"+str(num_hidden_layers) \\\n",
    "                + \",hidden_size=\"+str(hidden_size) \\\n",
    "                + \",dropout=\"+str(dropout) \\\n",
    "                + \",lambda=\"+str(lambda_loss) + '/'\n",
    "    \n",
    "    # need to actually create these subfolders lol\n",
    "    try:\n",
    "        os.makedirs(model_dir + subfolder) # will throw error if subfolder already exists\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return model_dir + subfolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_config_supervised(opts):\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    print(path_to_save)\n",
    "    \n",
    "    # supervised part -- embeddings\n",
    "    model = LSTM_model(opts).to(current_device)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    num_unfrozen_epochs = opts['num_unfrozen_epochs']\n",
    "    train_supervised_model(model, criterion, train_loader_labelled, val_loader, num_unfrozen_epochs=num_unfrozen_epochs, path_to_save=path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "2019-12-09 00:28:58.871606 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813bf340a0ca4417b41a199c03bf58c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.681\n",
      "\n",
      "Average training loss after epoch  0 : 0.305\n",
      "Average validation loss after epoch  0 : 0.285\n",
      "2019-12-09 00:29:08.868317 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f489e4052bf4c06a7a2164de2660cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.290\n",
      "\n",
      "Average training loss after epoch  1 : 0.153\n",
      "Average validation loss after epoch  1 : 0.345\n",
      "2019-12-09 00:29:18.431704 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e7b9dba82f48eab6af0d4a6e701079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.156\n",
      "\n",
      "Average training loss after epoch  2 : 0.071\n",
      "Average validation loss after epoch  2 : 0.428\n",
      "2019-12-09 00:29:27.717276 | Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2fc179e0af41348b23ac406d07daff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.024\n",
      "\n",
      "Average training loss after epoch  3 : 0.043\n",
      "Average validation loss after epoch  3 : 0.550\n",
      "2019-12-09 00:29:37.047992 | Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83cf32b5e8c64e75abf7da1f9b2addac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.013\n",
      "\n",
      "Average training loss after epoch  4 : 0.028\n",
      "Average validation loss after epoch  4 : 0.588\n",
      "2019-12-09 00:29:46.375049 | Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efa77fb35174f128777750714f090ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.007\n",
      "\n",
      "Average training loss after epoch  5 : 0.019\n",
      "Average validation loss after epoch  5 : 0.677\n",
      "2019-12-09 00:29:55.664964 | Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1137ecd01d3241cd92a3b8fb22a9941e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.020\n",
      "\n",
      "Average training loss after epoch  6 : 0.018\n",
      "Average validation loss after epoch  6 : 0.639\n",
      "2019-12-09 00:30:04.997294 | Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1134c025804a9d8b6c7923c289342d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.002\n",
      "\n",
      "Average training loss after epoch  7 : 0.029\n",
      "Average validation loss after epoch  7 : 0.569\n",
      "2019-12-09 00:30:14.361884 | Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2482d1c5ed4b029274356191296adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.090\n",
      "\n",
      "Average training loss after epoch  8 : 0.098\n",
      "Average validation loss after epoch  8 : 0.468\n",
      "2019-12-09 00:30:23.736264 | Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bc8186539c47e6a9ecd48b57368e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.014\n",
      "\n",
      "Average training loss after epoch  9 : 0.040\n",
      "Average validation loss after epoch  9 : 0.551\n",
      "2019-12-09 00:30:33.063394 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800cab16bd0c4388b0dead3d7ad4edea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.011\n",
      "\n",
      "Average training loss after epoch  0 : 0.050\n",
      "Average validation loss after epoch  0 : 0.630\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "2019-12-09 00:30:48.780527 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e6e9f5c4de4caca86692f9c4e76fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.693\n",
      "\n",
      "Average training loss after epoch  0 : 0.310\n",
      "Average validation loss after epoch  0 : 0.336\n",
      "2019-12-09 00:30:58.060573 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abfccce53f141199243960780f63889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.225\n",
      "\n",
      "Average training loss after epoch  1 : 0.162\n",
      "Average validation loss after epoch  1 : 0.309\n",
      "2019-12-09 00:31:07.411366 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9a7e773316407c9416e51874392007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.049\n",
      "\n",
      "Average training loss after epoch  2 : 0.081\n",
      "Average validation loss after epoch  2 : 0.414\n",
      "2019-12-09 00:31:16.680821 | Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b0a8f35765473e80981253471c0526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.093\n",
      "\n",
      "Average training loss after epoch  3 : 0.040\n",
      "Average validation loss after epoch  3 : 0.459\n",
      "2019-12-09 00:31:26.924357 | Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100fbaeca9944f1282885dab078d1a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.043\n",
      "\n",
      "Average training loss after epoch  4 : 0.033\n",
      "Average validation loss after epoch  4 : 0.633\n",
      "2019-12-09 00:31:36.995673 | Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9182fbe1a7454dde96f374649a47bfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.003\n",
      "\n",
      "Average training loss after epoch  5 : 0.020\n",
      "Average validation loss after epoch  5 : 0.522\n",
      "2019-12-09 00:31:46.349656 | Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb4c28e6595432b8321ef5191ed23c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.004\n",
      "\n",
      "Average training loss after epoch  6 : 0.017\n",
      "Average validation loss after epoch  6 : 0.581\n",
      "2019-12-09 00:31:57.015397 | Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954c6817a3224870b04dd284d4f2ea89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.001\n",
      "\n",
      "Average training loss after epoch  7 : 0.013\n",
      "Average validation loss after epoch  7 : 0.638\n",
      "2019-12-09 00:32:06.781533 | Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb917f608544212bb98e61ae1eaa417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.001\n",
      "\n",
      "Average training loss after epoch  8 : 0.010\n",
      "Average validation loss after epoch  8 : 0.621\n",
      "2019-12-09 00:32:16.092525 | Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5de0c3e887849a7a3efdeff034d1dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.001\n",
      "\n",
      "Average training loss after epoch  9 : 0.007\n",
      "Average validation loss after epoch  9 : 0.659\n",
      "2019-12-09 00:32:25.654975 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6480b577c7481a886b32e89eaa715d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.003\n",
      "\n",
      "Average training loss after epoch  0 : 0.062\n",
      "Average validation loss after epoch  0 : 0.702\n",
      "2019-12-09 00:32:41.158434 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30692136bf94d92a67aac5a4261bfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 0.048\n",
      "\n",
      "Average training loss after epoch  1 : 0.025\n",
      "Average validation loss after epoch  1 : 0.721\n"
     ]
    }
   ],
   "source": [
    "#num_hidden_layers_list = [0, 1, 2]\n",
    "num_hidden_layers_list = [1]\n",
    "#hidden_sizes = [128, 256, 512]\n",
    "hidden_sizes = [128]\n",
    "#dropouts = [0, .2, .4, .6, .8]\n",
    "dropouts = [0]\n",
    "num_unfrozen_epochs_list = [1, 2]\n",
    "lambda_loss = None  # NOT TRAINING THIS YET\n",
    "\n",
    "for num_hidden_layers in num_hidden_layers_list:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for dropout in dropouts:\n",
    "            for num_unfrozen_epochs in num_unfrozen_epochs_list:\n",
    "                opts = {\n",
    "                    'embedding_matrix': glove_embedding_index,\n",
    "                    'num_hidden_layers': num_hidden_layers,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'dropout': dropout,\n",
    "                    'num_unfrozen_epochs': num_unfrozen_epochs,\n",
    "                    'lambda_loss': lambda_loss\n",
    "                }\n",
    "                train_config_supervised(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Unsupervised / Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_config_unsupervised(opts):    \n",
    "    # get load directory\n",
    "    opts_load = opts.copy()\n",
    "    opts_load['lambda_loss'] = None\n",
    "    path_to_load = get_save_directory(opts_load)\n",
    "    print(path_to_load)\n",
    "    \n",
    "    # load \n",
    "    model = LSTM_model(opts)\n",
    "    model.load_state_dict(torch.load(path_to_load+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\n",
    "    model = model.to(current_device)\n",
    "    \n",
    "    # get save directory\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    print(path_to_save)\n",
    "    \n",
    "    # unsupervised part -- assign clusters to unlabelled data\n",
    "    model.projection = nn.Identity()\n",
    "    centroids = centroid_init(2, 2*model.hidden_size, train_loader_labelled, model, current_device)\n",
    "    criterion = KMeansCriterion().to(current_device)    \n",
    "    num_batches = int(len(train_loader_unlabelled.dataset)/train_loader_unlabelled.batch_size)+1\n",
    "    train_clusters(model, centroids, criterion, train_loader_labelled, train_loader_unlabelled, val_loader, num_epochs=3, num_batches=num_batches, path_to_save=path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=0.1/\n",
      "2019-12-09 00:36:08.576457 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ba1af33c174078b14f1a1206e4ad96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 10.458\n",
      "Average training loss at batch  1000 : 10.298\n",
      "Average training loss at batch  2000 : 10.177\n",
      "Average training loss at batch  3000 : 10.791\n",
      "\n",
      "Average training loss after epoch  0 : 20.362\n",
      "Average validation loss after epoch  0 : 20.678\n",
      "2019-12-09 00:37:49.385312 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2511130b94774b3b95d2e6a45ef0a368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 8.557\n",
      "Average training loss at batch  1000 : 11.090\n",
      "Average training loss at batch  2000 : 10.576\n",
      "Average training loss at batch  3000 : 10.173\n",
      "\n",
      "Average training loss after epoch  1 : 20.218\n",
      "Average validation loss after epoch  1 : 20.662\n",
      "2019-12-09 00:39:21.361801 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdb3e6e3ed74e11a64eaeb36f50251a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 10.775\n",
      "Average training loss at batch  1000 : 10.020\n",
      "Average training loss at batch  2000 : 11.811\n",
      "Average training loss at batch  3000 : 10.806\n",
      "\n",
      "Average training loss after epoch  2 : 20.197\n",
      "Average validation loss after epoch  2 : 20.657\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=0.5/\n",
      "2019-12-09 00:40:56.720367 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d480de9a45495387408c574118ed35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 13.642\n",
      "Average training loss at batch  1000 : 14.347\n",
      "Average training loss at batch  2000 : 15.805\n",
      "Average training loss at batch  3000 : 15.718\n",
      "\n",
      "Average training loss after epoch  0 : 28.105\n",
      "Average validation loss after epoch  0 : 20.678\n",
      "2019-12-09 00:42:37.763666 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b9a7b9386e46db950f82cb2b51bb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 15.609\n",
      "Average training loss at batch  1000 : 14.709\n",
      "Average training loss at batch  2000 : 15.290\n",
      "Average training loss at batch  3000 : 14.056\n",
      "\n",
      "Average training loss after epoch  1 : 27.979\n",
      "Average validation loss after epoch  1 : 20.662\n",
      "2019-12-09 00:44:09.456702 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaadf887e2c409f925d891d1c11e71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 14.643\n",
      "Average training loss at batch  1000 : 16.235\n",
      "Average training loss at batch  2000 : 13.916\n",
      "Average training loss at batch  3000 : 14.994\n",
      "\n",
      "Average training loss after epoch  2 : 27.968\n",
      "Average validation loss after epoch  2 : 20.658\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=1/\n",
      "2019-12-09 00:45:48.256897 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55ca9bd680341f4b0e5748d7ab77e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 19.272\n",
      "Average training loss at batch  1000 : 19.966\n",
      "Average training loss at batch  2000 : 20.129\n",
      "Average training loss at batch  3000 : 19.259\n",
      "\n",
      "Average training loss after epoch  0 : 37.785\n",
      "Average validation loss after epoch  0 : 20.678\n",
      "2019-12-09 00:47:26.238151 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cf850f7ce3433a8612eee9040b6233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 22.167\n",
      "Average training loss at batch  1000 : 20.611\n",
      "Average training loss at batch  2000 : 19.633\n",
      "Average training loss at batch  3000 : 19.722\n",
      "\n",
      "Average training loss after epoch  1 : 37.685\n",
      "Average validation loss after epoch  1 : 20.662\n",
      "2019-12-09 00:48:54.648849 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78053bacc6c4d90b30473d4de043f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 20.790\n",
      "Average training loss at batch  1000 : 20.635\n",
      "Average training loss at batch  2000 : 19.039\n",
      "Average training loss at batch  3000 : 21.841\n",
      "\n",
      "Average training loss after epoch  2 : 37.673\n",
      "Average validation loss after epoch  2 : 20.658\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=5/\n",
      "2019-12-09 00:50:25.168589 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf50067abf6480ca16a0b301d2f9a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 64.098\n",
      "Average training loss at batch  1000 : 62.903\n",
      "Average training loss at batch  2000 : 68.055\n",
      "Average training loss at batch  3000 : 61.556\n",
      "\n",
      "Average training loss after epoch  0 : 115.213\n",
      "Average validation loss after epoch  0 : 20.678\n",
      "2019-12-09 00:51:57.799536 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db70f5f0339c40a08140f46a1205aba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 65.160\n",
      "Average training loss at batch  1000 : 60.811\n",
      "Average training loss at batch  2000 : 59.951\n",
      "Average training loss at batch  3000 : 61.458\n",
      "\n",
      "Average training loss after epoch  1 : 115.280\n",
      "Average validation loss after epoch  1 : 20.662\n",
      "2019-12-09 00:53:25.894222 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408e033a597e4a85a500c36d66fb7fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 60.373\n",
      "Average training loss at batch  1000 : 55.099\n",
      "Average training loss at batch  2000 : 57.614\n",
      "Average training loss at batch  3000 : 63.965\n",
      "\n",
      "Average training loss after epoch  2 : 115.397\n",
      "Average validation loss after epoch  2 : 20.657\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=10/\n",
      "2019-12-09 00:54:59.096287 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf95a1f30e4244acadffda0c6a44a1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 109.407\n",
      "Average training loss at batch  1000 : 112.742\n",
      "Average training loss at batch  2000 : 99.766\n",
      "Average training loss at batch  3000 : 118.571\n",
      "\n",
      "Average training loss after epoch  0 : 211.999\n",
      "Average validation loss after epoch  0 : 20.678\n",
      "2019-12-09 00:56:33.597416 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77319b4a6f0e4417bab6fc3dccc95478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 114.171\n",
      "Average training loss at batch  1000 : 117.940\n",
      "Average training loss at batch  2000 : 113.929\n",
      "Average training loss at batch  3000 : 108.838\n",
      "\n",
      "Average training loss after epoch  1 : 212.346\n",
      "Average validation loss after epoch  1 : 20.662\n",
      "2019-12-09 00:58:03.836690 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d92711db8d04037ae3d09095d611d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 113.033\n",
      "Average training loss at batch  1000 : 109.292\n",
      "Average training loss at batch  2000 : 117.583\n",
      "Average training loss at batch  3000 : 112.066\n",
      "\n",
      "Average training loss after epoch  2 : 212.503\n",
      "Average validation loss after epoch  2 : 20.657\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=1,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=25/\n",
      "2019-12-09 00:59:34.988257 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b1d9f801434f1687e80ec522006e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 261.934\n",
      "Average training loss at batch  1000 : 260.648\n",
      "Average training loss at batch  2000 : 277.752\n",
      "Average training loss at batch  3000 : 262.470\n",
      "\n",
      "Average training loss after epoch  0 : 502.339\n",
      "Average validation loss after epoch  0 : 20.678\n",
      "2019-12-09 01:01:07.076695 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70f09c3585d4690932340b87bc06549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 300.714\n",
      "Average training loss at batch  1000 : 266.296\n",
      "Average training loss at batch  2000 : 251.443\n",
      "Average training loss at batch  3000 : 287.236\n",
      "\n",
      "Average training loss after epoch  1 : 503.437\n",
      "Average validation loss after epoch  1 : 20.662\n",
      "2019-12-09 01:02:34.930097 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303f49d6b6de49dbb7c937d43be2c788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 264.626\n",
      "Average training loss at batch  1000 : 257.472\n",
      "Average training loss at batch  2000 : 262.942\n",
      "Average training loss at batch  3000 : 261.561\n",
      "\n",
      "Average training loss after epoch  2 : 503.842\n",
      "Average validation loss after epoch  2 : 20.658\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=0.1/\n",
      "2019-12-09 01:04:05.423199 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc1bd1b632f4f7aa8336dbffe8c8a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 10.092\n",
      "Average training loss at batch  1000 : 9.752\n",
      "Average training loss at batch  2000 : 12.706\n",
      "Average training loss at batch  3000 : 11.478\n",
      "\n",
      "Average training loss after epoch  0 : 20.333\n",
      "Average validation loss after epoch  0 : 20.933\n",
      "2019-12-09 01:05:37.321527 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af74320204d486183381878ce3014ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 10.196\n",
      "Average training loss at batch  1000 : 11.106\n",
      "Average training loss at batch  2000 : 11.360\n",
      "Average training loss at batch  3000 : 10.336\n",
      "\n",
      "Average training loss after epoch  1 : 20.205\n",
      "Average validation loss after epoch  1 : 20.926\n",
      "2019-12-09 01:07:05.007493 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa98b4129a94ba8b0ae187ce6e82e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 11.797\n",
      "Average training loss at batch  1000 : 10.429\n",
      "Average training loss at batch  2000 : 11.278\n",
      "Average training loss at batch  3000 : 11.364\n",
      "\n",
      "Average training loss after epoch  2 : 20.196\n",
      "Average validation loss after epoch  2 : 20.924\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=0.5/\n",
      "2019-12-09 01:08:35.856179 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e57017c234d49989ef89e83252efc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 14.143\n",
      "Average training loss at batch  1000 : 16.244\n",
      "Average training loss at batch  2000 : 14.469\n",
      "Average training loss at batch  3000 : 15.278\n",
      "\n",
      "Average training loss after epoch  0 : 28.296\n",
      "Average validation loss after epoch  0 : 20.933\n",
      "2019-12-09 01:10:08.364955 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75042b5e9f34f3289b30b6ad94f50f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 14.145\n",
      "Average training loss at batch  1000 : 15.592\n",
      "Average training loss at batch  2000 : 15.177\n",
      "Average training loss at batch  3000 : 14.386\n",
      "\n",
      "Average training loss after epoch  1 : 28.186\n",
      "Average validation loss after epoch  1 : 20.926\n",
      "2019-12-09 01:11:36.722506 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d0dfe068834a13bb295eabfb85d400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 15.770\n",
      "Average training loss at batch  1000 : 16.713\n",
      "Average training loss at batch  2000 : 15.918\n",
      "Average training loss at batch  3000 : 13.550\n",
      "\n",
      "Average training loss after epoch  2 : 28.180\n",
      "Average validation loss after epoch  2 : 20.924\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=1/\n",
      "2019-12-09 01:13:08.083324 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823fb2fd3eab40a58835f7a3d4ba7b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 21.400\n",
      "Average training loss at batch  1000 : 21.675\n",
      "Average training loss at batch  2000 : 21.894\n",
      "Average training loss at batch  3000 : 20.666\n",
      "\n",
      "Average training loss after epoch  0 : 38.251\n",
      "Average validation loss after epoch  0 : 20.933\n",
      "2019-12-09 01:14:40.582917 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55870dab646146488414e1bba4d48b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 17.973\n",
      "Average training loss at batch  1000 : 19.087\n",
      "Average training loss at batch  2000 : 20.538\n",
      "Average training loss at batch  3000 : 20.766\n",
      "\n",
      "Average training loss after epoch  1 : 38.159\n",
      "Average validation loss after epoch  1 : 20.925\n",
      "2019-12-09 01:16:10.307604 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a6ff2ffea44aa49df1bcf528db0b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 18.848\n",
      "Average training loss at batch  1000 : 21.696\n",
      "Average training loss at batch  2000 : 21.045\n",
      "Average training loss at batch  3000 : 21.062\n",
      "\n",
      "Average training loss after epoch  2 : 38.158\n",
      "Average validation loss after epoch  2 : 20.924\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=5/\n",
      "2019-12-09 01:17:41.319883 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018ad617f9a04a5c95070ac070bad33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 63.229\n",
      "Average training loss at batch  1000 : 59.722\n",
      "Average training loss at batch  2000 : 65.646\n",
      "Average training loss at batch  3000 : 63.266\n",
      "\n",
      "Average training loss after epoch  0 : 117.873\n",
      "Average validation loss after epoch  0 : 20.933\n",
      "2019-12-09 01:19:09.967835 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be370f04784944029393cb5fb9e6c982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 65.676\n",
      "Average training loss at batch  1000 : 62.451\n",
      "Average training loss at batch  2000 : 64.659\n",
      "Average training loss at batch  3000 : 62.502\n",
      "\n",
      "Average training loss after epoch  1 : 117.984\n",
      "Average validation loss after epoch  1 : 20.925\n",
      "2019-12-09 01:20:34.144908 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235737ad9a7846238a6941d5a6b1a506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 56.239\n",
      "Average training loss at batch  1000 : 66.258\n",
      "Average training loss at batch  2000 : 62.828\n",
      "Average training loss at batch  3000 : 69.486\n",
      "\n",
      "Average training loss after epoch  2 : 117.985\n",
      "Average validation loss after epoch  2 : 20.924\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=10/\n",
      "2019-12-09 01:22:04.947643 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e7e8e56e3746b19b864a9240a8b31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 113.278\n",
      "Average training loss at batch  1000 : 114.374\n",
      "Average training loss at batch  2000 : 110.533\n",
      "Average training loss at batch  3000 : 117.456\n",
      "\n",
      "Average training loss after epoch  0 : 217.400\n",
      "Average validation loss after epoch  0 : 20.933\n",
      "2019-12-09 01:23:38.276124 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdec0340b43d4842842b5a917ab30eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 124.965\n",
      "Average training loss at batch  1000 : 121.659\n",
      "Average training loss at batch  2000 : 110.655\n",
      "Average training loss at batch  3000 : 105.254\n",
      "\n",
      "Average training loss after epoch  1 : 217.742\n",
      "Average validation loss after epoch  1 : 20.926\n",
      "2019-12-09 01:25:08.835685 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b1ff7f88d9448bafd1d9036bf4c219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 122.250\n",
      "Average training loss at batch  1000 : 108.827\n",
      "Average training loss at batch  2000 : 109.424\n",
      "Average training loss at batch  3000 : 116.583\n",
      "\n",
      "Average training loss after epoch  2 : 217.841\n",
      "Average validation loss after epoch  2 : 20.924\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=None/\n",
      "/Users/elliotsilva/Desktop/DS-GA-1006/FairFrame/models/lstm_unfrozen_model/num_unfrozen_epochs=2,num_hidden_layers=1,hidden_size=128,dropout=0,lambda=25/\n",
      "2019-12-09 01:26:44.184437 | Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4573641324f64e0da7b614df8872bc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 258.041\n",
      "Average training loss at batch  1000 : 238.021\n",
      "Average training loss at batch  2000 : 280.207\n",
      "Average training loss at batch  3000 : 268.174\n",
      "\n",
      "Average training loss after epoch  0 : 515.960\n",
      "Average validation loss after epoch  0 : 20.933\n",
      "2019-12-09 01:28:22.534709 | Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc8dee37296416496b288fbf05ec3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 282.869\n",
      "Average training loss at batch  1000 : 269.541\n",
      "Average training loss at batch  2000 : 284.544\n",
      "Average training loss at batch  3000 : 291.659\n",
      "\n",
      "Average training loss after epoch  1 : 516.999\n",
      "Average validation loss after epoch  1 : 20.925\n",
      "2019-12-09 01:29:55.711269 | Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4323448ee43148088c88f7157842b2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3191), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss at batch  0 : 285.559\n",
      "Average training loss at batch  1000 : 300.334\n",
      "Average training loss at batch  2000 : 272.638\n",
      "Average training loss at batch  3000 : 258.166\n",
      "\n",
      "Average training loss after epoch  2 : 517.236\n",
      "Average validation loss after epoch  2 : 20.924\n"
     ]
    }
   ],
   "source": [
    "num_hidden_layers = 1  # BEST PARAM\n",
    "hidden_size = 128  # BEST PARAM\n",
    "dropout = 0  # BEST PARAM\n",
    "num_unfrozen_epochs = [0, 1, 2]\n",
    "lambda_losses = [.1, .5, 1, 5, 10, 25]\n",
    "\n",
    "for num_unfrozen_epochs in num_unfrozen_epochs_list:\n",
    "    for lambda_loss in lambda_losses:\n",
    "        opts = {\n",
    "            'embedding_matrix': glove_embedding_index,\n",
    "            'num_hidden_layers': num_hidden_layers,\n",
    "            'hidden_size': hidden_size,\n",
    "            'dropout': dropout,\n",
    "            'num_unfrozen_epochs': num_unfrozen_epochs,\n",
    "            'lambda_loss': lambda_loss\n",
    "        }\n",
    "        train_config_unsupervised(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_config_supervised(opts,verbose=True):\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    #print(path_to_save)\n",
    "    \n",
    "    model = LSTM_model(opts) #change here depending on model\n",
    "    model.load_state_dict(torch.load(path_to_save+'model_dict_labelled.pt',map_location=lambda storage, loc: storage))\n",
    "    model = model.to(current_device)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    criterion = criterion.to(current_device)\n",
    "    \n",
    "    empty_centroids = torch.tensor([])\n",
    "    TP_cluster, FP_cluster, results_dict = evaluation.main(model, empty_centroids, val_loader, criterion, data_dir, current_device, verbose)\n",
    "    results_dict.update(opts)\n",
    "    return TP_cluster, FP_cluster, results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_hidden_layers_list = [0, 1, 2]\n",
    "num_hidden_layers_list = [1]\n",
    "#hidden_sizes = [128, 256, 512]\n",
    "hidden_sizes = [128]\n",
    "#dropouts = [0, .2, .4, .6, .8]\n",
    "dropouts = [0]\n",
    "num_unfrozen_epochs_list = [0,1,2]\n",
    "lambda_loss = None  # NOT TRAINING THIS YET\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "for num_hidden_layers in num_hidden_layers_list:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for dropout in dropouts:\n",
    "            for num_unfrozen_epochs in num_unfrozen_epochs_list:\n",
    "                opts = {\n",
    "                    'embedding_matrix': glove_embedding_index,\n",
    "                    'num_hidden_layers': num_hidden_layers,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'dropout': dropout,\n",
    "                    'num_unfrozen_epochs': num_unfrozen_epochs,\n",
    "                    'lambda_loss': lambda_loss\n",
    "                }\n",
    "                _, _, results_dict = evaluate_config_supervised(opts,False)\n",
    "                results_df = results_df.append(results_dict,ignore_index=True)\n",
    "                \n",
    "results_df = results_df[['num_hidden_layers','hidden_size','dropout','num_unfrozen_epochs','Accuracy','F1 score','Precision','Recall',\n",
    "                        'TP_rate','FP_rate','FN_rate','TN_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_hidden_layers</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>num_unfrozen_epochs</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>TP_rate</th>\n",
       "      <th>FP_rate</th>\n",
       "      <th>FN_rate</th>\n",
       "      <th>TN_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791677</td>\n",
       "      <td>0.817824</td>\n",
       "      <td>0.935205</td>\n",
       "      <td>0.726623</td>\n",
       "      <td>0.935205</td>\n",
       "      <td>0.064795</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.648148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784668</td>\n",
       "      <td>0.811883</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.720787</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.070664</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.781915</td>\n",
       "      <td>0.809302</td>\n",
       "      <td>0.925532</td>\n",
       "      <td>0.719008</td>\n",
       "      <td>0.925532</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.638298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_hidden_layers  hidden_size  dropout  num_unfrozen_epochs  Accuracy  \\\n",
       "0  1.0                128.0        0.0      0.0                  0.791677   \n",
       "1  1.0                128.0        0.0      1.0                  0.784668   \n",
       "2  1.0                128.0        0.0      2.0                  0.781915   \n",
       "\n",
       "   F1 score  Precision    Recall   TP_rate   FP_rate   FN_rate   TN_rate  \n",
       "0  0.817824  0.935205   0.726623  0.935205  0.064795  0.351852  0.648148  \n",
       "1  0.811883  0.929336   0.720787  0.929336  0.070664  0.360000  0.640000  \n",
       "2  0.809302  0.925532   0.719008  0.925532  0.074468  0.361702  0.638298  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Clustering / Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_config_supervised(opts,verbose=True):\n",
    "    path_to_save = get_save_directory(opts)\n",
    "    #print(path_to_save)\n",
    "    \n",
    "    model = LSTM_model(opts) #change here depending on model\n",
    "    model.projection = nn.Identity()\n",
    "    model.load_state_dict(torch.load(path_to_save+'model_dict_unlabelled.pt',map_location=lambda storage, loc: storage))\n",
    "    model = model.to(current_device)\n",
    "    criterion = KMeansCriterion()\n",
    "    criterion = criterion.to(current_device)\n",
    "    centroids = torch.load(path_to_save+'centroids_unlabelled',map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    TP_cluster, FP_cluster, results_dict = evaluation.main(model, centroids, val_loader, criterion, data_dir, current_device, verbose)\n",
    "    results_dict.update(opts)\n",
    "    return TP_cluster, FP_cluster, results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 1  # BEST PARAM\n",
    "hidden_size = 128  # BEST PARAM\n",
    "dropout = 0  # BEST PARAM\n",
    "num_unfrozen_epoch = 0  # BEST PARAM\n",
    "lambda_losses = [.1, .5, 1, 5, 10, 25]\n",
    "\n",
    "results_df2 = results_df.copy()\n",
    "for lambda_loss in lambda_losses:\n",
    "    opts = {\n",
    "        'embedding_matrix': glove_embedding_index,\n",
    "        'num_hidden_layers': num_hidden_layers,\n",
    "        'hidden_size': hidden_size,\n",
    "        'dropout': dropout,\n",
    "        'num_unfrozen_epochs': num_unfrozen_epochs,\n",
    "        'lambda_loss': lambda_loss\n",
    "    }\n",
    "    _, _, results_dict = evaluate_config_supervised(opts,False)\n",
    "    results_df = results_df.append(results_dict,ignore_index=True)\n",
    "                \n",
    "results_df2 = results_df[['lambda_loss','num_hidden_layers','hidden_size','dropout','num_unfrozen_epochs','Accuracy','F1 score','Precision','Recall',\n",
    "                        'TP_rate','FP_rate','FN_rate','TN_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_loss</th>\n",
       "      <th>num_hidden_layers</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>num_unfrozen_epochs</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>TP_rate</th>\n",
       "      <th>FP_rate</th>\n",
       "      <th>FN_rate</th>\n",
       "      <th>TN_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791677</td>\n",
       "      <td>0.817824</td>\n",
       "      <td>0.935205</td>\n",
       "      <td>0.726623</td>\n",
       "      <td>0.935205</td>\n",
       "      <td>0.064795</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.648148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784668</td>\n",
       "      <td>0.811883</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.720787</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.070664</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.781915</td>\n",
       "      <td>0.809302</td>\n",
       "      <td>0.925532</td>\n",
       "      <td>0.719008</td>\n",
       "      <td>0.925532</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.638298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.747522</td>\n",
       "      <td>0.790355</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.675719</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.048165</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.543210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.747522</td>\n",
       "      <td>0.790355</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.675719</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.048165</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.543210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.747522</td>\n",
       "      <td>0.790355</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.675719</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.048165</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.543210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.747522</td>\n",
       "      <td>0.790355</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.675719</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.048165</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.543210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.747522</td>\n",
       "      <td>0.790355</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.675719</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.048165</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.543210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.747522</td>\n",
       "      <td>0.790355</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.675719</td>\n",
       "      <td>0.951835</td>\n",
       "      <td>0.048165</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.543210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_loss  num_hidden_layers  hidden_size  dropout  num_unfrozen_epochs  \\\n",
       "0 NaN           1.0                128.0        0.0      0.0                   \n",
       "1 NaN           1.0                128.0        0.0      1.0                   \n",
       "2 NaN           1.0                128.0        0.0      2.0                   \n",
       "3  0.1          1.0                128.0        0.0      2.0                   \n",
       "4  0.5          1.0                128.0        0.0      2.0                   \n",
       "5  1.0          1.0                128.0        0.0      2.0                   \n",
       "6  5.0          1.0                128.0        0.0      2.0                   \n",
       "7  10.0         1.0                128.0        0.0      2.0                   \n",
       "8  25.0         1.0                128.0        0.0      2.0                   \n",
       "\n",
       "   Accuracy  F1 score  Precision    Recall   TP_rate   FP_rate   FN_rate  \\\n",
       "0  0.791677  0.817824  0.935205   0.726623  0.935205  0.064795  0.351852   \n",
       "1  0.784668  0.811883  0.929336   0.720787  0.929336  0.070664  0.360000   \n",
       "2  0.781915  0.809302  0.925532   0.719008  0.925532  0.074468  0.361702   \n",
       "3  0.747522  0.790355  0.951835   0.675719  0.951835  0.048165  0.456790   \n",
       "4  0.747522  0.790355  0.951835   0.675719  0.951835  0.048165  0.456790   \n",
       "5  0.747522  0.790355  0.951835   0.675719  0.951835  0.048165  0.456790   \n",
       "6  0.747522  0.790355  0.951835   0.675719  0.951835  0.048165  0.456790   \n",
       "7  0.747522  0.790355  0.951835   0.675719  0.951835  0.048165  0.456790   \n",
       "8  0.747522  0.790355  0.951835   0.675719  0.951835  0.048165  0.456790   \n",
       "\n",
       "    TN_rate  \n",
       "0  0.648148  \n",
       "1  0.640000  \n",
       "2  0.638298  \n",
       "3  0.543210  \n",
       "4  0.543210  \n",
       "5  0.543210  \n",
       "6  0.543210  \n",
       "7  0.543210  \n",
       "8  0.543210  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
