{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataloaders for BERT models\n",
    "BERT requires input data in a specific format, so this notebook creates BERT-specific dataloaders. Overall flow is same as in `generate_dataloaders` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, RandomSampler, SequentialSampler\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from generate_dataloaders import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer\n",
    ")\n",
    "\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers.data.processors.utils import InputExample\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out Sentences with lengths greater than 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = path + '../data/'\n",
    "datasets = pd.read_excel(os.path.join(data_dir, \"master_df_labeled.xlsx\"), index_col = 0)\n",
    "datasets.review = datasets.review.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=datasets[datasets['review'].apply(lambda x: len(x)<=30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide ground_truth(test) and train/val dataset\n",
    "labeled = datasets.loc[datasets['true_pos'].isin([1,0])]\n",
    "unlabeled = datasets[datasets.true_pos==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_labeled\n",
    "train_labeled_dataset, remains_label_dataset = train_test_split(labeled, test_size=0.2, stratify=labeled['true_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset, test_dataset\n",
    "val_dataset, test_dataset = train_test_split(remains_label_dataset, test_size=0.5, stratify=remains_label_dataset['true_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = val_dataset.index.tolist()\n",
    "test_idx = test_dataset.index.tolist()\n",
    "\n",
    "remove_idx = val_idx + test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset - not really needed\n",
    "train_dataset = datasets.drop(remove_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_idx = train_labeled_dataset.index.tolist()\n",
    "#train_unlabeled\n",
    "train_unlabeled_dataset = train_dataset.drop(train_label_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = {\"train_labeled_dataset\":train_labeled_dataset,\n",
    "               \"val_dataset\": val_dataset,\n",
    "               \"test_dataset\":test_dataset,\n",
    "               \"train_unlabeled_dataset\":train_unlabeled_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(all_df):\n",
    "  preprocess_dict = {}\n",
    "  for key, df in all_df.items():\n",
    "    list_of_dicts=[]\n",
    "    for row in df.iterrows():\n",
    "      review = row[1].review\n",
    "      idx = row[1].flagged_index\n",
    "      text_a = \" \".join(review[:idx+1])\n",
    "      text_b = \" \".join(review[idx:])\n",
    "      temp_obj=InputExample(guid=str(row[0]), text_a=text_a, text_b=text_b, label=str(row[1].true_pos))\n",
    "      list_of_dicts.append(temp_obj)\n",
    "    preprocess_dict[key] = list_of_dicts\n",
    "  return preprocess_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dataset into format expected for creating BERT dataloaders\n",
    "preprocess_dict = preprocessing(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensored(dataset):\n",
    "  features = convert_examples_to_features(dataset,\n",
    "                                          tokenizer,\n",
    "                                          label_list=['0','1','-1'],\n",
    "                                          max_length=40,\n",
    "                                          output_mode='classification',\n",
    "                                          pad_on_left=False,\n",
    "                                          pad_token=tokenizer.pad_token_id,\n",
    "                                          pad_token_segment_id=0)\n",
    "  tensored_dataset = TensorDataset(torch.tensor([f.input_ids for f in features], dtype=torch.long), \n",
    "                                torch.tensor([f.attention_mask for f in features], dtype=torch.long), \n",
    "                                torch.tensor([f.token_type_ids for f in features], dtype=torch.long), \n",
    "                                torch.tensor([f.label for f in features], dtype=torch.long))\n",
    "  return tensored_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_labeled_dataloader = DataLoader(to_tensored(preprocess_dict[\"train_labeled_dataset\"]), batch_size=32)\n",
    "bert_train_unlabeled_dataloader = DataLoader(to_tensored(preprocess_dict[\"train_unlabeled_dataset\"]), batch_size=32)\n",
    "bert_val_dataloader = DataLoader(to_tensored(preprocess_dict[\"val_dataset\"]), batch_size=32)\n",
    "bert_test_dataloader = DataLoader(to_tensored(preprocess_dict[\"test_dataset\"]), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"bert_train_labeled_dataloader.p\"),\"wb\") as f:\n",
    "  pickle.dump(bert_train_labeled_dataloader, f)\n",
    "\n",
    "with open(os.path.join(data_dir, \"bert_train_unlabeled_dataloader.p\"),\"wb\") as f:\n",
    "  pickle.dump(bert_train_unlabeled_dataloader, f)\n",
    "\n",
    "with open(os.path.join(data_dir, \"bert_val_dataloader.p\"),\"wb\") as f:\n",
    "  pickle.dump(bert_val_dataloader, f)\n",
    "\n",
    "with open(os.path.join(data_dir, \"bert_test_dataloader.p\"),\"wb\") as f:\n",
    "  pickle.dump(bert_test_dataloader, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
