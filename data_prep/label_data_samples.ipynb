{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Annotated Reviews\n",
    "The first section is for merging in newly annotated data into the `master_df_labeled.xlsx` file, which is the final step before the datset is put into dataloaders. This can be rerun anytime new batches have been annotated. The other two sections remain as documentation on how the batches for annotation were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "#nltk.download('wordnet')\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for Merging in New Annotations\n",
    "1. Print current true and false positive counts\n",
    "2. After labeling, and putting labeled data in appropriate folder, run `batch_merge_into_master`\n",
    "3. Check for any annotation errors, correct them\n",
    "4. Save dataframe back into `master_file.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All excel file paths here, adjust path and directory names as necessary\n",
    "master_file = \"../data/master_df_labeled.xlsx\"\n",
    "#Annotated data should be saved into the batch_labeled folder\n",
    "batch_labeled = \"../data/FairFrame Annotations/labeled/\"\n",
    "greats_folder = \"../data/FairFrame Annotations/greats_batches/\"\n",
    "non_greats_folder = \"../data/FairFrame Annotations/non_greats_batches/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update our running master_df_labeled with multiple new files\n",
    "def batch_merge_into_master(master_file, batch_folder):\n",
    "  batch_files = os.listdir(batch_folder)\n",
    "  print(batch_files)\n",
    "  master_df_labeled = pd.read_excel(master_file, index_col = 0)\n",
    "  for new_file in batch_files:\n",
    "    print(new_file)\n",
    "    if re.match('batch_[0-9]*', new_file):\n",
    "      new_df = pd.read_excel(os.path.join(batch_folder, new_file), index_col = 0)\n",
    "      master_df_labeled.update(new_df.true_pos)\n",
    "      print_tf_pos(master_df_labeled)\n",
    "  #master_df_labeled.to_excel(master_file)\n",
    "  print(\"Please save master_df_labeled!\")\n",
    "  return master_df_labeled\n",
    "\n",
    "#Print out how many examples are labeled as True or False Positives\n",
    "def print_tf_pos(master_df_labeled):\n",
    "  print(f\"Count of true positives: {master_df_labeled[master_df_labeled.true_pos==1].shape[0]}\")\n",
    "  print(f\"Count of false positives: {master_df_labeled[master_df_labeled.true_pos==0].shape[0]}\")\n",
    "  assert master_df_labeled[master_df_labeled.problematic==0].shape[0]==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of true positives: 4647\n",
      "Count of false positives: 715\n",
      "(111029, 6)\n"
     ]
    }
   ],
   "source": [
    "#Load and print current true and false positive counts\n",
    "master_df_labeled = pd.read_excel(master_file,index_col=0)\n",
    "\n",
    "print_tf_pos(master_df_labeled)\n",
    "print(master_df_labeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge in all annotated files\n",
    "master_df_labeled = batch_merge_into_master(master_file, batch_labeled)\n",
    "print_tf_pos(master_df_labeled)\n",
    "print(master_df_labeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>flagged_word</th>\n",
       "      <th>flagged_index</th>\n",
       "      <th>problematic</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>true_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review, flagged_word, flagged_index, problematic, lemmatized, true_pos]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for mistakes in annotation\n",
    "master_df_labeled[master_df_labeled.true_pos.isin([1,0,-1])==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currently there exists 11's that actually should be 1's, correct them here, rerun previous cell to check afterwards\n",
    "master_df_labeled.true_pos[master_df_labeled.true_pos==11]=master_df_labeled.true_pos[master_df_labeled.true_pos==11].replace(11,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the merged dataset\n",
    "master_df_labeled.to_excel(master_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling for less common examples\n",
    "This code should not be run again, unless there is need to resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_words = [\"great\", \"outstanding\", \"excellent\", \"professional\", \"creative\", \"experience\"]\n",
    "batch_nums = range(200,1110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for b in batch_nums:\n",
    "  f_name = \"batch_\"+str(b)+\".xlsx\"\n",
    "  df_list.append(pd.read_excel(os.path.join(batch_folder, f_name), index_col=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df[\"lemmatized\"] = [wordnet_lemmatizer.lemmatize(w) for w in concat_df.flagged_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greats_df = concat_df[concat_df.lemmatized.isin(filter_words)]\n",
    "nongreats_df = concat_df[~concat_df.lemmatized.isin(filter_words)]\n",
    "#nongreats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fixed chunks of shuffled data for people to label\n",
    "def batch_please_label_greats(df, batch_folder, cat, sample_num=100):\n",
    "  df = df.drop(labels=[\"lemmatized\"],axis=1)\n",
    "  row_num = df.shape[0]\n",
    "  batches = row_num//sample_num\n",
    "  print(batches)\n",
    "  #assert 1==0\n",
    "  \n",
    "  for start_bin in range(batches):\n",
    "    end_bin = start_bin+1\n",
    "    temp = df.iloc[start_bin*sample_num:end_bin*sample_num,:]\n",
    "    if end_bin == batches and row_num%sample_num>0:#we have leftover rows to deal with\n",
    "      extra = df.iloc[end_bin*sample_num:,:]\n",
    "      f_name = \"batch_\"+str(end_bin)+\"_\"+cat+\".xlsx\"\n",
    "      extra.to_excel(os.path.join(batch_folder, f_name))\n",
    "    f_name = \"batch_\"+str(start_bin)+\"_\"+cat+\".xlsx\"\n",
    "    temp.to_excel(os.path.join(batch_folder, f_name))\n",
    "  print(f\"Full Batches: {batches}\")\n",
    "  print(f\"Partial Batch?: {row_num%sample_num>0}\")\n",
    "  print(f\"Go check {batch_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_please_label_greats(greats_df,greats_folder, \"greats\")\n",
    "batch_please_label_greats(nongreats_df,non_greats_folder, \"non_greats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Labeling Process\n",
    "This code is here in case there is ever a need to revert back to the first set of annotated data, or for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n",
      "448\n"
     ]
    }
   ],
   "source": [
    "#batch_folder = \"../data/please_label_batch/100/\"\n",
    "master_df = pickle.load(open(master_file, \"rb\"))\n",
    "master_df[\"lemmatized\"] = [wordnet_lemmatizer.lemmatize(w) for w in master_df.flagged_word]\n",
    "#\n",
    "print(len(set(master_df.flagged_word)))\n",
    "print(len(set(master_df.lemmatized)))\n",
    "#print(set(master_df.flagged_word)-set(master_df.lemmatized))\n",
    "# print(sorted(list(set(master_df.lemmatized)),key=str.lower))\n",
    "# master_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #creating random sample from master_df to label\n",
    "# sampled = master_df[master_df.problematic==1].sample(500)\n",
    "# sampled.to_excel(\"../data/fptp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of true positives: 84\n",
      "Count of false positives: 31\n"
     ]
    }
   ],
   "source": [
    "#merge by row index\n",
    "labeled = pd.read_excel(\"../data/fptp_labeled.xlsx\",index_col=0)\n",
    "master_df_labeled = master_df.join(labeled.true_pos).fillna(-1)\n",
    "master_df_labeled = master_df_labeled[master_df_labeled.problematic==1]\n",
    "#master_df_labeled\n",
    "print_tf_pos(master_df_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master file to continuously update\n",
    "master_df_labeled.to_excel(master_df_labeled)\n",
    "pickle.dump(master_df_labeled,open(master_file,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates new sample of data to be labeled\n",
    "def please_label(master_file, new_file, sample_num=100):\n",
    "  master_df = pd.read_excel(master_file,index_col=0)\n",
    "  to_label_df = master_df[master_df.true_pos==-1]\n",
    "  output_df = to_label_df.sample(sample_num)\n",
    "  output_df.to_excel(new_file)\n",
    "  return output_df\n",
    "\n",
    "#Create fixed chunks of shuffled data for people to label\n",
    "def batch_please_label(master_file, batch_folder, sample_num=100):\n",
    "  master_df = pd.read_excel(master_file,index_col=0).drop(labels=[\"lemmatized\",\"problematic\"],axis=1)\n",
    "  to_label_df = master_df[master_df.true_pos==-1].sample(frac=1,random_state=0)\n",
    "  row_num = to_label_df.shape[0]\n",
    "  batches = row_num//sample_num\n",
    "  print(batches)\n",
    "  #assert 1==0\n",
    "  \n",
    "  for start_bin in range(batches):\n",
    "    end_bin = start_bin+1\n",
    "    temp = to_label_df.iloc[start_bin*sample_num:end_bin*sample_num,:]\n",
    "    if end_bin == batches and row_num%sample_num>0:#we have leftover rows to deal with\n",
    "      extra = to_label_df.iloc[end_bin*sample_num:,:]\n",
    "      extra.to_excel(batch_folder+\"batch_\"+str(end_bin)+\".xlsx\")\n",
    "    temp.to_excel(batch_folder+\"batch_\"+str(start_bin)+\".xlsx\")\n",
    "  print(f\"Full Batches: {batches}\")\n",
    "  print(f\"Partial Batch?: {row_num%sample_num>0}\")\n",
    "  print(f\"Go check {batch_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update our running master_df_labeled, main function to produce files for dataloaders\n",
    "def merge_into_master(master_file, new_file, grounds_file, batch=False):\n",
    "  master_df_labeled = pd.read_excel(master_file,index_col=0)\n",
    "  new_df = pd.read_excel(new_file,index_col=0)\n",
    "\n",
    "  master_df_labeled.update(new_df.true_pos)\n",
    "  master_df_labeled.to_excel(master_file)\n",
    "  \n",
    "  #pickle.dump(master_df_labeled,open(\"../data/master_df_labeled.p\",\"wb\"))\n",
    "  return master_df_labeled\n",
    "  \n",
    "\n",
    "#Produces ground_truths_df\n",
    "def produce_ground_truths(master_df_labeled, grounds_file):\n",
    "  ground_truths_df = master_df_labeled[master_df_labeled.true_pos!=-1]\n",
    "  ground_truths_df.to_excel(grounds_file)\n",
    "  pickle.dump(ground_truths_df,open(\"../data/ground_truths_df.p\",\"wb\"))\n",
    "  return ground_truths_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
